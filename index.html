<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> 爱上口袋的天空</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/image1.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <!-- mermaid -->
      
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover1.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">爱上口袋的天空</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['欢迎来到爱上口袋的天空的博客', '愿你一生努力，一生被爱', '想要的都拥有，得不到的都释怀'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  <ul class="ads">
    
        <li>
            <a target="_blank" rel="noopener" href="https://curl.qcloud.com/kvO7hb43">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/ten_1.jpg" width="300" alt="云服务器限时秒杀">
            </a>
        </li>
    
        <li>
            <a target="_blank" rel="noopener" href="https://www.vultr.com/?ref=8630075">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/vultr.png" width="300" alt="vultr优惠vps">
            </a>
        </li>
    
</ul>
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-MapReduce知识点学习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E7%9F%A5%E8%AF%86%E7%82%B9%E5%AD%A6%E4%B9%A0/"
    >MapReduce知识点学习</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E7%9F%A5%E8%AF%86%E7%82%B9%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2021-07-18T14:10:35.243Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce知识点学习<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—title: MapReduce知识点学习<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—1. 1. 1. 1. 1. 1. 1. 1. </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之数据清洗(ETL)案例,倒排索引案例,ReduceTask 工作机制,Hadoop 数据压缩简介"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97(ETL)%E6%A1%88%E4%BE%8B,%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E6%A1%88%E4%BE%8B,ReduceTask%20%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6,Hadoop%20%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E7%AE%80%E4%BB%8B/"
    >MapReduce之数据清洗(ETL)案例,倒排索引案例,ReduceTask 工作机制,Hadoop 数据压缩简介</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97(ETL)%E6%A1%88%E4%BE%8B,%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E6%A1%88%E4%BE%8B,ReduceTask%20%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6,Hadoop%20%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E7%AE%80%E4%BB%8B/" class="article-date">
  <time datetime="2021-07-18T14:10:35.237Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce之数据清洗(ETL)案例,倒排索引案例,ReduceTask 工作机制,Hadoop 数据压缩简介<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：简介</p>
<p>             在运行核心业务 Mapreduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。       清理的过程往往只需要运行 mapper 程序，不需要运行 reduce 程序。</p>
<h1 id="二：日志清洗案例之简单解析版"><a href="#二：日志清洗案例之简单解析版" class="headerlink" title="二：日志清洗案例之简单解析版"></a>二：日志清洗案例之简单解析版</h1><ol>
<li> 需求：去除日志中字段长度小于等于11的日志(每一行按照空格切割，切割后数组长度小于11的日志不要)1.  数据如下：   <img alt="" class="has" height="154" src="https://img-blog.csdnimg.cn/20190808213958907.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="455"><li>代码实现如下：  ⑴创建mapper类：           <pre class="has"><code class="language-java">package com.kgf.mapreduce.weblog;</li>
</ol>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</p>
<pre><code>Text k = new Text();

@Override
protected void map(LongWritable key, Text value,Context context)
        throws IOException, InterruptedException &#123;
    
    //1:获取一行
    String line = value.toString();
    
    //2：解析一行数据
    boolean result = parseLog(line);
    
    if(!result) &#123;
        return;
    &#125;
    k.set(line);
    context.write(k, NullWritable.get());
&#125;

private boolean parseLog(String line) &#123;
    String[] fields = line.split(&quot; &quot;);
    if(fields.length&amp;gt;11) &#123;
        return true;
    &#125;
    return false;
&#125;
</code></pre>
<p>}<br></code></pre> ⑵创建Driver: <pre class="has"><code class="language-java">package com.kgf.mapreduce.weblog;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class LogDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1：获取Job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2：设置jar对象
    job.setJarByClass(LogDriver.class);
    
    //3:设置关联Mapper
    job.setMapperClass(LogMapper.class);
    
    //4：设置mapper输出类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(NullWritable.class);
    
    //5:设置reduce task为0
    job.setNumReduceTasks(0);
    
    //6：设置最终输出参数
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    
    //7：设置文件输入输出路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //8：提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>   </li>1.   效果：  ⑴日志清理前数据有如下数量：           <img alt="" class="has" height="120" src="https://img-blog.csdnimg.cn/20190808221608218.png" width="413">         ⑵清理后：            <img alt="" class="has" height="175" src="https://img-blog.csdnimg.cn/20190808221642199.png" width="474">   ⑶程序运行日志如下：           <img alt="" class="has" height="276" src="https://img-blog.csdnimg.cn/201908082218109.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="512">1. 计数器应用   ⑴简介：            Hadoop 为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已      处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。   ⑵计数器方式如下：          <img alt="" class="has" height="274" src="https://img-blog.csdnimg.cn/20190808223125431.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="462">  ⑶案例，我们可以在上面的日志清洗案例中加上计数器组应用：代码如下：         <img alt="" class="has" height="349" src="https://img-blog.csdnimg.cn/2019080822352462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="427">      效果如下（可以发现控制台出现了下面的日志内容，可以帮助我们定位问题）：         <img alt="" class="has" height="322" src="https://img-blog.csdnimg.cn/20190808223631620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="431"></p>
<h1 id="三：倒排索引案例"><a href="#三：倒排索引案例" class="headerlink" title="三：倒排索引案例"></a>三：倒排索引案例</h1><ol>
<li>现在有三个文件a.txt,b.txt,c.txt,现在我们需要将文件里面的单词汇总 ⑴文件如下：         <img alt="" class="has" height="67" src="https://img-blog.csdnimg.cn/20190809224946281.png" width="106"><img alt="" class="has" height="66" src="https://img-blog.csdnimg.cn/20190809224958271.png" width="156"><img alt="" class="has" height="65" src="https://img-blog.csdnimg.cn/2019080922501325.png" width="208"> ⑵最后要求的结果（汇总每个单词所在的文件中个数）：       <img alt="" class="has" height="117" src="https://img-blog.csdnimg.cn/20190809225058159.png" width="514"><li> 代码实现如下：  ⑴建立bean对象         <pre class="has"><code class="language-java">package com.kgf.mapreduce.index;</li>
</ol>
<p>import java.io.DataInput;<br>import java.io.DataOutput;<br>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.Writable;</p>
<p>public class IndexVo implements Writable&#123;</p>
<pre><code>private String word;

private String file;

private int num;

public IndexVo(String word, String file, int num) &#123;
    super();
    this.word = word;
    this.file = file;
    this.num = num;
&#125;

public IndexVo() &#123;
    super();
&#125;

@Override
public void readFields(DataInput in) throws IOException &#123;
    this.word = in.readUTF();
    this.file = in.readUTF();
    this.num = in.readInt();
&#125;

@Override
public void write(DataOutput out) throws IOException &#123;
    out.writeUTF(word);
    out.writeUTF(file);
    out.writeInt(num);
&#125;

public String getWord() &#123;
    return word;
&#125;

public void setWord(String word) &#123;
    this.word = word;
&#125;

public String getFile() &#123;
    return file;
&#125;

public void setFile(String file) &#123;
    this.file = file;
&#125;

public int getNum() &#123;
    return num;
&#125;

public void setNum(int num) &#123;
    this.num = num;
&#125;

@Override
public String toString() &#123;
    return word + &quot;\t&quot; + file + &quot;\t&quot; + num;
&#125;
</code></pre>
<p>}<br></code></pre>  ⑵建立mapper对象         <pre class="has"><code class="language-java">package com.kgf.mapreduce.index;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>public class IndexMapper extends Mapper&lt;LongWritable, Text, Text, IndexVo&gt;&#123;</p>
<pre><code>private String fileName = null;

Text k = new Text();

IndexVo v = new IndexVo();

@Override
protected void setup(Context context)
        throws IOException, InterruptedException &#123;
    //1：获取文件切片信息
    FileSplit splitFile = (FileSplit) context.getInputSplit();
    fileName = splitFile.getPath().getName();
&#125;

@Override
protected void map(LongWritable key, Text value,Context context)
        throws IOException, InterruptedException &#123;
    
    //1:获取一行数据
    String line = value.toString();
    //2:切割数据
    String[] fields = line.split(&quot;\t&quot;);
    for (String field : fields) &#123;
        k.set(field);
        v.setWord(field);
        v.setFile(fileName);
        v.setNum(1);
        context.write(k, v);
    &#125;
&#125;
</code></pre>
<p>}<br></code></pre> ⑶建立reducer对象       <pre class="has"><code class="language-java">package com.kgf.mapreduce.index;</p>
<p>import java.io.IOException;<br>import java.util.HashMap;<br>import java.util.Map;<br>import java.util.Set;</p>
<p>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>public class IndexReducer extends Reducer&lt;Text, IndexVo, Text, NullWritable&gt; &#123;</p>
<pre><code>Text k = new Text();

@Override
protected void reduce(Text key, Iterable&amp;lt;IndexVo&amp;gt; values,Context context)
        throws IOException, InterruptedException &#123;
    
    Map&amp;lt;String,Integer&amp;gt; map = new HashMap&amp;lt;String,Integer&amp;gt;();
    
    for (IndexVo indexVo : values) &#123;
        String name = indexVo.getFile();
        if(map.containsKey(name)) &#123;
            map.put(name, map.get(name)+indexVo.getNum());
        &#125;else &#123;
            map.put(name, indexVo.getNum());
        &#125;
    &#125;
    
    String result = key.toString()+&quot;\t&quot;;
    for (String fileName : map.keySet()) &#123;
        result+=(fileName+&quot;\t&quot;+map.get(fileName))+&quot;\t&quot;;
    &#125;
    k.set(result);
    context.write(k, NullWritable.get());
&#125;
</code></pre>
<p>}<br></code></pre> ⑷建立Driver对象       <pre class="has"><code class="language-java">package com.kgf.mapreduce.index;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class IndexDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1：获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2：设置jar
    job.setJarByClass(IndexDriver.class);
    
    //3:关联mapper和reducer
    job.setMapperClass(IndexMapper.class);
    job.setReducerClass(IndexReducer.class);
    
    //4:设置mapper输出参数
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IndexVo.class);
    
    //5:设置最终输出参数
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    
    //6：设置数据输入输出路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //7：提交
    boolean rsult = job.waitForCompletion(true);
    System.exit(rsult?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>         </li></p>
<h1 id="四：-ReduceTask-工作机制"><a href="#四：-ReduceTask-工作机制" class="headerlink" title="四： ReduceTask 工作机制"></a>四： ReduceTask 工作机制</h1><ol>
<li>ReduceTask有如下特点：  <img alt="" class="has" height="381" src="https://img-blog.csdnimg.cn/20190810091911354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="529">1.  流程图如下：  <img alt="" class="has" height="461" src="https://img-blog.csdnimg.cn/20190810092256618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="595">  ⑴Copy阶段：          ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，     则写到磁盘上，否则直接放到内存中。  ⑵Merge 阶段：           在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，     以防止内存使用过多或磁盘上文件过多。   ⑶Sort 阶段：           按照 MapReduce 语义，用户编写 reduce()函数输入数据是按 key 进行聚集的一组数据。     为了将 key 相同的数据聚在一起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经     实现对自己的处理结果进行了局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。   ⑷Reduce 阶段：            reduce()函数将计算结果写到 HDFS 上。 <h1 id="五：-Hadoop-数据压缩"><a href="#五：-Hadoop-数据压缩" class="headerlink" title="五： Hadoop 数据压缩"></a>五： Hadoop 数据压缩</h1></li>
<li>简介     压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。 在 Hadoop 下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下， I/O 操作和网络数据传输要花大量的时间。还有，Shuffle与 Merge 过程同样也面临着巨大的 I/O 压力。     鉴于磁盘 I/O 和网络带宽是 Hadoop 的宝贵资源，数据压缩对于节省资源、最小化磁盘 I/O 和网络传输非常有帮助。不过，尽管压缩与解压操作的 CPU 开销不高，其性能的提升和 资源的节省并非没有代价。      如果磁盘 I/O 和网络带宽影响了 MapReduce 作业性能，在任意 MapReduce 阶段启用压 缩都可以改善端到端处理时间并减少 I/O 和网络流量。       压缩 Mapreduce 的一种优化策略：通过压缩编码对 Mapper 或者 Reducer 的输出进行 压缩，以减少磁盘 IO，提高 MR 程序运行速度（但相应增加了 cpu 运算负担）。       <img alt="" class="has" height="201" src="https://img-blog.csdnimg.cn/20190810095228924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="562">1. MR支持的压缩编码     <img alt="" class="has" height="372" src="https://img-blog.csdnimg.cn/20190810095333486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="422">    <img alt="" class="has" height="277" src="https://img-blog.csdnimg.cn/2019081009535852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="583">1.  压缩性能的比较     <img alt="" class="has" height="297" src="https://img-blog.csdnimg.cn/20190810095427226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="629">1.  压缩方式选择       ⑴Gzip 压缩           <img alt="" class="has" height="291" src="https://img-blog.csdnimg.cn/20190810105343262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="518">   ⑵ Bzip2 压缩           <img alt="" class="has" height="310" src="https://img-blog.csdnimg.cn/20190810105413209.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="581">    ⑶Lzo 压缩           <img alt="" class="has" height="210" src="https://img-blog.csdnimg.cn/20190810105518675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="548">          <img alt="" class="has" height="76" src="https://img-blog.csdnimg.cn/20190810105535471.png" width="549">    ⑷Snappy 压缩           <img alt="" class="has" height="205" src="https://img-blog.csdnimg.cn/20190810105618682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="559">1. 压缩位置选择   压缩可以在 MapReduce 作用的任意阶段启用。   <img alt="" class="has" height="651" src="https://img-blog.csdnimg.cn/20190810105756931.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="754">1. 压缩配置参数   <img alt="" class="has" height="352" src="https://img-blog.csdnimg.cn/20190810105926815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="683">  <img alt="" class="has" height="369" src="https://img-blog.csdnimg.cn/20190810105950851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="680">  <img alt="" class="has" height="376" src="https://img-blog.csdnimg.cn/20190810110017564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="680">  <img alt="" class="has" height="336" src="https://img-blog.csdnimg.cn/20190810110042410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="677"></li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之入门概述以及WordCount 案例"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E4%B9%8B%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8AWordCount%20%E6%A1%88%E4%BE%8B/"
    >MapReduce之入门概述以及WordCount 案例</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E4%B9%8B%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8AWordCount%20%E6%A1%88%E4%BE%8B/" class="article-date">
  <time datetime="2021-07-18T14:10:35.227Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce之入门概述以及WordCount 案例<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：MapReduce定义</p>
<ol>
<li> 简介       Mapreduce 是一个分布式运算程序的编程框架，是用户开发“基于 hadoop 的数据分析应用”的核心框架。       Mapreduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序， 并发运行在一个 hadoop 集群上。1. Mapreduce 优缺点       ⑴优点：                 a：MapReduce  易于编程                           它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量                      廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模                      一样的。就是因为这个特点使得 MapReduce 编程变得非常流行。                 b：良好的 扩展性                            当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。                 c： 高容错性                            MapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上，这就要求它具有很高的容错性。                       比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行                       失败，而且这个过程不需要人工参与，而完全是由Hadoop 内部完成的。                 d：适合 PB  级以上海量数据的 离线处理                            这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，                       MapReduce 很难做到。       ⑵缺点：                 a：MapReduce 不 擅长做实时计算、流式计算、DAG（有向图 ） 计算                 b：实时计算：                            MapReduce 无法像 Mysql 一样，在毫秒或者秒级内返回结果。                 c：流式计算：                             流式计算的输入数据是动态的，而 MapReduce 的输入数据集是静态的，不能动态变化。                       这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。                 d：DAG （有向图）计算：                              多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，                       MapReduce 并不是不能做，而是使用后，每个 MapReduce 作业的输出结果都会写入到磁盘，                       会造成大量的磁盘 IO，导致性能非常的低下。1. MapReduce的核心思想      <img alt="" class="has" height="478" src="https://img-blog.csdnimg.cn/20190727112452226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="591">      <img alt="" class="has" height="278" src="https://img-blog.csdnimg.cn/20190727112523638.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="633">          <h1 id="二：MapReduce进程"><a href="#二：MapReduce进程" class="headerlink" title="二：MapReduce进程"></a>二：MapReduce进程</h1></li>
<li> 简介   <img alt="" class="has" height="251" src="https://img-blog.csdnimg.cn/20190727112710794.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="706">         <h1 id="三：MapReduce-编程规范-（八股文）"><a href="#三：MapReduce-编程规范-（八股文）" class="headerlink" title="三：MapReduce  编程规范 （八股文）"></a>三：MapReduce  编程规范 （八股文）</h1></li>
<li>简介       用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 mr 程序的客户端)。1. Mapper阶段       <img alt="" class="has" height="228" src="https://img-blog.csdnimg.cn/20190727113717265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="587">1. Reducer阶段       <img alt="" class="has" height="204" src="https://img-blog.csdnimg.cn/20190727114144791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="658">1. Driver 阶段      整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象            <h1 id="四：WordCount-案例"><a href="#四：WordCount-案例" class="headerlink" title="四：WordCount 案例"></a>四：WordCount 案例</h1></li>
<li>需求：         在一堆给定的文本文件中统计输出每一个单词出现的总次数。1. 数据准备，一个文件hello.txt,内容如下   <img alt="" class="has" height="287" src="https://img-blog.csdnimg.cn/20190727114806278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="354">1. 案例分析   <img alt="" class="has" height="361" src="https://img-blog.csdnimg.cn/20190727115633681.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="539"><li>代码实现     ⑴在eclipse中创建工程               <img alt="" class="has" height="154" src="https://img-blog.csdnimg.cn/2019072712170054.png" width="347">               jar包和之前的一样，从hadoop安装包中拷贝出来。环境变量之前已经配置好了。              <img alt="" class="has" height="212" src="https://img-blog.csdnimg.cn/20190727121812316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="437">     ⑵创建自定义的Mapper类去对数据进行分类，注意：我们这里会将所有数据分类完成后才会进入到下一阶段           <pre class="has"><code class="language-java">package com.kgf.mapreduce;</li>
</ol>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.IntWritable;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>/***</p>
<ul>
<li></li>
<li><p>继承的Mapper参数如下：</p>
</li>
<li><p>第一个参数key：LongWritable表示输入的key的行号</p>
</li>
<li><p>第二个参数value：Text表示一行内容</p>
</li>
<li><p>第三个参数key： Text表示单词</p>
</li>
<li><p>第四个参数value:IntWritable表示计算后的单词的个数</p>
</li>
<li><p>@author kgf</p>
</li>
<li></li>
<li><p>/<br>public class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt;&#123;</p>
<p>  Text k = new Text();<br>  IntWritable v = new IntWritable(1);</p>
<p>  /**</p>
<ul>
<li>使用map方法去处理数据，数据是一行一行进入到这个方法处理的</li>
<li>key：表示行号</li>
<li>value：表示一行数据内容</li>
<li>/<br>@Override<br>protected void map(LongWritable key, Text value, Context context)<pre><code>  throws IOException, InterruptedException &#123;
</code></pre>
  //首先我们将一行内容转换成String<br>  String line = value.toString();<br>  //数据的单词之间是以空格切割的<br>  String[] words = line.split(“ “);<br>  //将数据循环写出到下一阶段<br>  for (String word : words) {<pre><code>  k.set(word);
  context.write(k, v);
</code></pre>
  }<br>}<br>}<br></code></pre> ⑶创建自定义的Reducer类对分类的数据进行汇总      <pre class="has"><code class="language-java">package com.kgf.mapreduce;</li>
</ul>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.IntWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>/**</p>
<ul>
<li><p>注意：这里继承Reducer的前两个入参就是Mappper的出参数</p>
</li>
<li><p>@author kgf</p>
</li>
<li></li>
<li><p>/<br>public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</p>
<p>  /**</p>
<ul>
<li>这个方法主要是对map分类之后的数据进行聚合的</li>
<li>/<br>@Override<br>protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,<pre><code>  Context context) throws IOException, InterruptedException &#123;
</code></pre>
  //统计单词个数<br>  int sum = 0;<br>  for (IntWritable count : values) {<pre><code>  sum+=count.get();
</code></pre>
  }<br>  //输出单词总个数<br>  context.write(key, new IntWritable(sum));<br>}</li>
</ul>
</li>
</ul>
<p>}<br></code></pre> ⑷创建Driver提交任务      <pre class="has"><code class="language-java">package com.kgf.mapreduce;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.IntWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class WordCountDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    
    //1:首先获取job信息
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:获取jar包位置,指定入口类，hadoop会自己找到
    job.setJarByClass(WordCountDriver.class);
    
    //3：关联自定义的mapper和reducer
    job.setMapperClass(WordCountMapper.class);
    job.setReducerClass(WordCountReducer.class);
    
    //4:设置map输出类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IntWritable.class);
    
    //5:设置reducer输出类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    
    //6:设置数据输入和输出文件路径,这里我们通过main方法获取参数路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //7:提交代码
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>   </li>1.   在eclispe上将程序打成jar包   <img alt="" class="has" height="111" src="https://img-blog.csdnimg.cn/2019072715265789.png" width="150">1.  对jar包进行测试  ⑴启动集群  ⑵将jar包以及准备的hello.txt数据文本上传到/opt/module/hadoop-2.7.2目录下，并且设置有权限        <img alt="" class="has" height="336" src="https://img-blog.csdnimg.cn/20190727154242481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="404">           ⑶将准备的hello.txt文件上传的hdfs指定目录下         <img alt="" class="has" height="79" src="https://img-blog.csdnimg.cn/20190727154354678.png" width="652">         <img alt="" class="has" height="179" src="https://img-blog.csdnimg.cn/20190727154506556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="548">   ⑷使用我们的jar去测试（当前路径：/opt/module/hadoop-2.7.2）：         命令:hadoop jar +jar名称 +Driver入口的全路径 +输入路径 +输出路径         <img alt="" class="has" height="85" src="https://img-blog.csdnimg.cn/20190727155045147.png" width="666">        执行成功生成的文件：        <img alt="" class="has" height="225" src="https://img-blog.csdnimg.cn/20190727155247312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="578">    ⑸查看文件内容           <img alt="" class="has" height="158" src="https://img-blog.csdnimg.cn/20190727155447856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="488">1. 本地模式运行     ⑴上面我们是将jar包放到集群上运行，这里我们们需要在本地直接运行，因为本地我们有hadoop的jar包     ⑵在eclise上配置环境变量          <img alt="" class="has" height="189" src="https://img-blog.csdnimg.cn/20190727160144166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="405">          注意：输出路径不能提前建好。         <img alt="" class="has" height="168" src="https://img-blog.csdnimg.cn/20190727160219740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="366">     ⑶执行效果：         <img alt="" class="has" height="195" src="https://img-blog.csdnimg.cn/20190727160422404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="365">         <img alt="" class="has" height="229" src="https://img-blog.csdnimg.cn/20190727160525354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="405">        </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之Shuffle机制以及小文件处理案例（自定义 InputFormat），OutputFormat 案例"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E4%B9%8BShuffle%E6%9C%BA%E5%88%B6%E4%BB%A5%E5%8F%8A%E5%B0%8F%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B%EF%BC%88%E8%87%AA%E5%AE%9A%E4%B9%89%20InputFormat%EF%BC%89%EF%BC%8COutputFormat%20%E6%A1%88%E4%BE%8B/"
    >MapReduce之Shuffle机制以及小文件处理案例（自定义 InputFormat），OutputFormat 案例</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E4%B9%8BShuffle%E6%9C%BA%E5%88%B6%E4%BB%A5%E5%8F%8A%E5%B0%8F%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B%EF%BC%88%E8%87%AA%E5%AE%9A%E4%B9%89%20InputFormat%EF%BC%89%EF%BC%8COutputFormat%20%E6%A1%88%E4%BE%8B/" class="article-date">
  <time datetime="2021-07-18T14:10:35.219Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce之Shuffle机制以及小文件处理案例（自定义 InputFormat），OutputFormat 案例<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：Shuffle机制</p>
<ol>
<li>简介        Mapreduce 确保每个 reducer 的输入都是按键排序的。系统执行排序的过程（即将 map 输出作为输入传给 reducer）称为 shuffle。   <img alt="" class="has" height="298" src="https://img-blog.csdnimg.cn/20190801211802997.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="511">1.  详细流程如下：  <img alt="" class="has" height="350" src="https://img-blog.csdnimg.cn/20190801213915875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="415">          <h1 id="二：小文件处理案例（自定义-InputFormat）"><a href="#二：小文件处理案例（自定义-InputFormat）" class="headerlink" title="二：小文件处理案例（自定义 InputFormat）"></a>二：小文件处理案例（自定义 InputFormat）</h1></li>
<li>需求：      无论 hdfs 还是 mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文 件的场景，此时，就需要有相应解决方案。将多个小文件合并成一个文件 SequenceFile， SequenceFile 里面存储着多个文件，存储的形式为文件路径+名称为 key，文件内容为 value。1. 分析，小文件的优化无非以下几种方式：   a：在数据采集的时候，就将小文件或小批数据合成大文件再上传 HDFS   b：在业务处理之前，在 HDFS 上使用 mapreduce 程序对小文件进行合并   c：在 mapreduce 处理时，可采用 CombineTextInputFormat 提高效率 <img alt="" class="has" height="295" src="https://img-blog.csdnimg.cn/2019080122024881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="477">1.  数据准备（准备如下三个小文件）：  <img alt="" class="has" height="205" src="https://img-blog.csdnimg.cn/20190804081453104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="392"><li>具体实现如下：   ⑴自定义 InputFromat             <pre class="has"><code class="language-java">package com.kgf.mapreduce.inputformat;</li>
</ol>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.BytesWritable;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.mapreduce.InputSplit;<br>import org.apache.hadoop.mapreduce.JobContext;<br>import org.apache.hadoop.mapreduce.RecordReader;<br>import org.apache.hadoop.mapreduce.TaskAttemptContext;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</p>
<p>/**</p>
<ul>
<li>自定义WholeFileInputformat,主要是在自定义的mapper读取数据之前对数据进行处理，然后传给mapper：</li>
<li>这里我们的key是NullWritable，因为我们每次读取的是一个小文件，不需要关注行号等等。</li>
<li>value是BytesWritable，字节数组，代表的是我们每次读取的一个小文件，然后将这个文件内容传递给mapper</li>
<li>@author KGF</li>
<li></li>
<li>/<br>public class WholeFileInputformat extends FileInputFormat&lt;NullWritable,BytesWritable&gt;&#123;<br>  /***<ul>
<li>设置每个小文件不可分片,保证一个小文件生成一个key-value键值对</li>
<li>/<br>@Override<br>protected boolean isSplitable(JobContext context, Path filename) &#123;<br>  return false;<br>&#125;<br>/**</li>
<li>自定义需重写父类RecordReader方法，自定义读取文件的方式</li>
<li>split：表示的是我们每次读取一个小文件时的切片信息</li>
<li>context：代表上下文信息</li>
<li>/<br>@Override<br>public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit split,<pre><code>  TaskAttemptContext context)
  throws IOException, InterruptedException &#123;
</code></pre>
  //创建自定义的RecordReader对象<br>  WholeFileRecordReader record = new WholeFileRecordReader();<br>  //初始化record中的方法<br>  record.initialize(split, context);<br>  return record;<br>}</li>
</ul>
</li>
</ul>
<p>}<br></code></pre> ⑵自定义WholeFileRecordReader <pre class="has"><code class="language-java">package com.kgf.mapreduce.inputformat;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.FSDataInputStream;<br>import org.apache.hadoop.fs.FileSystem;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.BytesWritable;<br>import org.apache.hadoop.io.IOUtils;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.mapreduce.InputSplit;<br>import org.apache.hadoop.mapreduce.RecordReader;<br>import org.apache.hadoop.mapreduce.TaskAttemptContext;<br>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>/***</p>
<ul>
<li><p>该类主要用来读取每个小文件内容</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class WholeFileRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt;&#123;<br>  //创建返回结果对象<br>  BytesWritable v = new BytesWritable();<br>  //当前读取进度，默认没有读<br>  boolean isProgress = false;<br>  FileSplit fileSplit;<br>  Configuration conf;<br>  /***</p>
<ul>
<li>初始化方法：</li>
<li>split:表示传递过来的文件切片</li>
<li>/<br>@Override<br>public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123;<br>  //1：首先我们需要将切片转换为FileSplit<br>  this.fileSplit = (FileSplit) split;<br>  //2:获取上下文配置信息<br>  conf = context.getConfiguration();<br>&#125;</li>
</ul>
<p>  /***</p>
<ul>
<li>读取一个一个文件</li>
<li>/<br>@Override<br>public boolean nextKeyValue() throws IOException, InterruptedException &#123;<br>  //开始读取文件，我们有三个小文件，所以它要读取3次<br>  FileSystem fs = null;<br>  FSDataInputStream fis = null;<br>  if(!isProgress) &#123;//判断状态，程序没有读取文件时，我们就可以开始读了<pre><code>  try &#123;
      //定义一个缓冲区对象，大小就是一个小文件的大小
      byte[] buf = new byte[(int) fileSplit.getLength()];
      //获取文件路径
      Path path = this.fileSplit.getPath();
      //获取文件系统对象
      fs = path.getFileSystem(conf);
      //打开文件输入流
      fis = fs.open(path);
      //流的拷贝,将读取的文件拷贝到缓冲区中
      IOUtils.readFully(fis, buf, 0, buf.length);
      //最后我们将缓冲区的数据拷贝到最终输出的对象中
      v.set(buf, 0, buf.length);
      //是否继续读文件，防止重复读,当下一个小文件进来时，这里的变量就会重新初始化为false
      isProgress = true;
      return true;
  &#125; catch (Exception e) &#123;
      e.printStackTrace();
  &#125;finally &#123;
      IOUtils.closeStream(fs);
      IOUtils.closeStream(fis);
  &#125;
</code></pre>
  }<br>  return false;<br>}<br>/**</li>
<li>获取key,我们这里没有key，所以直接是NullWritable</li>
<li>/<br>@Override<br>public NullWritable getCurrentKey() throws IOException, InterruptedException {<br>  return NullWritable.get();<br>}<br>/***</li>
<li>这个是获取当前的value值方法</li>
<li>/<br>@Override<br>public BytesWritable getCurrentValue() throws IOException, InterruptedException {<br>  return v;<br>}<br>/**</li>
<li>获取当前进度的方法</li>
<li>/<br>@Override<br>public float getProgress() throws IOException, InterruptedException {<br>  //正在读返回1，否则返回0，通过这个方法别人可以知道我们是否正在读取文件<br>  return isProgress?1:0;<br>}</li>
</ul>
<p>  @Override<br>  public void close() throws IOException {</p>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre> ⑶创建SequenseFileMapper.java类 <pre class="has"><code class="language-java">package com.kgf.mapreduce.inputformat;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.BytesWritable;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>/***</p>
<ul>
<li>创建自定义的mapper，前两个参数就是WholeFileInputformat处理数据后的输出参数，</li>
<li>后面两个参数，一个是key-&gt;是文件路径加上名称拼接而成。一个是value，就是小文件内容</li>
<li>@author KGF</li>
<li></li>
<li>/<br>public class SequenseFileMapper extends Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;&#123;  Text k = new Text();  /***<ul>
<li>该方法是主要用来获取文件路径和名称,程序运行会先进入这个setup方法，再进入我们自定义的WholeFileInputformat类中的方法，</li>
<li>然后再回来进入map方法</li>
<li>/<br>@Override<br>protected void setup(Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;.Context context)<pre><code>  throws IOException, InterruptedException &#123;
</code></pre>
  //1:获取上下文切片信息<br>  FileSplit split = (FileSplit) context.getInputSplit();<br>  Path path = split.getPath();<br>  k.set(path.toString());<br>}<br>/***</li>
<li>map方法是对数据进行处理，并且写出到下一阶段，这里是每次写一个小文件</li>
<li>/<br>@Override<br>protected void map(NullWritable key, BytesWritable value,<pre><code>  Mapper&amp;lt;NullWritable, BytesWritable, Text, BytesWritable&amp;gt;.Context context)
  throws IOException, InterruptedException &#123;
</code></pre>
  context.write(k, value);<br>}<br>}<br></code></pre> ⑷自定义SequenseFileReducer      <img alt="" class="has" height="289" src="https://img-blog.csdnimg.cn/20190804100533568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="406"> ⑸创建SequenseFileDriver <pre class="has"><code class="language-java">package com.kgf.mapreduce.inputformat;</li>
</ul>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.BytesWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</p>
<p>public class SequenseFileDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1:获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:设置jar路径
    job.setJarByClass(SequenseFileDriver.class);
    
    //3：关联自定义的mapper和reducer
    job.setMapperClass(SequenseFileMapper.class);
    job.setReducerClass(SequenseFileReducer.class);
    
    //4:设置我们自定义的WholeFileInputformat对象,输出为SequenceFileOutputFormat
    job.setInputFormatClass(WholeFileInputformat.class);
    job.setOutputFormatClass(SequenceFileOutputFormat.class);
    
    //5：设置mapper的输出类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(BytesWritable.class);
    
    //6:设置reducer的输出类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(BytesWritable.class);
    
    //7：设置文件的输入输出路径
    FileInputFormat.setInputPaths(job,new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //8：提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre> ⑹效果：       <img alt="" class="has" height="211" src="https://img-blog.csdnimg.cn/20190804102313326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="454">       <img alt="" class="has" height="154" src="https://img-blog.csdnimg.cn/20190804102445966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="543">         </li></p>
<h1 id="三：OutputFormat-接口实现"><a href="#三：OutputFormat-接口实现" class="headerlink" title="三：OutputFormat 接口实现"></a>三：OutputFormat 接口实现</h1><ol>
<li>简介        OutputFormat 是 MapReduce 输出的基类，所有实现 MapReduce 输出都实现了  OutputFormat 接口。下面我们介绍几种常见的 OutputFormat 实现类。 <img alt="" class="has" height="265" src="https://img-blog.csdnimg.cn/20190804113214875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="655">1. 自定义 OutputFormat   <img alt="" class="has" height="227" src="https://img-blog.csdnimg.cn/2019080411325760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="765"><li> 案例：过滤日志及自定义日志输出路径案例（自定义 OutputFormat）   ⑴需求：过滤输入的log日志中是否包含topcheer，包含topcheer的行输出到e:/topcheer.log,否则                 输出到e:/other.log文件中。        <img alt="" class="has" height="268" src="https://img-blog.csdnimg.cn/20190804114012540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="390">   ⑵创建FilterOutputFormat.java用来自定义文件输出         <img alt="" class="has" height="269" src="https://img-blog.csdnimg.cn/20190804120644666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="374">  ⑶创建FilterRecordWriter.java,对输出的数据进行自定义处理          <pre class="has"><code class="language-java">package com.kgf.mapreduce.outputformat;</li>
</ol>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.FSDataOutputStream;<br>import org.apache.hadoop.fs.FileSystem;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.RecordWriter;<br>import org.apache.hadoop.mapreduce.TaskAttemptContext;</p>
<p>/**</p>
<ul>
<li><p>自定义RecordWriter实现类，对输出的数据进行自定义处理</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123;</p>
<p>  private Configuration conf = null;</p>
<p>  FSDataOutputStream topcheerFos = null;</p>
<p>  FSDataOutputStream otherFos = null;<br>  /**</p>
<ul>
<li><p>用来创建输出流相关配置</p>
</li>
<li><p>@param context</p>
</li>
<li><p>@throws IOException</p>
</li>
<li><p>/<br>public FilterRecordWriter(TaskAttemptContext context) throws IOException &#123;<br>  try &#123;</p>
<pre><code>  //1：获取配置信息
  conf = context.getConfiguration();
  //2：获取文件输出流，分别输出到两个不同的路径文件用
  FileSystem fs = FileSystem.get(conf);
  //3:创建两个不同路径的输出流
  topcheerFos = fs.create(new Path(&quot;e:/topcheer.log&quot;));
  otherFos = fs.create(new Path(&quot;e:/other.log&quot;));
</code></pre>
<p>  } catch (Exception e) {</p>
<pre><code>  e.printStackTrace();
</code></pre>
<p>  }<br>}<br>/**</p>
</li>
<li><p>根据输入参数key的内容去判断数据要输出到那个文件路径中</p>
</li>
<li><p>/<br>@Override<br>public void write(Text key, NullWritable arg1) throws IOException, InterruptedException {</p>
<p>  //判断key中是否包含topcheer<br>  if(key.toString().contains(“topcheer”)) {</p>
<pre><code>  topcheerFos.write(key.getBytes());
</code></pre>
<p>  }else {</p>
<pre><code>  otherFos.write(key.getBytes());
</code></pre>
<p>  }<br>}<br>/**</p>
</li>
<li><p>最后我们需要将流关闭</p>
</li>
<li><p>/<br>@Override<br>public void close(TaskAttemptContext arg0) throws IOException, InterruptedException {<br>  //关闭资源<br>  if(topcheerFos!=null) {</p>
<pre><code>  topcheerFos.close();
</code></pre>
<p>  }<br>  if(otherFos!=null) {</p>
<pre><code>  otherFos.close();
</code></pre>
<p>  }<br>}<br>}<br></code></pre> ⑷自定义mapper阶段类FilterMapper        <img alt="" class="has" height="331" src="https://img-blog.csdnimg.cn/20190804121243957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="406">     ⑸自定义reducer类        <img alt="" class="has" height="221" src="https://img-blog.csdnimg.cn/20190804121702752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="465">   ⑹自定义FilterDriver         <pre class="has"><code class="language-java">package com.kgf.mapreduce.outputformat;</p>
</li>
</ul>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class FilterDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1:获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:设置jar
    job.setJarByClass(FilterDriver.class);
    
    //3:关联自定义的mapper和reducer
    job.setMapperClass(FilterMapper.class);
    job.setReducerClass(FilterReducer.class);
    
    //4:设置mapper输出参数类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(NullWritable.class);
    
    //5:设置Reducer输出类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    
    //6:将自定义的输出outputFormat设置进来
    job.setOutputFormatClass(FilterOutputFormat.class);
    
    //7：设置输出路径，虽然我们自定义了 outputformat，但是因为我们的 outputformat 继承自 fileoutputformat， 
    //而 fileoutputformat 要输出一个_SUCCESS 文件，所以，在这还得指定一个输 出目录 .
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //8：提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre> ⑺效果：       <img alt="" class="has" height="138" src="https://img-blog.csdnimg.cn/20190804123037268.png" width="575">       <img alt="" class="has" height="151" src="https://img-blog.csdnimg.cn/20190804123049492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="280">       <img alt="" class="has" height="130" src="https://img-blog.csdnimg.cn/20190804123720268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="378">  ⑻具体流程：       数据–&gt;FilterMapper-&gt;FilterOutputFormat-&gt;FilterRecordWriter中构造方法初始化全局对象—&gt;FilterReducer中接受FilterMapper中       传递的每一行数据并且在输出的时候调用FilterRecordWriter中的write方法，将数据写入到指定的路径下。        </li></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之Reduce join以及map join分布式缓存"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E4%B9%8BReduce%20join%E4%BB%A5%E5%8F%8Amap%20join%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"
    >MapReduce之Reduce join以及map join分布式缓存</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E4%B9%8BReduce%20join%E4%BB%A5%E5%8F%8Amap%20join%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/" class="article-date">
  <time datetime="2021-07-18T14:10:35.209Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce之Reduce join以及map join分布式缓存<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：Reduce join </p>
<ol>
<li>简介：       ⑴原理：               Map 端的主要工作：                        为来自不同表(文件)的 key/value 对打标签以区别不同来源的记录然后用连接字段作为 key，                   其余部分和新加的标志作为 value，最后进行输出。                Reduce 端的主要工作：                        在 reduce 端以连接字段作为 key 的分组已经完成，我们只需要在每一个分组当中将那些来                   源于不同文件的记录(在 map 阶段已经打标志)分开，最后进行合并就 ok 了。         ⑵缺点：                这种方式的缺点很明显就是会造成 map和 reduce 端也就是 shuffle 阶段出现大量的数据传输，效率很低。   1.  案例：reduce 端表合并（数据倾斜）         ⑴需求：               订单数据表 t_order，在order.txt中：                     <img alt="" class="has" height="156" src="https://img-blog.csdnimg.cn/20190805220657652.png" width="435">                商品信息表 t_product，在t_product.txt文件中。                    <img alt="" class="has" height="119" src="https://img-blog.csdnimg.cn/20190805210117522.png" width="247">                将商品信息表中数据根据商品 pid 合并到订单数据表中。 最终数据如下：                 <img alt="" class="has" height="128" src="https://img-blog.csdnimg.cn/20190806205404170.png" width="254"> <li>代码实现     ⑴数据准备             <img alt="" class="has" height="168" src="https://img-blog.csdnimg.cn/20190806205500385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="311"><img alt="" class="has" height="97" src="https://img-blog.csdnimg.cn/20190806205522863.png" width="368">      ⑵思路分析：             a：首先订单表和产品表是多对一的关系，因为每个pid在产品表中是唯一的，而在订单表中可能很多个订单都包含                   这个产品。             b：我们需要定义一个bean对象，如下：                     <img alt="" class="has" height="237" src="https://img-blog.csdnimg.cn/20190806205902974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="314">             c：我们以pId作为Mapper阶段输出的key，其它内容为value                   mapper中整合后的数据如下：                   <img alt="" class="has" height="179" src="https://img-blog.csdnimg.cn/20190806210343656.png" width="227">            d：在ruducer阶段，我们以key（pId）作为条件，一个pId在产品表中只有一条数据，那么就好办了，直接查出产品                  表中对应的名称，将order表中的名称填充上即可输出。      ⑶TableBean对象代码如下：             <pre class="has"><code class="language-java">package com.kgf.mapreduce.reducerJoin;</li>
</ol>
<p>import java.io.DataInput;<br>import java.io.DataOutput;<br>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.Writable;</p>
<p>/***</p>
<ul>
<li><p>定义实体对象</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class TableBean implements Writable &#123;</p>
<p>  /<strong>订单ID</strong>/<br>  private String orderId;<br>  /<strong>产品ID</strong>/<br>  private String pId;<br>  /<strong>产品数量</strong>/<br>  private int amount;<br>  /<strong>产品名称</strong>/<br>  private String pName;<br>  /<strong>表类型：0-订单表，1-产品表</strong>/<br>  private String tableType;</p>
<p>  public TableBean(String orderId, String pId, int amount, String pName, String tableType) &#123;</p>
<pre><code>  super();
  this.orderId = orderId;
  this.pId = pId;
  this.amount = amount;
  this.pName = pName;
  this.tableType = tableType;
</code></pre>
<p>  }</p>
<p>  public TableBean() {</p>
<pre><code>  super();
</code></pre>
<p>  }<br>  /***</p>
<ul>
<li>反序列化方法</li>
<li>/<br>@Override<br>public void readFields(DataInput di) throws IOException {<br>  this.orderId = di.readUTF();<br>  this.pId = di.readUTF();<br>  this.amount = di.readInt();<br>  this.pName = di.readUTF();<br>  this.tableType = di.readUTF();<br>}<br>/***</li>
<li>序列化操作方法</li>
<li>/<br>@Override<br>public void write(DataOutput dot) throws IOException {<br>  dot.writeUTF(orderId);<br>  dot.writeUTF(pId);<br>  dot.writeInt(amount);<br>  dot.writeUTF(pName);<br>  dot.writeUTF(tableType);<br>}</li>
</ul>
<p>  @Override<br>  public String toString() {</p>
<pre><code>  return orderId+&quot;\t&quot;+ pName + &quot;\t&quot;+amount;
</code></pre>
<p>  }</p>
<p>  public String getOrderId() {</p>
<pre><code>  return orderId;
</code></pre>
<p>  }</p>
<p>  public void setOrderId(String orderId) {</p>
<pre><code>  this.orderId = orderId;
</code></pre>
<p>  }</p>
<p>  public String getpId() {</p>
<pre><code>  return pId;
</code></pre>
<p>  }</p>
<p>  public void setpId(String pId) {</p>
<pre><code>  this.pId = pId;
</code></pre>
<p>  }</p>
<p>  public int getAmount() {</p>
<pre><code>  return amount;
</code></pre>
<p>  }</p>
<p>  public void setAmount(int amount) {</p>
<pre><code>  this.amount = amount;
</code></pre>
<p>  }</p>
<p>  public String getpName() {</p>
<pre><code>  return pName;
</code></pre>
<p>  }</p>
<p>  public void setpName(String pName) {</p>
<pre><code>  this.pName = pName;
</code></pre>
<p>  }</p>
<p>  public String getTableType() {</p>
<pre><code>  return tableType;
</code></pre>
<p>  }</p>
<p>  public void setTableType(String tableType) {</p>
<pre><code>  this.tableType = tableType;
</code></pre>
<p>  }<br>}<br></code></pre> ⑷TableMapper类代码： <pre class="has"><code class="language-java">package com.kgf.mapreduce.reducerJoin;</p>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>/***</p>
<ul>
<li><p>   创建mapper类：主要功能如下</p>
</li>
<li><p>   a：获取读取数据来自的标名称</p>
</li>
<li><p>   b：对每一行数据进行切割，将我们需要的数据筛选出来，并且标记来自的文件表</p>
</li>
<li><p>   c：最后将数据写出到reducer</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt; &#123;</p>
<p>  TableBean v = new TableBean();</p>
<p>  Text k = new Text();</p>
<p>  @Override<br>  protected void map(LongWritable key, Text value, Context context)</p>
<pre><code>      throws IOException, InterruptedException &#123;
  //1：获取读取文件的名称
  FileSplit splitFile = (FileSplit) context.getInputSplit();
  String fileName = splitFile.getPath().getName();
  //2：获取一行数据
  String line = value.toString();
  //3：判断表文件，对数据进行切割
  String[] files = line.split(&quot;\t&quot;);
  if(fileName.startsWith(&quot;order&quot;)) &#123;
      //订单表
      v.setOrderId(files[0]);
      v.setpId(files[1]);
      v.setAmount(Integer.parseInt(files[2]));
      v.setpName(&quot;&quot;);
      v.setTableType(&quot;0&quot;);
      k.set(files[1]);
  &#125;else &#123;
      //产品表
      v.setOrderId(&quot;&quot;);
      v.setpId(files[0]);
      v.setAmount(0);
      v.setpName(files[1]);
      v.setTableType(&quot;1&quot;);
      k.set(files[0]);
  &#125;
  //4:写出数据
  context.write(k,v);
</code></pre>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre> ⑸TableReducer类代码： <pre class="has"><code class="language-java">package com.kgf.mapreduce.reducerJoin;</p>
<p>import java.io.IOException;<br>import java.lang.reflect.InvocationTargetException;<br>import java.util.ArrayList;<br>import java.util.List;</p>
<p>import org.apache.commons.beanutils.BeanUtils;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;<br>/***</p>
<ul>
<li><pre><code>创建TableReducer类：
</code></pre>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; {</p>
<p>  @Override<br>  protected void reduce(Text key, Iterable&lt;TableBean&gt; values,Context context) </p>
<pre><code>      throws IOException, InterruptedException &#123;
  //1：循环所有的values,
  List&amp;lt;TableBean&amp;gt; tbList = new ArrayList&amp;lt;TableBean&amp;gt;();//用来存放订单表数据
  TableBean tBean = new TableBean();//存放产品表数据
  for (TableBean val : values) &#123;
      //2：判断表类型
      if(&quot;0&quot;.equals(val.getTableType())) &#123;//订单表
          try &#123;
              //3：创建一个TableBean对象
              TableBean tb = new TableBean();
              //4：将val拷贝到tb中
              BeanUtils.copyProperties(tb, val);
              tbList.add(tb);//注意：如果我们不进行拷贝会出问题，都是最后一个对象的值，前面的对象数据会被覆盖
          &#125; catch (IllegalAccessException e) &#123;
              e.printStackTrace();
          &#125; catch (InvocationTargetException e) &#123;
              e.printStackTrace();
          &#125;
      &#125;else &#123;//产品表
          try &#123;
              BeanUtils.copyProperties(tBean, val);
          &#125; catch (IllegalAccessException e) &#123;
              e.printStackTrace();
          &#125; catch (InvocationTargetException e) &#123;
              e.printStackTrace();
          &#125;
      &#125;
  &#125;
  //拼接表
  for (TableBean tableBean : tbList) &#123;
      tableBean.setpName(tBean.getpName());
      context.write(tableBean, NullWritable.get());
  &#125;
</code></pre>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre> ⑹TableDriver类代码 <pre class="has"><code class="language-java">package com.kgf.mapreduce.reducerJoin;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class TableDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1：获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    //2:设置jar对象
    job.setJarByClass(TableDriver.class);
    //3:关联mapper和reducer
    job.setMapperClass(TableMapper.class);
    job.setReducerClass(TableReducer.class);
    //4:设置mapper输出参数
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(TableBean.class);
    //5:设置最终输出参数
    job.setOutputKeyClass(TableBean.class);
    job.setOutputValueClass(NullWritable.class);
    //6：设置数据路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    //7:提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>   </li>1.  缺点       上面这种方式中，合并的操作是在 reduce 阶段完成，reduce 端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在 reduce 阶段极易产生数据倾斜 。 <li>  解决方案：map 端实现数据合并 之Map Join    ⑴ 使用场景：           Map Join适用于一张表十分小、一张表很大的场景    。  ⑵优点：         在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。  ⑶具体解决办法：        采用DistributedCache。        a:<strong>在Mapper的setup阶段，将文件读取到缓存集合中。        b:在驱动函数中加载缓存</strong>。  ⑷DistributedCacheDriver类代码： <pre class="has"><code class="language-java">package com.kgf.mapreduce.mapperJoin;</p>
<p>import java.io.IOException;<br>import java.net.URI;<br>import java.net.URISyntaxException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class DistributedCacheDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException &#123;
    //1：获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    //2:设置jar对象
    job.setJarByClass(DistributedCacheDriver.class);
    //3:关联mapper
    job.setMapperClass(DistributedCacheMapper.class);
    //4:设置最终输出参数
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    //5：设置数据路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //6：加载缓存数据
    job.addCacheFile(new URI(&quot;file:///e:/t_product.txt&quot;));
    
    //7：map端join的逻辑不需要reducer阶段，设置reducetask的数量为0
    job.setNumReduceTasks(0);
    
    //8:提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre> ⑸DistributedCacheMapper类： <pre class="has"><code class="language-java">package com.kgf.mapreduce.mapperJoin;</p>
<p>import java.io.BufferedReader;<br>import java.io.File;<br>import java.io.FileInputStream;<br>import java.io.IOException;<br>import java.io.InputStreamReader;<br>import java.net.URI;<br>import java.util.HashMap;<br>import java.util.Map;</p>
<p>import org.apache.commons.lang3.StringUtils;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</p>
<pre><code>Map&amp;lt;String, String&amp;gt; pdMap = new HashMap&amp;lt;String, String&amp;gt;();

/***
 * 这个属于初始化方法，只执行一些，这个我们用来加载缓存中文件数据
 */
@Override
protected void setup(Mapper&amp;lt;LongWritable, Text, Text, NullWritable&amp;gt;.Context context)
        throws IOException, InterruptedException &#123;
    // 1：获取缓存文件
    URI[] cacheFiles = context.getCacheFiles();
    String path = cacheFiles[0].getPath().toString();
    BufferedReader reader = new BufferedReader(
            new InputStreamReader(new FileInputStream(path), &quot;UTF-8&quot;));
    String line = null;
    while(StringUtils.isNoneBlank(line=reader.readLine())) &#123;
        //对一行进行切割
        String[] fields = line.split(&quot;\t&quot;);
        //数据缓存到集合中
        pdMap.put(fields[0],fields[1]);
    &#125;
    //关闭流
    reader.close();
&#125;

Text k = new Text();

@Override
protected void map(LongWritable key, Text value,
        Mapper&amp;lt;LongWritable, Text, Text, NullWritable&amp;gt;.Context context)
        throws IOException, InterruptedException &#123;
    //1：读取一行
    String line = value.toString();
    //2：切割
    String[] fields = line.split(&quot;\t&quot;);
    //3：获取pId
    String pid = fields[1];
    //4：获取pid对应的名称
    String pName = pdMap.get(pid);
    //5：替换掉名称
    line = fields[0]+&quot;\t&quot;+pName+&quot;\t&quot;+fields[2];
    k.set(line);
    context.write(k, NullWritable.get());
&#125;
</code></pre>
<p>}<br></code></pre> ⑹这里我们不需要reducer类，注意：可以没有reducer,但是mapper必须有，效果如下：       <img alt="" class="has" height="210" src="https://img-blog.csdnimg.cn/20190806231953323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="600"> </li></p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之Hadoop序列化,MapTask  工作机制，CombineTextInputFormat 切片机制，Partition 分区，WritableComparable 排序"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E4%B9%8BHadoop%E5%BA%8F%E5%88%97%E5%8C%96,MapTask%20%C2%A0%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%8CCombineTextInputFormat%20%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6%EF%BC%8CPartition%20%E5%88%86%E5%8C%BA%EF%BC%8CWritableComparable%20%E6%8E%92%E5%BA%8F/"
    >MapReduce之Hadoop序列化,MapTask  工作机制，CombineTextInputFormat 切片机制，Partition 分区，WritableComparable 排序</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E4%B9%8BHadoop%E5%BA%8F%E5%88%97%E5%8C%96,MapTask%20%C2%A0%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%8CCombineTextInputFormat%20%E5%88%87%E7%89%87%E6%9C%BA%E5%88%B6%EF%BC%8CPartition%20%E5%88%86%E5%8C%BA%EF%BC%8CWritableComparable%20%E6%8E%92%E5%BA%8F/" class="article-date">
  <time datetime="2021-07-18T14:10:35.199Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce之Hadoop序列化,MapTask  工作机制，CombineTextInputFormat 切片机制，Partition 分区，WritableComparable 排序<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：Hadoop序列化</p>
<ol>
<li>为什么要序列化？  <img alt="" class="has" height="144" src="https://img-blog.csdnimg.cn/20190727221421643.png" width="701">1. 什么是序列化？  <img alt="" class="has" height="166" src="https://img-blog.csdnimg.cn/20190727221450619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="668"> 1.  为什么不用 Java ？  <img alt="" class="has" height="137" src="https://img-blog.csdnimg.cn/20190727221518294.png" width="711">1.   为什么序列化对 Hadoop  很重要？  <img alt="" class="has" height="374" src="https://img-blog.csdnimg.cn/20190727221546511.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="587">1.  常用数据序列化类型  <img alt="" class="has" height="283" src="https://img-blog.csdnimg.cn/20190727221638148.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="550">1.   自定义 bean  对象 实现序列化接口（Writable ）  ⑴自定义 bean 对象要想序列化传输，必须实现序列化接口，需要注意以下 7 项：         a：必须实现 Writable 接口         b：反序列化时，需要反射调用空参构造函数，所以必须有空参构造。         c：重写序列化方法         d：重写反序列化方法         e：注意反序列化的顺序和序列化的顺序完全一致         f：要想把结果显示在文件中，需要重写 toString()，可用”\t”分开，方便后续用         g：如果需要将自定义的 bean 放在 key 中传输，则还需要实现 comparable 接口，因为               mapreduce 框中的 shuffle 过程一定会对 key 进行排序。<li>  案例之流量汇总   ⑴需求：           统计每一个手机号耗费的总上行流量、下行流量、总流量(序列化)   ⑵数据准备(phone_data.txt)，数据格式如下，现在我们只知道每一行倒数第二个是下行流量，       倒数第三个是上行流量，总流量需要自己去算，第二个是手机号。           <img alt="" class="has" height="316" src="https://img-blog.csdnimg.cn/20190727223341939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="557">   ⑶最后输出的数据格式大概如下：         <img alt="" class="has" height="120" src="https://img-blog.csdnimg.cn/20190727223727598.png" width="475">   ⑷基本思路分析如下：         ①Map 阶段                 <img alt="" class="has" height="79" src="https://img-blog.csdnimg.cn/20190727224459494.png" width="426">         ②Reduce 阶段                <img alt="" class="has" height="111" src="https://img-blog.csdnimg.cn/20190727224612980.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="474">                <img alt="" class="has" height="84" src="https://img-blog.csdnimg.cn/20190727224638661.png" width="474">   ⑸代码实现如下：          ①首先定义一个bean序列化对象                 <pre class="has"><code class="language-java">package com.kgf.mapreduce.flowsum;</li>
</ol>
<p>import java.io.DataInput;<br>import java.io.DataOutput;<br>import java.io.IOException;<br>import org.apache.hadoop.io.Writable;</p>
<p>/**</p>
<ul>
<li><p>首先我们需要实现序列化接口Writable</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class FlowBean implements Writable&#123;</p>
<p>  /<strong>上行流量</strong>/<br>  private long upFlow;<br>  /<strong>下行流量</strong>/<br>  private long downFlow;<br>  /<strong>总流量</strong>/<br>  private long sumFlow;<br>  /**</p>
<ul>
<li>必须要有无参构造方法</li>
<li>反序列化时，需要反射调用空参构造函数，所以必须有空参构造</li>
<li>/<br>public FlowBean() &#123;<br>  super();<br>&#125;</li>
</ul>
<p>  public void setFlowBean(long upFlow, long downFlow) &#123;</p>
<pre><code>  this.upFlow = upFlow;
  this.downFlow = downFlow;
  this.sumFlow = upFlow+downFlow;//总流量等于上行流量加上下行流量
</code></pre>
<p>  }</p>
<p>  public FlowBean(long upFlow, long downFlow) {</p>
<pre><code>  this.upFlow = upFlow;
  this.downFlow = downFlow;
  this.sumFlow = upFlow+downFlow;//总流量等于上行流量加上下行流量
</code></pre>
<p>  }</p>
<p>  /***</p>
<ul>
<li>这个是序列化方法:这个方法其实就是mapper阶段向Reduce阶段写数据的过程</li>
<li>/<br>@Override<br>public void write(DataOutput out) throws IOException {<br>  //按照顺序依次将数据写入<br>  out.writeLong(upFlow);<br>  out.writeLong(downFlow);<br>  out.writeLong(sumFlow);<br>}</li>
</ul>
<p>  /**</p>
<ul>
<li>这个是反序列化方法</li>
<li>/<br>@Override<br>public void readFields(DataInput in) throws IOException {<br>  //这个反序列化顺序要和上面序列化顺序保持一致<br>  this.upFlow = in.readLong();<br>  this.downFlow = in.readLong();<br>  this.sumFlow = in.readLong();<br>}</li>
</ul>
<p>  @Override<br>  public String toString() {</p>
<pre><code>  return upFlow + &quot;\t&quot; + downFlow + &quot;\t&quot; + sumFlow;//可用”\t”分开，方便后续用
</code></pre>
<p>  }</p>
<p>  public long getUpFlow() {</p>
<pre><code>  return upFlow;
</code></pre>
<p>  }</p>
<p>  public void setUpFlow(long upFlow) {</p>
<pre><code>  this.upFlow = upFlow;
</code></pre>
<p>  }</p>
<p>  public long getDownFlow() {</p>
<pre><code>  return downFlow;
</code></pre>
<p>  }</p>
<p>  public void setDownFlow(long downFlow) {</p>
<pre><code>  this.downFlow = downFlow;
</code></pre>
<p>  }</p>
<p>  public long getSumFlow() {</p>
<pre><code>  return sumFlow;
</code></pre>
<p>  }</p>
<p>  public void setSumFlow(long sumFlow) {</p>
<pre><code>  this.sumFlow = sumFlow;
</code></pre>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre>     ⑵自定义mapper对象 <pre class="has"><code class="language-java">package com.kgf.mapreduce.flowsum;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>/**</p>
<ul>
<li><p>继承Mapper接口：定义输入和输出参数</p>
</li>
<li><p>输入参数：第一个是数据行号，第二个是一行数据</p>
</li>
<li><p>输出参数：第一个是手机号，第二个是自定义的实体对象</p>
</li>
<li><p>@author kgf</p>
</li>
<li></li>
<li><p>/<br>public class FlowMapper extends Mapper&lt;LongWritable,Text, Text,FlowBean&gt;&#123;</p>
<p>  //定义输出参数<br>  Text k = new Text();<br>  FlowBean v = new FlowBean();</p>
<p>  @Override<br>  protected void map(LongWritable key, Text value,Context context)</p>
<pre><code>      throws IOException, InterruptedException &#123;
  
  //1:获取一行数据
  String line = value.toString();
  //2:对数据进行切割，这里数据间的是以制表符分割的，就是tab
  String[] fields = line.split(&quot;\t&quot;);
  //3:获取我们需要的数据
  String phoneNum = fields[1];//手机号
  long upFlow = Long.parseLong(fields[fields.length-3]);//上行流量
  long downFlow = Long.parseLong(fields[fields.length-2]);//下行流量
  //封装数据
  v.setFlowBean(upFlow,downFlow);
  k.set(phoneNum);
  
  //写出数据
  context.write(k, v);
</code></pre>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre>    ⑶自定义FlowReducer <pre class="has"><code class="language-java">package com.kgf.mapreduce.flowsum;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>/**</p>
<ul>
<li><p>继承Reducer接口：</p>
</li>
<li><p>输入参数：手机号–&gt;自定义bean对象</p>
</li>
<li><p>输出参数：手机号–&gt;自定义bean对象</p>
</li>
<li><p>@author 86136</p>
</li>
<li></li>
<li><p>/<br>public class FlowReducer extends Reducer&lt;Text, FlowBean, Text, FlowBean&gt;&#123;</p>
<p>  @Override<br>  protected void reduce(Text key, Iterable&lt;FlowBean&gt; values,Context context)</p>
<pre><code>      throws IOException, InterruptedException &#123;
  //1：因为可能存在多条相同的手机号码，所以我们需要对相同的key数据进行数据汇总
  long sum_upFlow = 0;
  long sum_downFlow =0;
  
  //2:求和累加
  for (FlowBean flowBean : values) &#123;
      sum_upFlow+=flowBean.getUpFlow();
      sum_downFlow+=flowBean.getDownFlow();
  &#125;
  
  FlowBean flowBean = new FlowBean(sum_upFlow,sum_downFlow);
  //3：输出数据
  context.write(key, flowBean);
</code></pre>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre>   ⑷自定义FlowDriver <pre class="has"><code class="language-java">package com.kgf.mapreduce.flowsum;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class FlowDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1:获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:设置jar包路径
    job.setJarByClass(FlowDriver.class);
    
    //3:管理自定义的Mapper和Reducer类
    job.setMapperClass(FlowMapper.class);
    job.setReducerClass(FlowReducer.class);
    
    //4:Mapper输出类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(FlowBean.class);
    
    //5：Reducer输出类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(FlowBean.class);
    
    //6：设置输出路径
    FileInputFormat.setInputPaths(job,new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    //7：提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre> ⑸本地测试      a：设置运行的环境变量             <img alt="" class="has" height="163" src="https://img-blog.csdnimg.cn/20190728001704387.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="263">              在输入目录下数据文件已经准备好。               <img alt="" class="has" height="196" src="https://img-blog.csdnimg.cn/20190728001720271.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="428">       b：效果（具体结果自己可以校验一下）             <img alt="" class="has" height="138" src="https://img-blog.csdnimg.cn/20190728001818909.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="301">             <img alt="" class="has" height="313" src="https://img-blog.csdnimg.cn/20190728001908764.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="387">    </li></p>
<h1 id="二：MapTask-工作机制"><a href="#二：MapTask-工作机制" class="headerlink" title="二：MapTask  工作机制"></a>二：MapTask  工作机制</h1><ol>
<li> 简介         maptask 的并行度决定 map 阶段的任务处理并发度，进而影响到整个 job 的处理速度。 一个 job 的 map 阶段 MapTask 并行度（个数），由客户端提交 job 时的切片个数决定1.  MapTask 并行度决定机制         <img alt="" class="has" height="428" src="https://img-blog.csdnimg.cn/2019072808544236.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="528">     <h1 id="三：CombineTextInputFormat-切片机制"><a href="#三：CombineTextInputFormat-切片机制" class="headerlink" title="三：CombineTextInputFormat 切片机制"></a>三：CombineTextInputFormat 切片机制</h1></li>
<li>关于大量小文件的优化策略       <img alt="" class="has" height="113" src="https://img-blog.csdnimg.cn/20190728101550887.png" width="598">1. 优化策略        <img alt="" class="has" height="296" src="https://img-blog.csdnimg.cn/20190728101655451.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="535">1. 案例之大量小文件的切片优化   ⑴现在我们就使用之前的WordCount案例来测试，我们在输入路径下放入多个很小的文件，来使用WordCount       去计算，我们看看默认的切片机制，以及创建了几个maptask？        <img alt="" class="has" height="222" src="https://img-blog.csdnimg.cn/20190728103453328.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="604">       看运行结果和日志：       <img alt="" class="has" height="215" src="https://img-blog.csdnimg.cn/20190728103622869.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="674">      通过上面的日志可以发现，6个很小的文件在计算的时候却分为6个切片，开了6个maptask，肯定是      不太好的，处理效率极其低下，那么我们如何将其合为一个切片呢？    ⑵解决方案，在 WordcountDriver 中增加如下代码         <img alt="" class="has" height="333" src="https://img-blog.csdnimg.cn/20190728105228277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="347">         效果（可以发现只分了一个切片，只开了一个maptask）：         <img alt="" class="has" height="195" src="https://img-blog.csdnimg.cn/2019072810530220.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="504">          <h1 id="四：Partition-分区"><a href="#四：Partition-分区" class="headerlink" title="四：Partition 分区"></a>四：Partition 分区</h1></li>
<li>要求将统计结果按照条件输出到不同文件中（分区）。比如：将统计结果按照手机归属地不同省份输出到不同文件中（分区）。1. 默认分区     <img alt="" class="has" height="226" src="https://img-blog.csdnimg.cn/2019072811064097.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="566">1. 自定义 Partitioner 分区   <img alt="" class="has" height="364" src="https://img-blog.csdnimg.cn/20190728131935896.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="442">1.  在 job 驱动中，设置自定义 partitioner   <img alt="" class="has" height="378" src="https://img-blog.csdnimg.cn/20190728132941914.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="395">   注意：   <img alt="" class="has" height="334" src="https://img-blog.csdnimg.cn/20190728133023471.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="515">1.   运行效果   <img alt="" class="has" height="271" src="https://img-blog.csdnimg.cn/20190728133157912.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="375">         如下：第一个分区都是136的，后面都是根据不同手机号在不同的分区。   <img alt="" class="has" height="189" src="https://img-blog.csdnimg.cn/20190728133232505.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="485"><h1 id="五：WritableComparable-排序"><a href="#五：WritableComparable-排序" class="headerlink" title="五：WritableComparable 排序"></a>五：WritableComparable 排序</h1></li>
<li>简介        排序是 MapReduce 框架中最重要的操作之一。Map Task 和 Reduce Task 均会对数据（按照 key）进行排序。 该操作属于 Hadoop 的默认行为。任何应用程序中的数据均会被排序，而不管逻辑上是否需要。默认排序是按照字 典顺序排序，且实现该排序的方法是快速排序。        对于 Map Task，它会将处理的结果暂时放到一个缓冲区中，当缓冲区使用率达到一定阈值后，再对缓冲区中的 数据进行一次排序，并将这些有序数据写到磁盘上，而当数据处理完毕后，它会对磁盘上所有文件进行一次合并， 以将这些文件合并成一个大的有序文件。        对于 Reduce Task，它从每个 Map Task 上远程拷贝相应的数据文件，如果文件大小超过一定阈值，则放到磁盘上， 否则放到内存中。如果磁盘上文件数目达到一定阈值，则进行一次合并以生成一个更大文件；如果内存中文件大小或者 数目超过一定阈值，则进行一次合并后将数据写到磁盘上。当所有数据拷贝完毕后，Reduce Task 统一对内存和磁盘上 的所有数据进行一次合并。1.  排序的分类      ⑴部分排序                 MapReduce 根据输入记录的键对数据集排序。保证输出的每个文件内部排序。      ⑵全排序                 <img alt="" class="has" height="229" src="https://img-blog.csdnimg.cn/20190728144810792.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="524">       ⑶辅助排序：（GroupingComparator 分组）                <img alt="" class="has" height="152" src="https://img-blog.csdnimg.cn/2019072814493740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="517">       ⑷二次排序                在自定义排序过程中，如果 compareTo 中的判断条件为两个即为二次排序。       ⑸自定义排序 WritableComparable            bean 对象实现 WritableComparable 接口重写 compareTo 方法，就可以实现排序 <li>案例之手机流量按总流量进行倒序排序       ⑴数据准备               <img alt="" class="has" height="313" src="https://img-blog.csdnimg.cn/20190728154515743.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="415">       ⑵自定义Bean对象实现WritableComparable接口，重写 compareTo 方法 <pre class="has"><code class="language-java">package com.kgf.mapreduce.flowsort;</li>
</ol>
<p>import java.io.DataInput;<br>import java.io.DataOutput;<br>import java.io.IOException;<br>import org.apache.hadoop.io.WritableComparable;</p>
<p>public class FlowSortBean implements WritableComparable&lt;FlowSortBean&gt;&#123;</p>
<pre><code>/**上行流量**/
private long upFlow;
/**下行流量**/
private long downFlow;
/**总流量**/
private long sumFlow;
/**
 * 必须要有无参构造方法
 * 反序列化时，需要反射调用空参构造函数，所以必须有空参构造
 */
public FlowSortBean() &#123;
    super();
&#125;

public void setFlowSortBean(long upFlow, long downFlow) &#123;
    this.upFlow = upFlow;
    this.downFlow = downFlow;
    this.sumFlow = upFlow+downFlow;//总流量等于上行流量加上下行流量
&#125;

public FlowSortBean(long upFlow, long downFlow) &#123;
    this.upFlow = upFlow;
    this.downFlow = downFlow;
    this.sumFlow = upFlow+downFlow;//总流量等于上行流量加上下行流量
&#125;

/***
 * 这个是序列化方法:这个方法其实就是mapper阶段向Reduce阶段写数据的过程
 */
@Override
public void write(DataOutput out) throws IOException &#123;
    //按照顺序依次将数据写入
    out.writeLong(upFlow);
    out.writeLong(downFlow);
    out.writeLong(sumFlow);
&#125;

/**
 * 这个是反序列化方法
 */
@Override
public void readFields(DataInput in) throws IOException &#123;
    //这个反序列化顺序要和上面序列化顺序保持一致
    this.upFlow = in.readLong();
    this.downFlow = in.readLong();
    this.sumFlow = in.readLong();
&#125;

@Override
public String toString() &#123;
    return upFlow + &quot;\t&quot; + downFlow + &quot;\t&quot; + sumFlow;//可用”\t”分开，方便后续用
&#125;

public long getUpFlow() &#123;
    return upFlow;
&#125;

public void setUpFlow(long upFlow) &#123;
    this.upFlow = upFlow;
&#125;

public long getDownFlow() &#123;
    return downFlow;
&#125;

public void setDownFlow(long downFlow) &#123;
    this.downFlow = downFlow;
&#125;

public long getSumFlow() &#123;
    return sumFlow;
&#125;

public void setSumFlow(long sumFlow) &#123;
    this.sumFlow = sumFlow;
&#125;

/**
 * 按照总流量的倒序排序
 */
@Override
public int compareTo(FlowSortBean o) &#123;
    return (int) (this.sumFlow-o.getSumFlow());
&#125;
</code></pre>
<p>}<br></code></pre>   ⑶自定义Mapper类 <pre class="has"><code class="language-java">package com.kgf.mapreduce.flowsort;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>/**</p>
<ul>
<li><p>我们需要将排序的参数作为KEY</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class FlowSortMapper extends Mapper&lt;LongWritable,Text,FlowSortBean,Text&gt;&#123;</p>
<p>  FlowSortBean k = new FlowSortBean();<br>  Text v = new Text();</p>
<p>  @Override<br>  protected void map(LongWritable key, Text value,Context context)</p>
<pre><code>      throws IOException, InterruptedException &#123;
  //1：获取一行数据
  String line = value.toString();
  //2:切割数据
  String[] fields = line.split(&quot;\t&quot;);
  //3:封装对象
  long upFlow = Long.parseLong(fields[1]);//上行流量
  long downFlow = Long.parseLong(fields[2]);//下行流量
  
  k.setFlowSortBean(upFlow, downFlow);
  v.set(fields[0]);
  
  //4：将数据写出
  context.write(k, v);
</code></pre>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre>   ⑷自定义Reducer类        <img alt="" class="has" height="259" src="https://img-blog.csdnimg.cn/2019072816005613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="429">    ⑸自定义Driver类 <pre class="has"><code class="language-java">package com.kgf.mapreduce.flowsort;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class FlowSortDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1:获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:设置jar包路径
    job.setJarByClass(FlowSortDriver.class);
    
    //3:设置自定义的mapper和Reducer类
    job.setMapperClass(FlowSortMapper.class);
    job.setReducerClass(FlowSortReducer.class);
    
    //4:设置mapper输出的k,v类型
    job.setMapOutputKeyClass(FlowSortBean.class);
    job.setMapOutputValueClass(Text.class);
    
    //5:设置reducer的输出类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(FlowSortBean.class);
    
    //6：设置输入输出路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job,new Path(args[1]));
    
    //7:提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>   ⑹运行效果         <img alt="" class="has" height="235" src="https://img-blog.csdnimg.cn/20190728160205523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="460">        <img alt="" class="has" height="365" src="https://img-blog.csdnimg.cn/20190728160235175.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="488"> </li>1.  在上面排序的基础上，将不同省份的手机号输出到不同的文件中，并且内部排序    ⑴增加分区类           <img alt="" class="has" height="297" src="https://img-blog.csdnimg.cn/20190728162325650.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="377">             ⑵在Driver中引入自定义的分区          <img alt="" class="has" height="365" src="https://img-blog.csdnimg.cn/20190728162411687.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="361">   ⑶效果：         <img alt="" class="has" height="209" src="https://img-blog.csdnimg.cn/20190728162438778.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="407">        里面的每个文件内容都排了序：        <img alt="" class="has" height="258" src="https://img-blog.csdnimg.cn/20190728162529395.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="538"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce实现寻找共同好友"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E5%AE%9E%E7%8E%B0%E5%AF%BB%E6%89%BE%E5%85%B1%E5%90%8C%E5%A5%BD%E5%8F%8B/"
    >MapReduce实现寻找共同好友</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E5%AE%9E%E7%8E%B0%E5%AF%BB%E6%89%BE%E5%85%B1%E5%90%8C%E5%A5%BD%E5%8F%8B/" class="article-date">
  <time datetime="2021-07-18T14:10:35.192Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce实现寻找共同好友<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—title: MapReduce实现寻找共同好友<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—1. 需求：给出A-O个人中每个人的好友列表，求出哪些人两两之间有共同好友，以及他们的共同好友都有谁。    注意：这些人好友都是单向的，可能A是B的好友，但是B不一定是A的好友，这种类似的微博的关注，               A关注B，但是B不一定关注了A。1. 原始文件如下：   <img alt="" class="has" height="345" src="https://img-blog.csdnimg.cn/20190811145226975.png" width="289">  1. 要求输出的格式如下：  <img alt="" class="has" height="266" src="https://img-blog.csdnimg.cn/20190811145317894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="328">1. 思路分析：    ⑴我们从上面可以现在我们知道A-O每个人拥有哪些好友，但是我们现在是要找出两两之间的人有哪些共同好友。那么        我们可以逆向思维，第一步找出哪些好友拥有A,哪些好友拥有B…..依次找出，结果如下：        <img alt="" class="has" height="264" src="https://img-blog.csdnimg.cn/20190811145923175.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="320">    ⑵通过得出上面的数据后，我们可以对后面的好友进行排序，避免重复，将 “拥有这名朋友的所有人”进行两两配对，并将配对后的         字符串当做键，“朋友”当做值输出，即输出&lt;人-人，共同朋友&gt;        <img alt="" class="has" height="362" src="https://img-blog.csdnimg.cn/20190811150157279.png" width="206"> <li>代码实现，通过两次job运算  a：FriendMapper01         <pre class="has"><code class="language-java">package com.kgf.mapreduce.friend;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>public class FriendMapper01 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</p>
<pre><code>Text k  =new Text();
Text v  =new Text();

@Override
protected void map(LongWritable key, Text value,Context context)
        throws IOException, InterruptedException &#123;
    //1：获取一行数据
    String line = value.toString();
    //2：对一行数据进行切割
    String[] fields = line.split(&quot;:&quot;);
    String person = fields[0];
    String[] friends = fields[1].split(&quot;,&quot;);
    for (String friend : friends) &#123;
        k.set(friend);
        v.set(person);
        context.write(k, v);
    &#125;
&#125;
</code></pre>
<p>}<br></code></pre> b：FriendReducer       <pre class="has"><code class="language-java">package com.kgf.mapreduce.friend;</p>
<p>import java.io.IOException;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>public class FriendReducer extends Reducer&lt;Text, Text, Text, Text&gt; &#123;</p>
<pre><code>@Override
protected void reduce(Text key, Iterable&amp;lt;Text&amp;gt; values,Context context)
        throws IOException, InterruptedException &#123;
    
    StringBuffer sb = new StringBuffer();
    //1：获取哪些好友都有对应的人
    for (Text text : values) &#123;
        sb.append(text.toString()+&quot;,&quot;);
    &#125;
    sb.deleteCharAt(sb.length()-1);
    context.write(key, new Text(sb.toString()));
&#125;
</code></pre>
<p>}<br></code></pre> c：FriendDriver01       <pre class="has"><code class="language-java">package com.kgf.mapreduce.friend;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class FriendDriver01 &#123;</p>
<pre><code>public static void main(String[] args) throws Exception &#123;
    //1：获取Job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:设置jar
    job.setJarByClass(FriendDriver01.class);
    
    //3:关联Mapper和reducer
    job.setMapperClass(FriendMapper01.class);
    job.setReducerClass(FriendReducer.class);
    
    //4:设置mapper输出参数
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(Text.class);
    
    //5：设置最终输出
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);
    
    //6:设置文件输入输出路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //7：提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre> d：FriengMapper02 <pre class="has"><code class="language-java">package com.kgf.mapreduce.friend;</p>
<p>import java.io.IOException;<br>import java.util.Arrays;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>public class FriengMapper02 extends Mapper&lt;LongWritable, Text, Text, Text&gt;&#123;</p>
<pre><code>@Override
protected void map(LongWritable key, Text value,Context context)
        throws IOException, InterruptedException &#123;
    //1：获取一行
    String line = value.toString();
    //2：切割数据
    String[] fileds = line.split(&quot;\t&quot;);
    String friend = fileds[0];
    String[] persons = fileds[1].split(&quot;,&quot;);
    Arrays.sort(persons);//排序
    for (int i = 0; i &amp;lt; persons.length; i++) &#123;
        for (int j = i+1; j &amp;lt; persons.length; j++) &#123;
            context.write(new Text(persons[i]+&quot;-&quot;+persons[j]),new Text(friend));
        &#125;
    &#125;
&#125;
</code></pre>
<p>}<br></code></pre> e：FriendReducer2 <pre class="has"><code class="language-java">package com.kgf.mapreduce.friend;</p>
<p>import java.io.IOException;<br>import java.util.HashSet;</p>
<p>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>public class FriendReducer2 extends Reducer&lt;Text, Text, Text, Text&gt; &#123;</p>
<pre><code>@Override
protected void reduce(Text key, Iterable&amp;lt;Text&amp;gt; values,Context context)
        throws IOException, InterruptedException &#123;
    
    StringBuffer sb = new StringBuffer();
    HashSet&amp;lt;String&amp;gt; set = new HashSet&amp;lt;String&amp;gt;();
    
    for (Text value : values) &#123;
        String v = value.toString();
        if(!set.contains(v)) &#123;
            set.add(v);
            sb.append(v).append(&quot;,&quot;);
        &#125;
    &#125;
    sb.deleteCharAt(sb.length()-1);
    context.write(key, new Text(sb.toString()));
&#125;
</code></pre>
<p>}<br></code></pre> f：FriendDriver2 <pre class="has"><code class="language-java">package com.kgf.mapreduce.friend;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class FriendDriver2 &#123;</p>
<pre><code>public static void main(String[] args) throws Exception &#123;
    //1：获取Job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:设置jar
    job.setJarByClass(FriendDriver2.class);
    
    //3:关联Mapper和reducer
    job.setMapperClass(FriengMapper02.class);
    job.setReducerClass(FriendReducer2.class);
    
    //4:设置mapper输出参数
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(Text.class);
    
    //5：设置最终输出
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(Text.class);
    
    //6:设置文件输入输出路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //7：提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>   </li></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce，MapTask工作流程，Combiner 合并以及二次排序GroupingComparator"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%EF%BC%8CMapTask%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%EF%BC%8CCombiner%20%E5%90%88%E5%B9%B6%E4%BB%A5%E5%8F%8A%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8FGroupingComparator/"
    >MapReduce，MapTask工作流程，Combiner 合并以及二次排序GroupingComparator</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%EF%BC%8CMapTask%E5%B7%A5%E4%BD%9C%E6%B5%81%E7%A8%8B%EF%BC%8CCombiner%20%E5%90%88%E5%B9%B6%E4%BB%A5%E5%8F%8A%E4%BA%8C%E6%AC%A1%E6%8E%92%E5%BA%8FGroupingComparator/" class="article-date">
  <time datetime="2021-07-18T14:10:35.183Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce，MapTask工作流程，Combiner 合并以及二次排序GroupingComparator<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：MapTask工作流程</p>
<ol>
<li>简介      <img alt="" class="has" height="337" src="https://img-blog.csdnimg.cn/20190728221249534.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="539">1.  详细流程如下    ⑴Read阶段：              Map Task 通过用户编写的 RecordReader，从输入 InputSplit 中解析出一个个 key/value。    ⑵Map 阶段：               该节点主要是将解析出的 key/value 交给用户编写 map()函数处理，并产生一系列新的 key/value。    ⑶Collect 收集阶段：               在用户编写 map()函数中，当数据处理完成后，一般会调用OutputCollector.collect()输出结果。       在该函数内部，它会将生成的 key/value 分区（调用Partitioner），并写入一个环形内存缓冲区中。    ⑷Spill 阶段：                即“溢写”，当环形缓冲区满后，MapReduce 会将数据写到本地磁盘上，生成一个临时文件。        需要注意的是，将数据写入本地磁盘之前，先要对数据进行一次本地排序，并在必要时对数据进        行合并、压缩等操作。                溢写阶段详情：                         a：利用快速排序算法对缓存区内的数据进行排序，排序方式是，先按照分区编号partition                               进行排序，然后按照 key 进行排序。这样，经过排序后，数据以分区为单位聚集在一起，                               且同一分区内所有数据按照 key 有序。                         b：按照分区编号由小到大依次将每个分区中的数据写入任务工作目录下的临时文件                               output/spillN.out（N 表示当前溢写次数）中。如果用户设置了 Combiner，则写入文件之                                前，对每个分区中的数据进行一次聚集操作。                         c：将分区数据的元信息写到内存索引数据结构 SpillRecord 中，其中每个分区的元信息包括                              在临时文件中的偏移量、压缩前数据大小和压缩后数据大小。如果当前内存索引大小超过                              1MB，则将内存索引写到文件 output/spillN.out.index 中。    ⑸Combine 阶段：             当所有数据处理完成后，MapTask 对所有临时文件进行一次合并以确保最终只会生成一个数据文件。             当所有数据处理完后，MapTask 会将所有临时文件合并成一个大文件，并保存到文件output/file.out 中，        同时生成相应的索引文件 output/file.out.index。              在进行文件合并过程中，MapTask 以分区为单位进行合并。对于某个分区，它将采用多轮递归合并的        方式。每轮合并 io.sort.factor（默认 100）个文件，并将产生的文件重新加入待合并列表中，对文件排序后，        重复以上过程，直到最终得到一个大文件。              让每个 MapTask 最终只生成一个数据文件，可避免同时打开大量文件和同时读取大量小文件产生的随机        读取带来的开销。         <h1 id="二：MapReduce-工作流程"><a href="#二：MapReduce-工作流程" class="headerlink" title="二：MapReduce  工作流程"></a>二：MapReduce  工作流程</h1></li>
<li>流程示意图如下：      <img alt="" class="has" height="335" src="https://img-blog.csdnimg.cn/20190728223420512.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="508">      <img alt="" class="has" height="345" src="https://img-blog.csdnimg.cn/20190728223526161.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="504">1.  流程详解如下：  ⑴maptask 收集我们的 map()方法输出的 kv 对，放到内存缓冲区中  ⑵从内存缓冲区不断溢出本地磁盘文件，可能会溢出多个文件  ⑶多个溢出文件会被合并成大的溢出文件  ⑷在溢出过程中，及合并的过程中，都要调用 partitioner 进行分区和针对 key 进行排序  ⑸reducetask 根据自己的分区号，去各个 maptask 机器上取相应的结果分区数据 。  ⑹reducetask 会取到同一个分区的来自不同 maptask 的结果文件，reducetask 会将这些      文件再进行合并（归并排序）  ⑺合并成大文件后，shuffle 的过程也就结束了，后面进入 reducetask 的逻辑运算过程          （从文件中取出一个一个的键值对 group，调用用户自定义的 reduce()方法）   注意：         Shuffle 中的缓冲区大小会影响到 mapreduce 程序的执行效率，原则上说，缓冲区越大，  磁盘 io 的次数越少，执行速度就越快。缓冲区的大小可以通过参数调整，参数：io.sort.mb 默认 100M。<h1 id="三：Combiner-合并"><a href="#三：Combiner-合并" class="headerlink" title="三：Combiner 合并"></a>三：Combiner 合并</h1></li>
<li>简介       <img alt="" class="has" height="300" src="https://img-blog.csdnimg.cn/20190729220522148.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="583">       <img alt="" class="has" height="116" src="https://img-blog.csdnimg.cn/20190729220719373.png" width="635">1.  案例：我们在wordcount案例中使用Conbiner    ⑴数据准备            <img alt="" class="has" height="215" src="https://img-blog.csdnimg.cn/2019072922243974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="469">    ⑵使用原本的wordcount代码看看日志           <img alt="" class="has" height="265" src="https://img-blog.csdnimg.cn/20190729222738153.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="363">    ⑶自定义一个 combiner 继承 Reducer，重写 reduce 方法           <img alt="" class="has" height="300" src="https://img-blog.csdnimg.cn/20190729222824707.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="413">     ⑷在 job 驱动类中设置：          <img alt="" class="has" height="347" src="https://img-blog.csdnimg.cn/20190729223022718.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="358">     ⑸效果：         <img alt="" class="has" height="263" src="https://img-blog.csdnimg.cn/20190729223311563.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="339">               ⑹其实我们可以发现Combiner的代码和Reducer的代码一样，只不过执行的位置不同：          <img alt="" class="has" height="331" src="https://img-blog.csdnimg.cn/20190729223603934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="440">          所以我们直接在Driver中将Combiner指定reducer也可以：          <img alt="" class="has" height="355" src="https://img-blog.csdnimg.cn/20190729223745619.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="344"><h1 id="四：辅助-排序和二次排序案例（GroupingComparator）"><a href="#四：辅助-排序和二次排序案例（GroupingComparator）" class="headerlink" title="四：辅助 排序和二次排序案例（GroupingComparator）"></a>四：辅助 排序和二次排序案例（GroupingComparator）</h1></li>
<li> 需求：     <img alt="" class="has" height="277" src="https://img-blog.csdnimg.cn/20190730213326408.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="521">1. 数据准备：    <img alt="" class="has" height="114" src="https://img-blog.csdnimg.cn/20190731223724507.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="274"><img alt="" class="has" height="111" src="https://img-blog.csdnimg.cn/20190731223812489.png" width="322">1.  思路分析   ⑴在Mapper中处理的事情：         a：获取每一行输入的数据         b：对数据进行切割，只需要订单号，金额         c：将切割好的数据分装到自定义的bean当中，并且对订单号以及金额进行排序，都是从大到小。               因为在后面定义分组的时候，只会传递一个key给reducer,这时候我们取最大金额的订单。    ⑵自定义分区，从mapper端传递过来的数据，只要订单号一样我们就分到一个分区之中，最后         3中编号的订单分到3个分区文件中。    ⑶自定义groupingcomparator分组，获取相同订单号的数据，并且取第一个订单号数据发送给reducer.    ⑷最后reducer分别把不同订单号的第一条金额最大的数据写出。<li>代码实现如下：   ⑴自定义bean，并且实现订单号以及金额排序           <pre class="has"><code class="language-java">package com.kgf.mapreduce.order;</li>
</ol>
<p>import java.io.DataInput;<br>import java.io.DataOutput;<br>import java.io.IOException;<br>import org.apache.hadoop.io.WritableComparable;</p>
<p>/***</p>
<ul>
<li><p>序列化排序自定义的bean对象</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class OrderBean implements WritableComparable&lt;OrderBean&gt;&#123;</p>
<p>  /<strong>订单号</strong>/<br>  private int orderId;<br>  /<strong>价格</strong>/<br>  private double price;</p>
<p>  public OrderBean(int orderId, double price) &#123;</p>
<pre><code>  super();
  this.orderId = orderId;
  this.price = price;
</code></pre>
<p>  }<br>  public OrderBean() {</p>
<pre><code>  super();
</code></pre>
<p>  }<br>  /***</p>
<ul>
<li>反序列化操作</li>
<li>/<br>@Override<br>public void readFields(DataInput input) throws IOException {<br>  this.orderId = input.readInt();<br>  this.price = input.readDouble();<br>}<br>/***</li>
<li>序列化操作</li>
<li>/<br>@Override<br>public void write(DataOutput output) throws IOException {<br>  output.writeInt(orderId);<br>  output.writeDouble(price);<br>}<br>public int getOrderId() {<br>  return orderId;<br>}<br>public void setOrderId(int orderId) {<br>  this.orderId = orderId;<br>}<br>public double getPrice() {<br>  return price;<br>}<br>public void setPrice(double price) {<br>  this.price = price;<br>}<br>@Override<br>public String toString() {<br>  return orderId + “\t” + price;<br>}<br>@Override<br>public int compareTo(OrderBean o) {<br>  int result = 0;<br>  if(this.orderId&gt;o.getOrderId()) {<pre><code>  result = 1;
</code></pre>
  }else if(this.orderId&lt;o.getOrderId()) {<pre><code>  result = -1;
</code></pre>
  }else {<pre><code>  if(this.price&amp;gt;o.getPrice()) &#123;
      result = -1;
  &#125;else if(this.price&amp;lt;o.getPrice()) &#123;
      result = 1;
  &#125;
</code></pre>
  }<br>  return result;<br>}<br>}<br></code></pre> ⑵自定义OrderMapper对数据进行切割 <pre class="has"><code class="language-java">package com.kgf.mapreduce.order;</li>
</ul>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;<br>/***</p>
<ul>
<li><p>继承Mapper接口,对读取的文件数据进行切割</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class OrderMapper extends Mapper&lt;LongWritable, Text, OrderBean, NullWritable&gt;&#123;</p>
<p>  OrderBean k = new OrderBean();</p>
<p>  @Override<br>  protected void map(LongWritable key, Text value,Context context)</p>
<pre><code>      throws IOException, InterruptedException &#123;
  System.out.println(&quot;===========OrderMapper BEGIN=============&quot;);
  //获取一行数据
  String line = value.toString();
  //切割数据
  String[] fields = line.split(&quot;\t&quot;);
  System.out.println(&quot;[fields:]&quot;+fields);
  //封装数据对象
  k.setOrderId(Integer.valueOf(fields[0]));
  k.setPrice(Double.parseDouble(fields[2]));
  //输出
  context.write(k, NullWritable.get());
  System.out.println(&quot;===========OrderMapper END=============&quot;);
</code></pre>
<p>  }<br>}<br></code></pre>  ⑶自定义分区，将不同订单号的数据写入到不同分区 <pre class="has"><code class="language-java">package com.kgf.mapreduce.order;</p>
</li>
</ul>
<p>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.mapreduce.Partitioner;</p>
<p>/***</p>
<ul>
<li><p>继承分区函数，分为多个区</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class OrderPartitioner extends Partitioner&lt;OrderBean,NullWritable&gt;&#123;</p>
<p>  /**</p>
<ul>
<li>注意：这个numberPartitions就是Driver中job.setNumReduceTasks(3);</li>
<li>/<br>@Override<br>public int getPartition(OrderBean key, NullWritable value, int numberPartitions) &#123;<br>  return (key.getOrderId() &amp; Integer.MAX_VALUE) % numberPartitions;<br>&#125;</li>
</ul>
</li>
</ul>
<p>&#125;<br></code></pre> ⑷自定义分组，将相同订单号的第一条金额最大的数据写入到reducer中 <pre class="has"><code class="language-java">package com.kgf.mapreduce.order;</p>
<p>import org.apache.hadoop.io.WritableComparable;<br>import org.apache.hadoop.io.WritableComparator;</p>
<p>/***</p>
<ul>
<li>自定义GroupingComparator进行分组,WritableComparator是一个类 这个类是用于mapreduce编程模型中的比较 排序 .</li>
<li>mapreduce中有两次排序 一次是 在环形缓冲区域之中进行分区 排序,还有一次是数据在reduce端获取文件之后进行分组</li>
<li>它是用来给Key分组的,</li>
<li>@author kgf</li>
<li></li>
<li>/<br>public class OrderGroupingComparator extends WritableComparator&#123;<br>   /***<pre><code>           * 无参构造子 必须调用父类的构造子 不然会报空指针 未初始化 buffer
*/
</code></pre>
  public OrderGroupingComparator() {<pre><code>  super(OrderBean.class,true);
</code></pre>
  }<br>  /**<ul>
<li>我们这里通过比较orderId分组，相同的一组</li>
<li>/<br>@SuppressWarnings(“rawtypes”)<br>@Override<br>public int compare(WritableComparable a, WritableComparable b) {<br>  System.out.println(“======OrderGroupingComparator begin=======”);<br>  OrderBean aBean = (OrderBean) a;<br>  OrderBean bBean = (OrderBean) b;<br>  System.out.println(“[aBean:]”+aBean);<br>  System.out.println(“[bBean:]”+bBean);<br>  int result = 0;<br>  if(bBean.getOrderId()&gt;aBean.getOrderId()) {<pre><code>  result = 1;
</code></pre>
  }else if(bBean.getOrderId()&lt;aBean.getOrderId()) {<pre><code>  result = -1;
</code></pre>
  }else {<pre><code>  result = 0;
</code></pre>
  }<br>  System.out.println(“======OrderGroupingComparator end=======”);<br>  return result;<br>}<br>}<br></code></pre>  ⑸自定义reducer，写出不同订单号的金额最大的数据 <pre class="has"><code class="language-java">package com.kgf.mapreduce.order;</li>
</ul>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>/***</p>
<ul>
<li></li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class OrderReducer extends Reducer&lt;OrderBean, NullWritable, OrderBean, NullWritable&gt;&#123;</p>
<p>  @Override<br>  protected void reduce(OrderBean key, Iterable&lt;NullWritable&gt; value,Context context)</p>
<pre><code>      throws IOException, InterruptedException &#123;
  System.out.println(&quot;=========OrderReducer=====begin=========&quot;);
  context.write(key, NullWritable.get());
  System.out.println(&quot;=========OrderReducer=====END=========&quot;);
</code></pre>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre> ⑹OrderDriver启动服务类 <pre class="has"><code class="language-java">package com.kgf.mapreduce.order;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class OrderDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    
    //1：获取job配置信息
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    //2:设置jar包加载路径
    job.setJarByClass(OrderDriver.class);
    //3:加载自定义的mapper和reducer
    job.setMapperClass(OrderMapper.class);
    job.setReducerClass(OrderReducer.class);
    //4:设置mapper的输出类型
    job.setMapOutputKeyClass(OrderBean.class);
    job.setMapOutputValueClass(NullWritable.class);
    //5：设置reducer的输出类型
    job.setOutputKeyClass(OrderBean.class);
    job.setOutputValueClass(NullWritable.class);
    //6:设置文件输入和输出路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job,new Path(args[1]));
    
    //8：设置分区
    job.setPartitionerClass(OrderPartitioner.class);
    job.setNumReduceTasks(3);
    
    //9:设置自定义分组
    job.setGroupingComparatorClass(OrderGroupingComparator.class);
    
    //7:提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:-1);
&#125;
</code></pre>
<p>}<br></code></pre>   </li>1. OVER</p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Hive知识点入门学习一"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/Hive%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E4%B8%80/"
    >Hive知识点入门学习一</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/Hive%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E4%B8%80/" class="article-date">
  <time datetime="2021-07-18T14:10:35.176Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: Hive知识点入门学习一<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：Hive基本概念</p>
<ol>
<li> 什么是Hive？    <img alt="" class="has" height="298" src="https://img-blog.csdnimg.cn/20190814222311472.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="620">1. Hive优缺点  ⑴优点           <img alt="" class="has" height="208" src="https://img-blog.csdnimg.cn/20190815214618589.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="616">  ⑵缺点           <img alt="" class="has" height="264" src="https://img-blog.csdnimg.cn/20190815214836193.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="622">1. Hive 架构原理 <img alt="" class="has" height="566" src="https://img-blog.csdnimg.cn/201908152155550.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="611"><img alt="" class="has" height="412" src="https://img-blog.csdnimg.cn/20190815215626780.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="610"><img alt="" class="has" height="182" src="https://img-blog.csdnimg.cn/2019081521564683.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="610"><img alt="" class="has" height="112" src="https://img-blog.csdnimg.cn/20190815215701849.png" width="610">1. Hive 和数据库比较  由于 Hive 采用了类似 SQL 的查询语言 HQL(Hive Query Language)，因此很容易 将 Hive 理解为数据库。其实从结构上来看，Hive 和数据库除了拥有类似的查询语言，再无 类似之处。数据库可以用在 Online 的应用中，但是 Hive 是为数据仓库而设计的，类似mysql, Oracle等常规数据库一般用于增删改查，但是Hive一般只用于查询。清楚这一点，有助于从应 用角度理解 Hive 的特性。 ⑴查询语言         <img alt="" class="has" height="61" src="https://img-blog.csdnimg.cn/20190815221532680.png" width="546"> ⑵数据存储位置          <img alt="" class="has" height="63" src="https://img-blog.csdnimg.cn/2019081522160170.png" width="551"> ⑶数据更新          <img alt="" class="has" height="128" src="https://img-blog.csdnimg.cn/2019081522163091.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="553">  ⑷ 索引         <img alt="" class="has" height="209" src="https://img-blog.csdnimg.cn/20190815221713667.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="559"> ⑸执行         <img alt="" class="has" height="61" src="https://img-blog.csdnimg.cn/20190815221748180.png" width="558"> ⑹执行延迟         <img alt="" class="has" height="167" src="https://img-blog.csdnimg.cn/2019081522181594.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="555"> ⑺可扩展性         <img alt="" class="has" height="136" src="https://img-blog.csdnimg.cn/20190815221845638.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="556"> ⑻数据规模         <img alt="" class="has" height="62" src="https://img-blog.csdnimg.cn/20190815221911671.png" width="572"><h1 id="二：Hive-安装环境准备以及基本操作"><a href="#二：Hive-安装环境准备以及基本操作" class="headerlink" title="二：Hive 安装环境准备以及基本操作"></a>二：Hive 安装环境准备以及基本操作</h1></li>
<li>Hive各个版本下载地址 ：****  <img alt="" class="has" height="343" src="https://img-blog.csdnimg.cn/20190816213210468.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="435">        1. 使用hive的版本  <img alt="" class="has" height="42" src="https://img-blog.csdnimg.cn/2019081621555789.png" width="311">1. 将软件包上传到/opt/software目录下  <img alt="" class="has" height="166" src="https://img-blog.csdnimg.cn/2019081621592082.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="593">1. 解压软件到/opt/module目录下<img alt="" class="has" height="59" src="https://img-blog.csdnimg.cn/20190816220052503.png" width="757"><img alt="" class="has" height="137" src="https://img-blog.csdnimg.cn/20190816220118970.png" width="579">1. 将名称修改为hive  <img alt="" class="has" height="226" src="https://img-blog.csdnimg.cn/20190816220446367.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="484">1. 将/opt/module/hive/conf目录下的配置文件hive-env.sh.template名称修改为hive-env.sh<img alt="" class="has" height="264" src="https://img-blog.csdnimg.cn/20190816220725287.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="512">1. 修改hive-env.sh文件内容  <img alt="" class="has" height="217" src="https://img-blog.csdnimg.cn/20190816221104424.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="568">1. hadoop集群配置，因为hive是依赖于hadoop的，所以hadoop的hdfs和yarn必须启动 ⑴在hadoop102启动hdfs       <img alt="" class="has" height="191" src="https://img-blog.csdnimg.cn/20190816221531105.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="564"> ⑵到hadoop103上启动yarn        <img alt="" class="has" height="215" src="https://img-blog.csdnimg.cn/2019081622173717.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="478">     1. 在 HDFS 上创建/tmp 和/user/hive/warehouse(这个目录主要给hive存储数据用) 两个目录并修改他们的同组权限可写     <img alt="" class="has" height="100" src="https://img-blog.csdnimg.cn/20190816223829322.png" width="647">1. 修改新建目录权限   ⑴目前目录权限，同组用户可读，可执行，不可写,修改为可读可写          <img alt="" class="has" height="110" src="https://img-blog.csdnimg.cn/20190816224122506.png" width="763">          <img alt="" class="has" height="61" src="https://img-blog.csdnimg.cn/20190816224502264.png" width="586">          <img alt="" class="has" height="96" src="https://img-blog.csdnimg.cn/20190816224531899.png" width="587">           1. hive基本操作   ⑴启动hive          a：进入/opt/module/hive目录下，出现下面效果就启动完成                  <img alt="" class="has" height="200" src="https://img-blog.csdnimg.cn/20190816225102613.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="465">  ⑵查看数据库命令        <img alt="" class="has" height="180" src="https://img-blog.csdnimg.cn/20190816225235943.png" width="423"> ⑶打开默认数据库        <img alt="" class="has" height="192" src="https://img-blog.csdnimg.cn/20190816225325485.png" width="442">  ⑷显示数据库中的表         <img alt="" class="has" height="100" src="https://img-blog.csdnimg.cn/20190816225406640.png" width="326">  ⑸创建表         <img alt="" class="has" height="201" src="https://img-blog.csdnimg.cn/2019081622554793.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="489">  ⑹向表中插入数据         <img alt="" class="has" height="228" src="https://img-blog.csdnimg.cn/20190816225815241.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="443">  ⑺查表中的数据        <img alt="" class="has" height="156" src="https://img-blog.csdnimg.cn/20190816225935445.png" width="484">    ⑻退出：          <img alt="" class="has" height="79" src="https://img-blog.csdnimg.cn/20190816225958759.png" width="395">
 </li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Hive知识点入门学习五"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/Hive%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E4%BA%94/"
    >Hive知识点入门学习五</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/Hive%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E4%BA%94/" class="article-date">
  <time datetime="2021-07-18T14:10:35.169Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: Hive知识点入门学习五<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：函数 </p>
<p>      <img alt="" class="has" height="257" src="https://img-blog.csdnimg.cn/20190825215121162.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="427">       <img alt="" class="has" height="314" src="https://img-blog.csdnimg.cn/20190825215231991.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="627">       <img alt="" class="has" height="277" src="https://img-blog.csdnimg.cn/20190825215259467.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="534">       <img alt="" class="has" height="401" src="https://img-blog.csdnimg.cn/20190825215350821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="536">       <img alt="" class="has" height="309" src="https://img-blog.csdnimg.cn/20190825215423162.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="563">       <img alt="" class="has" height="405" src="https://img-blog.csdnimg.cn/20190825215500629.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="554"></p>
<h1 id="二：行转列"><a href="#二：行转列" class="headerlink" title="二：行转列"></a>二：行转列</h1><ol>
<li>表结构如下：  <img alt="" class="has" height="190" src="https://img-blog.csdnimg.cn/20190829215916927.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="647">1. 需求：把星座和血型一样的人归类到一起 ，效果如下  <img alt="" class="has" height="81" src="https://img-blog.csdnimg.cn/20190829220254118.png" width="295">1. sql步骤如下  <img alt="" class="has" height="196" src="https://img-blog.csdnimg.cn/20190829221220361.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="711">           sql:        select              t1.base,              concat_ws(‘|’, collect_set(t1.name)) name         from (                    select                           name,                           concat(constellation,’,’,blood_type) as base                    from  person_info                  ) t1 group by t1.base<img alt="" class="has" height="202" src="https://img-blog.csdnimg.cn/20190829221438842.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="507">  <img alt="" class="has" height="418" src="https://img-blog.csdnimg.cn/20190829221644867.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="671">  <img alt="" class="has" height="294" src="https://img-blog.csdnimg.cn/20190829221815775.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="567"><h1 id="三：列转行"><a href="#三：列转行" class="headerlink" title="三：列转行"></a>三：列转行</h1></li>
<li>表结构  <img alt="" class="has" height="189" src="https://img-blog.csdnimg.cn/20190831215607885.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="688">1.  创建表   <img alt="" class="has" height="159" src="https://img-blog.csdnimg.cn/20190831220314536.png" width="520">1. 导入数据   <img alt="" class="has" height="131" src="https://img-blog.csdnimg.cn/20190831220512348.png" width="434">  <img alt="" class="has" height="215" src="https://img-blog.csdnimg.cn/201908312206423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="710">1. 将数组展开  <img alt="" class="has" height="89" src="https://img-blog.csdnimg.cn/20190831220853427.png" width="759">           sql如下：  <strong>select movie,category_name from movie_info lateral view explode(category) table_temp as category_name;</strong>  <img alt="" class="has" height="282" src="https://img-blog.csdnimg.cn/20190831221355527.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="695">1. 数组等相关简介  <img alt="" class="has" height="121" src="https://img-blog.csdnimg.cn/20190831221557350.png" width="889">  <img alt="" class="has" height="412" src="https://img-blog.csdnimg.cn/20190831221629324.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="636">  <img alt="" class="has" height="179" src="https://img-blog.csdnimg.cn/20190831222953324.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="690"> <h1 id="四：youtube项目实战"><a href="#四：youtube项目实战" class="headerlink" title="四：youtube项目实战"></a>四：youtube项目实战</h1></li>
<li>表结构  a：视频表结构相关字段          <img alt="" class="has" height="168" src="https://img-blog.csdnimg.cn/2019090109262326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="625">  b：用户表         <img alt="" class="has" height="145" src="https://img-blog.csdnimg.cn/20190901092523720.png" width="588">1. 相关原始数据脚本  a：视频数据文件          <img alt="" class="has" height="170" src="https://img-blog.csdnimg.cn/20190901092742237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="651">  b：用户数据文件          <img alt="" class="has" height="226" src="https://img-blog.csdnimg.cn/20190901092822641.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="377"><li>对原始数据进行ETL编码清洗   通过观察原始数据形式，可以发现，视频可以有多个所属分类，每个所属分类用&amp;符号分割， 且分割的两边有空格字符，同时相关视频也是可以有多个元素，多个相关视频又用“\t”进 行分割。为了分析数据时方便对存在多个子元素的数据进行操作，我们首先进行数据重组清 洗操作。即：将所有的类别用“&amp;”分割，同时去掉两边空格，多个相关视频 id 也使用“&amp;” 进行分割。 ⑴项目：       <img alt="" class="has" height="292" src="https://img-blog.csdnimg.cn/20190901124134443.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="264">  ⑵pom.xml文件          <pre class="has"><code class="language-html">&lt;project xmlns="<a target="_blank" rel="noopener" href="http://maven.apache.org/POM/4.0.0&quot;">http://maven.apache.org/POM/4.0.0&quot;</a> xmlns:xsi="<a target="_blank" rel="noopener" href="http://www.w3.org/2001/XMLSchema-instance&quot;">http://www.w3.org/2001/XMLSchema-instance&quot;</a> xsi:schemaLocation="<a target="_blank" rel="noopener" href="http://maven.apache.org/POM/4.0.0">http://maven.apache.org/POM/4.0.0</a> <a target="_blank" rel="noopener" href="http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;">http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;</a>&gt;<br>&lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt;<br>&lt;groupId&gt;com.kgf&lt;/groupId&gt;<br>&lt;artifactId&gt;youtube&lt;/artifactId&gt;<br>&lt;version&gt;0.0.1-SNAPSHOT&lt;/version&gt;</li>
</ol>
<p>  &lt;properties&gt;<br>      &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt;<br>  &lt;/properties&gt;</p>
<p>  &lt;dependencies&gt;<br>      &lt;!–引入hadoop依赖  –&gt;<br>      &lt;dependency&gt;<br>       &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;<br>       &lt;artifactId&gt;hadoop-client&lt;/artifactId&gt;<br>       &lt;version&gt;2.7.2&lt;/version&gt;<br>      &lt;/dependency&gt;<br>      &lt;dependency&gt;<br>           &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;<br>         &lt;artifactId&gt;hadoop-yarn-server-resourcemanager&lt;/artifactId&gt;<br>         &lt;version&gt;2.7.2&lt;/version&gt;<br>      &lt;/dependency&gt;<br>  &lt;/dependencies&gt;<br>&lt;/project&gt;</code></pre> ⑶ETLUtil工具类 <pre class="has"><code class="language-java">package com.kgf.youtube.utils;</p>
<p>/**</p>
<ul>
<li>对数据文件中的一行数据进行处理</li>
<li>@author KGF</li>
<li></li>
<li>/<br>public class ETLUtil &#123;<br>  //RX24KLBhwMI    lemonette    697    People &amp; Blogs    512    24149    4.22    315    474    t60tW0WevkE    WZgoejVDZlo<br>  public static String getEtlString(String ori) &#123;<pre><code>  StringBuilder sb = new StringBuilder();
  //对数据进行切割
  String[] splitArray = ori.split(&quot;\t&quot;);
  //一行数据中相关视频可以没有，其它相关数据必须存在，否则该条数据作废
  if(splitArray.length&amp;lt;9)return null;
  //因为视频类别是以&amp;amp;连接，我们将中间空格去掉，变成如下效果
  //RX24KLBhwMI    lemonette    697    People&amp;amp;Blogs    512    24149    4.22    315    474    t60tW0WevkE    WZgoejVDZlo
  splitArray[3] = splitArray[3].replaceAll(&quot; &quot;,&quot;&quot;);
  //拼接数据
  for (int i = 0; i &amp;lt; splitArray.length; i++) &#123;
      sb.append(splitArray[i]);
      if(i&amp;lt;9) &#123;
          if(i!=splitArray.length-1) &#123;
              sb.append(&quot;\t&quot;);
          &#125;
      &#125;else &#123;
          if(i!=splitArray.length-1) &#123;
              sb.append(&quot;&amp;amp;&quot;);//这里对相关视频进行特殊处理，使用&amp;amp;连接
          &#125;
      &#125;
  &#125;
  //RX24KLBhwMI    lemonette    697    People&amp;amp;Blogs    512    24149    4.22    315    474    t60tW0WevkE&amp;amp;WZgoejVDZlo
  return sb.toString();
</code></pre>
  }<br>}<br></code></pre> ⑷VideoEtlMapper.java类 <pre class="has"><code class="language-java">package com.kgf.youtube.etl;</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.commons.lang.StringUtils;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import com.kgf.youtube.utils.ETLUtil;<br>/**</p>
<ul>
<li><p>读取一行对数据进行处理</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class VideoEtlMapper extends Mapper&lt;Object,Text, NullWritable, Text&gt; &#123;</p>
<p>  Text k = new Text();</p>
<p>  @Override<br>  protected void map(Object key, Text value, Mapper&lt;Object, Text, NullWritable, Text&gt;.Context context)</p>
<pre><code>      throws IOException, InterruptedException &#123;
  //对读取的一行数据进行处理
  String etlString = ETLUtil.getEtlString(value.toString());
  if(StringUtils.isNotBlank(etlString)) &#123;
      k.set(etlString);
      context.write(NullWritable.get(),k);
  &#125;
</code></pre>
<p>  }<br>}<br></code></pre> ⑸VideoEtlRunner.java类 <pre class="has"><code class="language-java">package com.kgf.youtube.etl;</p>
</li>
</ul>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.FileSystem;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br>import org.apache.hadoop.util.Tool;<br>import org.apache.hadoop.util.ToolRunner;</p>
<p>public class VideoEtlRunner implements Tool &#123;</p>
<pre><code>private Configuration conf = null;

@Override
public void setConf(Configuration conf) &#123;
    this.conf = conf;//使用集群中的conf赋值
&#125;

@Override
public Configuration getConf() &#123;
    return this.conf;
&#125;

@Override
public int run(String[] args) throws Exception &#123;
    //将路径存入conf中
    conf.set(&quot;inpath&quot;,args[0]);
    conf.set(&quot;outpath&quot;,args[1]);
    //获取job对象
    Job job = Job.getInstance(conf);
    //设置jar
    job.setJarByClass(VideoEtlRunner.class);
    //设置mapper
    job.setMapperClass(VideoEtlMapper.class);
    job.setMapOutputKeyClass(NullWritable.class);
    job.setMapOutputValueClass(Text.class);
    //设置reduce
    job.setNumReduceTasks(0);
    //设置输入，输出路径
    initInputJobPath(job);
    initOutputJobPath(job);
    boolean result = job.waitForCompletion(true);
    return result?1:-1;
&#125;

private void initOutputJobPath(Job job) &#123;
    try &#123;
        Configuration conf = job.getConfiguration();
        String outPath = conf.get(&quot;outpath&quot;);//获取conf中的输出路径
        FileSystem fs = FileSystem.get(conf);//获取文件系统对象
        Path path = new Path(outPath);
        if(fs.exists(path))&#123;
            fs.delete(path, true);//删除文件夹，包括子目录
        &#125;
        FileOutputFormat.setOutputPath(job, path);
    &#125; catch (Exception e) &#123;
        e.printStackTrace();
    &#125;
&#125;

private void initInputJobPath(Job job) &#123;
    try &#123;
        Configuration conf = job.getConfiguration();
        String inPath = conf.get(&quot;inpath&quot;);//获取conf中的输出路径
        FileSystem fs = FileSystem.get(conf);//获取文件系统对象
        Path path = new Path(inPath);
        if(fs.exists(path))&#123;
            FileInputFormat.setInputPaths(job, path);
        &#125;else &#123;
            throw new Exception(&quot;inPath not exists&quot;);
        &#125;
    &#125; catch (Exception e) &#123;
        e.printStackTrace();
    &#125;
&#125;

public static void main(String[] args) throws Exception &#123;
    //调用run方法
    int resultCode = ToolRunner.run(new VideoEtlRunner(), args);
    System.out.println(resultCode);
&#125;
</code></pre>
<p>}<br></code></pre> ⑹将代码打成jar上传到集群       <img alt="" class="has" height="208" src="https://img-blog.csdnimg.cn/20190901124457905.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="594">  ⑺将原始数据文件video.txt上传到集群/youtube/video/2008路径下      <img alt="" class="has" height="186" src="https://img-blog.csdnimg.cn/20190901124619827.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="561"> ⑻执行jar生成文件      a：进入到/opt/module/hadoop-2.7.2目录下      b：执行如下命令           hadoop jar ../jars/youtube-0.0.1-SNAPSHOT.jar com.kgf.youtube.etl.VideoEtlRunner           /youtube/video/2008 /youtube/video/output          <img alt="" class="has" height="305" src="https://img-blog.csdnimg.cn/20190901124850213.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="604"> </li>1. 效果  <img alt="" class="has" height="285" src="https://img-blog.csdnimg.cn/20190901124927461.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="589">  <img alt="" class="has" height="279" src="https://img-blog.csdnimg.cn/20190901125028770.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="752"><li>创建表    <img alt="" class="has" height="292" src="https://img-blog.csdnimg.cn/20190901173330821.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="364"><img alt="" class="has" height="174" src="https://img-blog.csdnimg.cn/20190901173428254.png" width="427">  <img alt="" class="has" height="277" src="https://img-blog.csdnimg.cn/2019090117352126.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="450">  <img alt="" class="has" height="179" src="https://img-blog.csdnimg.cn/20190901173547731.png" width="432">   <pre class="has"><code class="language-sql">create table youtube_ori(<br>    videoId string,<br>    uploader string,<br>    age int,<br>    category array&lt;string&gt;,<br>    length int,<br>    views int,<br>    rate float,<br>    ratings int,<br>    comments int,<br>    relatedId array&lt;string&gt;<br>)<br>row format delimited<br>fields terminated by "\t"<br>collection items terminated by "&amp;"<br>stored as textfile; </p>
<p>create table youtube_user_ori(<br>    uploader string,<br>    videos int,<br>    friends int)<br>clustered by (uploader) into 24 buckets<br>row format delimited<br>fields terminated by “\t”<br>stored as textfile;</p>
<p>create table youtube_orc(<br>    videoId string,<br>    uploader string,<br>    age int,<br>    category array&lt;string&gt;,<br>    length int,<br>    views int,<br>    rate float,<br>    ratings int,<br>    comments int,<br>    relatedId array&lt;string&gt;)<br>clustered by (uploader) into 8 buckets<br>row format delimited fields terminated by “\t”<br>collection items terminated by “&amp;”<br>stored as orc;</p>
<p>create table youtube_user_orc(<br>    uploader string,<br>    videos int,<br>    friends int)<br>clustered by (uploader) into 24 buckets<br>row format delimited<br>fields terminated by “\t”<br>stored as orc; </code></pre> <img alt="" class="has" height="50" src="https://img-blog.csdnimg.cn/20190901173813131.png" width="829"><img alt="" class="has" height="232" src="https://img-blog.csdnimg.cn/20190901173855528.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="619"><img alt="" class="has" height="53" src="https://img-blog.csdnimg.cn/20190901173909838.png" width="426"> </li>1. 导入 ETL 后的数据到ori表中  <img alt="" class="has" height="329" src="https://img-blog.csdnimg.cn/20190901174937582.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="787"><img alt="" class="has" height="256" src="https://img-blog.csdnimg.cn/20190901175123561.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="788">1. 向 ORC 表插入数据  a：insert into table youtube_orc select * from youtube_ori;   b：insert into table youtube_user_orc select * from youtube_user_ori;  1. 需求1：统计视频观看数 Top10   SQL:<strong>select videoId,category,views from  youtube_orc order by views desc limit 10;</strong>  <img alt="" class="has" height="353" src="https://img-blog.csdnimg.cn/20190901181041639.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="768">1. 需求2：统计视频类别热度 Top10   <img alt="" class="has" height="233" src="https://img-blog.csdnimg.cn/2019090118113433.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="765">  <img alt="" class="has" height="294" src="https://img-blog.csdnimg.cn/20190901182646488.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="621">  <img alt="" class="has" height="322" src="https://img-blog.csdnimg.cn/20190901182737120.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="574">1. 需求3：统计出视频观看数最高的 20 个视频的所属类别以及类别包含 这 Top20 视频的个数 <img alt="" class="has" height="357" src="https://img-blog.csdnimg.cn/20190901210611997.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="716"><img alt="" class="has" height="283" src="https://img-blog.csdnimg.cn/20190901210707215.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="609">1. 统计视频观看数 Top50 所关联视频的所属类别的热度排名   <img alt="" class="has" height="373" src="https://img-blog.csdnimg.cn/20190901212411101.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="437">  <img alt="" class="has" height="237" src="https://img-blog.csdnimg.cn/20190901212538808.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="569"></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><a class="page-number" href="/page/4/">4</a><a class="extend next" rel="next" href="/page/2/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2021
        <i class="ri-heart-fill heart_icon"></i> kgf
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 <a href="https://hexo.io" target="_blank">Hexo</a> 强力驱动
        <span class="division">|</span>
        主题 - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/image1.ico" alt="爱上口袋的天空"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
  </div>
</body>

</html>