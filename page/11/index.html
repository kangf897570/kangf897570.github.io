<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> 爱上口袋的天空</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/image1.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <!-- mermaid -->
      
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover1.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">爱上口袋的天空</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['欢迎来到爱上口袋的天空的博客', '愿你一生努力，一生被爱', '想要的都拥有，得不到的都释怀'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  <ul class="ads">
    
        <li>
            <a target="_blank" rel="noopener" href="https://curl.qcloud.com/kvO7hb43">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/ten_1.jpg" width="300" alt="云服务器限时秒杀">
            </a>
        </li>
    
        <li>
            <a target="_blank" rel="noopener" href="https://www.vultr.com/?ref=8630075">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/vultr.png" width="300" alt="vultr优惠vps">
            </a>
        </li>
    
</ul>
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-hadoop之快照管理,回收站"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/hadoop%E4%B9%8B%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86,%E5%9B%9E%E6%94%B6%E7%AB%99/"
    >hadoop之快照管理,回收站</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/hadoop%E4%B9%8B%E5%BF%AB%E7%85%A7%E7%AE%A1%E7%90%86,%E5%9B%9E%E6%94%B6%E7%AB%99/" class="article-date">
  <time datetime="2021-07-18T14:10:35.066Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: hadoop之快照管理,回收站<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：快照管理</p>
<ol>
<li>简介       快照相当于对目录做一个备份。并不会立即复制所有文件，而是指向同一个文件。当写入发生时，才会产生新文件。1.   基本语法如下      <img alt="" class="has" height="342" src="https://img-blog.csdnimg.cn/2019072521270446.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="527">1.  案例实操     ⑴开启/禁用指定目录的快照功能                <img alt="" class="has" height="147" src="https://img-blog.csdnimg.cn/20190725213117140.png" width="590">     ⑵ 列出当前用户所有可快照目录                <img alt="" class="has" height="95" src="https://img-blog.csdnimg.cn/20190725213247981.png" width="657">         ⑶对指定目录创建快照，将快照放在/user/kgf/input/.snapshot/s20190725-213524.180这个目录下             <img alt="" class="has" height="131" src="https://img-blog.csdnimg.cn/20190725213538808.png" width="563">           注意：生成的是隐藏文件，浏览器上看不到，但是可以直接访问。          <img alt="" class="has" height="229" src="https://img-blog.csdnimg.cn/20190725213907297.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="542">    ⑷重命名快照          <img alt="" class="has" height="141" src="https://img-blog.csdnimg.cn/20190725214235569.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="624">          <img alt="" class="has" height="269" src="https://img-blog.csdnimg.cn/20190725214258523.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="623">      ⑸比较两个快照目录的不同之处           a：创建过快照的目录下的一些文件                 <img alt="" class="has" height="207" src="https://img-blog.csdnimg.cn/20190725215155901.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="495">          b：删掉创建过快照的目录下的一些文件，并且新增一些其它文件                <img alt="" class="has" height="213" src="https://img-blog.csdnimg.cn/20190725215403106.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="517">          c：使用命令比较这个目录和之前创建的快照区别               <img alt="" class="has" height="143" src="https://img-blog.csdnimg.cn/20190725215722279.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="525">               就是说快照里面比现在的目录多了一个wait.sh,少了一个README.txt        <h1 id="二：回收站"><a href="#二：回收站" class="headerlink" title="二：回收站"></a>二：回收站</h1></li>
<li>默认回收站，一般公司里，防止误删数据会保存一周时间      <img alt="" class="has" height="144" src="https://img-blog.csdnimg.cn/20190725221204854.png" width="750">1.  启用回收站并且修改访问垃圾回收站用户名称（进入垃圾回收站用户名称，默认是 dr.who）  ⑴进入namenode的/opt/module/hadoop-2.7.2/etc/hadoop目录下在core-site.xml中修改       <img alt="" class="has" height="283" src="https://img-blog.csdnimg.cn/20190725221945370.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="406">    ⑵将修改后的文件分发到集群的各个服务器上          <img alt="" class="has" height="274" src="https://img-blog.csdnimg.cn/20190725222402894.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="409">1.  重新启动集群，并且删除hdfs上一个文件    <img alt="" class="has" height="92" src="https://img-blog.csdnimg.cn/20190725223236997.png" width="823">           可以发现删除的文件进入回收站hdfs://hadoop102:9000/user/kgf/.Trash/Current目录下，一分钟后自动删除    <img alt="" class="has" height="228" src="https://img-blog.csdnimg.cn/20190725223339934.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="651">1. 注意：  <img alt="" class="has" height="104" src="https://img-blog.csdnimg.cn/20190725223844497.png" width="582"></li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Hadoop之编译源码"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/Hadoop%E4%B9%8B%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/"
    >Hadoop之编译源码</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/Hadoop%E4%B9%8B%E7%BC%96%E8%AF%91%E6%BA%90%E7%A0%81/" class="article-date">
  <time datetime="2021-07-18T14:10:35.060Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: Hadoop之编译源码<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：服务器，jar包等准备</p>
<ol>
<li>首先我们需要准备一台内存大约4G左右的服务器，2G的内存不一定够，4G的稳妥一点，并且  最好是一个纯净版的服务器，就是除了网络，主机名之外其它什么都没有配置的。1. 采用root用户编译，减少文件夹权限出现问题1. 将准备好的包上传到服务器上             <img alt="" class="has" height="249" src="https://img-blog.csdnimg.cn/20190713212010732.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="730">1. 安装JDK A：将jdk解压到module文件夹，并且配置JAVA_HOME路径在/etc/profile        <img alt="" class="has" height="157" src="https://img-blog.csdnimg.cn/2019071321231673.png" width="469">        <img alt="" class="has" height="196" src="https://img-blog.csdnimg.cn/20190713212339654.png" width="509">        效果：          <img alt="" class="has" height="181" src="https://img-blog.csdnimg.cn/20190713212419149.png" width="609"><h1 id="二：开始配置MAVEN"><a href="#二：开始配置MAVEN" class="headerlink" title="二：开始配置MAVEN"></a>二：开始配置MAVEN</h1></li>
<li>解压软件包到module  <img alt="" class="has" height="191" src="https://img-blog.csdnimg.cn/20190714104529965.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="699">  <img alt="" class="has" height="145" src="https://img-blog.csdnimg.cn/20190714104557576.png" width="589">1.  配置MAVEN的环境变量  在/etc/profile中配置  <img alt="" class="has" height="107" src="https://img-blog.csdnimg.cn/20190714104718607.png" width="470">            <img alt="" class="has" height="226" src="https://img-blog.csdnimg.cn/2019071410500076.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="511">      <img alt="" class="has" height="74" src="https://img-blog.csdnimg.cn/20190714105045844.png" width="465">   <img alt="" class="has" height="188" src="https://img-blog.csdnimg.cn/201907141051189.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="718"><h1 id="三：开始配置ANT"><a href="#三：开始配置ANT" class="headerlink" title="三：开始配置ANT"></a>三：开始配置ANT</h1></li>
<li>解压软件包        <img alt="" class="has" height="118" src="https://img-blog.csdnimg.cn/20190714105350368.png" width="771">        <img alt="" class="has" height="191" src="https://img-blog.csdnimg.cn/20190714105419404.png" width="643">1.   配置ANT的环境变量        <img alt="" class="has" height="285" src="https://img-blog.csdnimg.cn/20190714105658799.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="556">1.   效果       <img alt="" class="has" height="129" src="https://img-blog.csdnimg.cn/20190714105742835.png" width="552">        <h1 id="四：安装glibc-headers-和g-命令"><a href="#四：安装glibc-headers-和g-命令" class="headerlink" title="四：安装glibc-headers 和g++命令"></a>四：安装glibc-headers 和g++命令</h1></li>
<li>安装glibc-headers  命令：yum install glibc-headers1. 安装g++   命令：yum install gcc-c++         1. 安装make    命令：yum install make1. 安装cmake    命令：yum install cmake<h1 id="五：开始配置protobuf"><a href="#五：开始配置protobuf" class="headerlink" title="五：开始配置protobuf"></a>五：开始配置protobuf</h1></li>
<li>解压软件包      <img alt="" class="has" height="104" src="https://img-blog.csdnimg.cn/20190714110646897.png" width="642">      <img alt="" class="has" height="206" src="https://img-blog.csdnimg.cn/20190714110706298.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="620">1.  开始配置protobuf   a：进入到/opt/module/protobuf-2.5.0目录下，执行./configure命令         <img alt="" class="has" height="117" src="https://img-blog.csdnimg.cn/20190714110911980.png" width="457">     b：在使用make命令编译一下        <img alt="" class="has" height="98" src="https://img-blog.csdnimg.cn/2019071411114969.png" width="443">   c：使用make check命令         <img alt="" class="has" height="104" src="https://img-blog.csdnimg.cn/20190714111522363.png" width="451">    d：执行make install          <img alt="" class="has" height="105" src="https://img-blog.csdnimg.cn/20190714111653738.png" width="451">    e：执行ldconfig          <img alt="" class="has" height="84" src="https://img-blog.csdnimg.cn/2019071411185587.png" width="399">    f：配置环境变量           <img alt="" class="has" height="246" src="https://img-blog.csdnimg.cn/20190714112330812.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="389">           <img alt="" class="has" height="114" src="https://img-blog.csdnimg.cn/20190714112231262.png" width="531">         1. 安装openssl库  命令：yum install openssl-devel  <img alt="" class="has" height="119" src="https://img-blog.csdnimg.cn/20190714112516456.png" width="560">1. 安装ncurses-devel库      命令：yum install ncurses-devel   <img alt="" class="has" height="89" src="https://img-blog.csdnimg.cn/20190714112653636.png" width="564">    <h1 id="六：开始编译源码"><a href="#六：开始编译源码" class="headerlink" title="六：开始编译源码"></a>六：开始编译源码</h1></li>
<li> 将hadoop-2.7.2-src.tar.gz源码包解压到/opt目录  <img alt="" class="has" height="157" src="https://img-blog.csdnimg.cn/20190714113147233.png" width="569"><img alt="" class="has" height="186" src="https://img-blog.csdnimg.cn/20190714113210834.png" width="632">1.  进入到/opt/hadoop-2.7.2-src目录，通过MAVEN执行编译命令 （时间大约30分钟）  命令：mvn package -Pdist,native -DskipTests -Dtar  <img alt="" class="has" height="107" src="https://img-blog.csdnimg.cn/20190714113733978.png" width="747">1. 效果  <img alt="" class="has" height="799" src="https://img-blog.csdnimg.cn/20190714131840370.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="770">1. 成功后的64位hadoop包在/opt/hadoop-2.7.2-src/hadoop-dist/target目录下  <img alt="" class="has" height="239" src="https://img-blog.csdnimg.cn/20190714132131141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="712">​​​​​​​</li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Hadoop之HDFS文件系统"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/Hadoop%E4%B9%8BHDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"
    >Hadoop之HDFS文件系统</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/Hadoop%E4%B9%8BHDFS%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/" class="article-date">
  <time datetime="2021-07-18T14:10:35.053Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: Hadoop之HDFS文件系统<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：HDFS概述</p>
<ol>
<li>HDFS产生的背景       随着数据量越来越大，在一个操作系统管辖的范围内存不下了，那么就分配到更多的操作系统 管理的磁盘中，但是不方便管理和维护，迫切需要一种系统来管理多台机器上的文件，这就是分布式 文件管理系统。HDFS只是分布式文件管理系统中的一种。1.  HDFS概念       HDFS它是一个文件系统，用于存储文件，通过目录树来定位文件，其次它是分布式的，由很多 服务器联合起来实现其功能，集群中的服务器有各自的角色。       HDFS的设计适合一次写入，多次读出的场景，且不支持文件的修改。适合用来做数据分析，并 不适合用来做网盘应用。1. HDFS的优缺点       优点：                a：高容错性                             ⑴数据自动保存多个副本。它通过增加副本的形式，提高容错性。                             ⑵某一个副本丢失以后，它可以自动恢复。                b：适合大数据处理                             ⑴数据规模：                                         能够处理数据规模达到GB,TB甚至PB级别的数据。                             ⑵文件规模：                                         能够处理百万规模以上的文件数量，数量相当之大。                c：流式数据访问                             ⑴一次写入，多次读取，不能修改，只能追加                             ⑵它能保证数据的一致性。                d：可构建在廉价的机器上，通过多副本机制，提高可靠性。       缺点：                a：不适合低延迟数据访问。比如毫秒级的存储数据，是做不到的。                b：无法高效的对大量小文件进行存储                              ⑴存储大量小文件的话，它会占用NameNode大量的内存来存储文件，目录                                  和块信息。这样是不可取的，因为NameNode的内存总是有限的。                              ⑵小文件存储的寻道时间会超过读取时间，它违反了HDFS的设计目标。                c：并发写入，文件随机修改                               ⑴一个文件只能有一个写，不允许多个线程同时写。                               ⑵仅支持数据append(追加)，不支持文件随机修改。              <h1 id="二：HDFS块大小"><a href="#二：HDFS块大小" class="headerlink" title="二：HDFS块大小"></a>二：HDFS块大小</h1></li>
</ol>
<p>        <img alt="" class="has" height="136" src="https://img-blog.csdnimg.cn/20190714220247623.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="775">                    <img alt="" class="has" height="372" src="https://img-blog.csdnimg.cn/2019071422120926.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="667">   </p>
<h1 id="三：HDFS命令行操作"><a href="#三：HDFS命令行操作" class="headerlink" title="三：HDFS命令行操作"></a>三：HDFS命令行操作</h1><ol>
<li>首先启动集群  a：集群的部署重新调整了一下       <img alt="" class="has" height="151" src="https://img-blog.csdnimg.cn/20190717215248590.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="630">  b：在hadoop102上启动namenode,datanode以及secondarynamenode        <img alt="" class="has" height="199" src="https://img-blog.csdnimg.cn/20190717215346270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="516">  c：在hadoop103上启动yarn        <img alt="" class="has" height="134" src="https://img-blog.csdnimg.cn/2019071721551871.png" width="403">       OK集群启动完毕。1. 相关命令  <img alt="" class="has" height="380" src="https://img-blog.csdnimg.cn/20190717215855669.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="720">  <img alt="" class="has" height="352" src="https://img-blog.csdnimg.cn/20190717220258474.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="687">         <h1 id="四：HDFS客户端环境准备"><a href="#四：HDFS客户端环境准备" class="headerlink" title="四：HDFS客户端环境准备"></a>四：HDFS客户端环境准备</h1></li>
<li>通过Eclipse去连接hdfs，首先准备jar包  a：将hadoop-2.7.2.tar.gz解压到非中文目录       <img alt="" class="has" height="228" src="https://img-blog.csdnimg.cn/20190717223355164.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="350">  b：进入hadoop-2.7.2\share\hadoop文件夹，查找所有jar包，并且把jar都拷贝出来到新建的lib文件夹      <img alt="" class="has" height="341" src="https://img-blog.csdnimg.cn/20190717223948728.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="572">  c：新建一个source文件夹，将-source.jar结尾的jar挑出来，放到source文件夹中       <img alt="" class="has" height="322" src="https://img-blog.csdnimg.cn/20190717224052282.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="551">   d：再建立一个test文件夹，将-tests.jar结尾的jar挑出来，放到test文件夹中           <img alt="" class="has" height="281" src="https://img-blog.csdnimg.cn/20190718210547295.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="686">      1. Eclipse环境准备   ⑴配置hadoop环境变量         <img alt="" class="has" height="268" src="https://img-blog.csdnimg.cn/20190718212303557.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="414">         1. 创建第一个java工程HdfsClientDemo1         <img alt="" class="has" height="191" src="https://img-blog.csdnimg.cn/20190718212853542.png" width="374">    引入上面准备的jar包，除了test和source挑出的jar，其它的都引入。 ⑴将本地文件上传到hdfs上的第一种方式       <img alt="" class="has" height="325" src="https://img-blog.csdnimg.cn/20190718220301321.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="504">       注意：运行的时候我们需要配置环境变量，最后执行main方法       <img alt="" class="has" height="254" src="https://img-blog.csdnimg.cn/20190718220359443.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="411">      效果：     <img alt="" class="has" height="189" src="https://img-blog.csdnimg.cn/20190718220450436.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="476">     <img alt="" class="has" height="338" src="https://img-blog.csdnimg.cn/20190718220523822.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="473">  ⑵第二种方式，不需要配置环境变量，直接在代码中配置用户名，效果和上面一样       <img alt="" class="has" height="260" src="https://img-blog.csdnimg.cn/20190718221021277.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="591">       1. HDFS文件上传   ⑴在项目的src根目录下引入hdfs-site.xml配置文件，默认会读取根目录下的这个文件        <img alt="" class="has" height="303" src="https://img-blog.csdnimg.cn/20190719221020921.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="700">        <img alt="" class="has" height="362" src="https://img-blog.csdnimg.cn/20190719221208565.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="577">       效果：       <img alt="" class="has" height="254" src="https://img-blog.csdnimg.cn/20190719221310378.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="605">1. 同理，我们可以直接在代码里面配置，这个优先级最高，可覆盖配置文件中的配置  <img alt="" class="has" height="343" src="https://img-blog.csdnimg.cn/20190719221451855.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="540">  效果：  <img alt="" class="has" height="328" src="https://img-blog.csdnimg.cn/20190719221609756.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="570">1. HDFS文件下载  <img alt="" class="has" height="403" src="https://img-blog.csdnimg.cn/20190719223003129.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="567"> 效果：<img alt="" class="has" height="214" src="https://img-blog.csdnimg.cn/20190719223041958.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="512">1. HDFS创建目录  <img alt="" class="has" height="212" src="https://img-blog.csdnimg.cn/20190719223816315.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="612"><img alt="" class="has" height="245" src="https://img-blog.csdnimg.cn/20190719223835240.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="615">1. HDFS文件删除操作  <img alt="" class="has" height="217" src="https://img-blog.csdnimg.cn/20190719225219938.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="571">1. HDFS上文件名称修改，第一个参数是原文件名，第二个是新的名称  <img alt="" class="has" height="196" src="https://img-blog.csdnimg.cn/20190719225736608.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="570"><li>HDFS文件详情的查看，主要是查看文件的名称，权限，长度，块信息   <pre class="has"><code class="language-java">public void readListFiles() throws IOException, InterruptedException, URISyntaxException &#123;<pre><code> //1：获取hadoop的文件系统
 Configuration configuration = new Configuration();
 //这里配置的是和core-site.xml文件中一样，主要是namenode的路径
 FileSystem fs = FileSystem.get(new URI(&quot;hdfs://hadoop102:9000&quot;), configuration, &quot;kgf&quot;);
 //2:在HDFS上修改文件名称
 RemoteIterator&amp;lt;LocatedFileStatus&amp;gt; fileStatus = fs.listFiles(new Path(&quot;/&quot;),true);
 while(fileStatus.hasNext()) &#123;
     LocatedFileStatus file = fileStatus.next();
     //文件名称
     System.out.println(file.getPath().getName());
     //文件大小
     System.out.println(file.getLen());
     //文件权限
     System.out.println(file.getPermission());
     //文件所属组
     System.out.println(file.getGroup());
     //获取文件块信息
     BlockLocation[] blockLocations = file.getBlockLocations();
     for (BlockLocation blockLocation : blockLocations) &#123;
         //获取文件所属节点信息
         String[] hosts = blockLocation.getHosts();
         for (String host : hosts) &#123;
             System.out.println(host);
         &#125;
     &#125;
     System.out.println(&quot;====================================&quot;);
 &#125;
 //3:关闭资源
 fs.close();
</code></pre>
 }</code></pre> 效果：<img alt="" class="has" height="455" src="https://img-blog.csdnimg.cn/20190719231824473.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="446"> </li>1. HDFS上文件和文件夹的判断  <img alt="" class="has" height="375" src="https://img-blog.csdnimg.cn/2019071923260996.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="636"></li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-hadoop入门之概述"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/hadoop%E5%85%A5%E9%97%A8%E4%B9%8B%E6%A6%82%E8%BF%B0/"
    >hadoop入门之概述</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/hadoop%E5%85%A5%E9%97%A8%E4%B9%8B%E6%A6%82%E8%BF%B0/" class="article-date">
  <time datetime="2021-07-18T14:10:35.037Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: hadoop入门之概述<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：什么是Hadoop?</p>
<ol>
<li>Hadoop 是一个由 Apache 基金会所开发的分布式系统基础架构1. 主要解决，海量数据的存储和海量数据的分析计算问题。1. 广义上来说，HADOOP 通常是指一个更广泛的概念——HADOOP 生态圈         1. Hadoop 三大发行版本: Apache、Cloudera、Hortonworks。  a:Apache 版本最原始（最基础）的版本，对于入门学习最好。  b:Cloudera 在大型互联网企业中用的较多  c:Hortonworks 文档较好。<h1 id="二：Hadoop-的优势"><a href="#二：Hadoop-的优势" class="headerlink" title="二：Hadoop 的优势"></a>二：Hadoop 的优势</h1></li>
<li>高可靠性      因为 Hadoop 假设计算元素和存储会出现故障，因为它维护多个工作数据副本，在出现故障时可以对失败的节点重新分布处理1. 高扩展性      在集群间分配任务数据，可方便的扩展数以千计的节点1.  高效性       在 MapReduce 的思想下，Hadoop 是并行工作的，以加快任务处理速度1.  高容错性       自动保存多份副本数据，并且能够自动将失败的任务重新分配    <h1 id="三：Hadoop-组成"><a href="#三：Hadoop-组成" class="headerlink" title="三：Hadoop  组成"></a>三：Hadoop  组成</h1></li>
<li> Hadoop HDFS      一个高可靠、高吞吐量的分布式文件系统。1. Hadoop MapReduce      一个分布式的离线并行计算框架1. Hadoop YARN     作业调度与集群资源管理的框架。1. Hadoop Common     支持其他模块的工具模块（Configuration、RPC、序列化机制、日志 操作）。     <h1 id="四：HDFS-架构-概述"><a href="#四：HDFS-架构-概述" class="headerlink" title="四：HDFS  架构 概述"></a>四：HDFS  架构 概述</h1></li>
<li>NameNode（nn）     存储文件的元数据，如文件名，文件目录结构，文件属性（生成时间、副本数、文件权限），以及每个文件的块列表和块所在的DataNode等。1. DataNode(dn)     在本地文件系统存储文件块数据，以及块数据的校验和。1. Secondary NameNode(2nn)      用来监控HDFS状态的辅助后台程序，每隔一段时间获取HDFS元数据的快照     <h1 id="五：YARN-Yet-Another-Resource-Negotiator-架构"><a href="#五：YARN-Yet-Another-Resource-Negotiator-架构" class="headerlink" title="五：YARN(Yet Another Resource Negotiator)  架构"></a>五：YARN(Yet Another Resource Negotiator)  架构</h1></li>
<li>ResourceManager(rm)      处理客户端请求、启动/监控 ApplicationMaster、监控 NodeManager、资源分配与调度1. NodeManager(nm)      单个节点上的资源管理、处理来自 ResourceManager 的命令、处理来自 ApplicationMaster 的命令。 它会定时地向RM汇报本节点上的资源使用情况和各个Container的运行状态；同时会接收并处理来自AM 的Container 启动/停止等请求。1. ApplicationMaster      数据切分、为应用程序申请资源，并分配给内部任务、任务监控与容错。 用户提交的应用程序均包含一个AM，负责应用的监控，跟踪应用执行状态，重启失败任务等。ApplicationMaster 是应用框架，它负责向ResourceManager协调资源，并且与NodeManager协同工作完成Task的执行和监控。1. Container      对任务运行环境的抽象，封装了 CPU、内存等多维资源以及环境变量、启动命令等任务运行相关的信息 。 Container是YARN中的资源抽象，它封装了某个节点上的多维度资源，如内存、CPU、磁盘、网络等，当AM向RM 申请资源时，RM为AM返回的资源便是用Container 表示的。 YARN会为每个任务分配一个Container且该任务只能 使用该Container中描述的资源。1. 简介（detail）     YARN（Yet Another Resource Negotiator）是一个<strong>通用</strong>的资源管理平台，可为各类计算框架提供资源的管理和调度。 其核心出发点是为了分离资源管理与作业调度/监控，实现分离的做法是拥有一个全局的资源管理器。以及每个应用程序 对应一个的应用管理器（ApplicationMaster，AM）。1. 架构如下：     <img alt="" class="has" height="408" src="https://img-blog.csdnimg.cn/20190630220652270.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="642">   </li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Flume知识点入门学习一"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/Flume%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E4%B8%80/"
    >Flume知识点入门学习一</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/Flume%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E4%B8%80/" class="article-date">
  <time datetime="2021-07-18T14:10:35.026Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: Flume知识点入门学习一<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：简介</p>
<p>         <img alt="" class="has" height="250" src="https://img-blog.csdnimg.cn/20190907164649823.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="817"></p>
<h1 id="二：Flume-角色"><a href="#二：Flume-角色" class="headerlink" title="二：Flume 角色"></a>二：Flume 角色</h1><p>          <img alt="" class="has" height="387" src="https://img-blog.csdnimg.cn/20190907170335438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="570">           <img alt="" class="has" height="242" src="https://img-blog.csdnimg.cn/20190907170416665.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="570"></p>
<h1 id="三：Flume-传输过程"><a href="#三：Flume-传输过程" class="headerlink" title="三：Flume 传输过程"></a>三：Flume 传输过程</h1><p>        <img alt="" class="has" height="116" src="https://img-blog.csdnimg.cn/20190907170520627.png" width="699"></p>
<h1 id="四：安装flume"><a href="#四：安装flume" class="headerlink" title="四：安装flume"></a>四：安装flume</h1><ol>
<li>上传安装包到linux,并且解压到指定目录下   <img alt="" class="has" height="210" src="https://img-blog.csdnimg.cn/20190907212016939.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="723">    <img alt="" class="has" height="217" src="https://img-blog.csdnimg.cn/20190907212204323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="717">1. 修改配置文件名称  <img alt="" class="has" height="306" src="https://img-blog.csdnimg.cn/20190907212941545.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="660">1. 修改flume-env.sh文件  <img alt="" class="has" height="186" src="https://img-blog.csdnimg.cn/20190907213705941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="547">        <h1 id="五：案例一：监控端口数据"><a href="#五：案例一：监控端口数据" class="headerlink" title="五：案例一：监控端口数据"></a>五：案例一：监控端口数据</h1></li>
<li>需求：Flume 监控一端 Console，另一端 Console 发送消息，使被监控端实时显示1. 创建 Flume Agent 配置文件 job_flume_telnet.conf   <img alt="" class="has" height="445" src="https://img-blog.csdnimg.cn/20190907220902438.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="635">1. 判断 44444 端口是否被占用   <img alt="" class="has" height="151" src="https://img-blog.csdnimg.cn/20190907221017173.png" width="681">1. 先开启 flume 先听端口   <img alt="" class="has" height="147" src="https://img-blog.csdnimg.cn/201909072224329.png" width="669">  <img alt="" class="has" height="304" src="https://img-blog.csdnimg.cn/20190907222655263.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="644">1.  使用 telnet 工具向本机的 44444 端口发送内容   <img alt="" class="has" height="561" src="https://img-blog.csdnimg.cn/20190907222854781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="566">
 </li>
</ol>
<h1 id="六：案例二：实时读取本地文件到-HDFS"><a href="#六：案例二：实时读取本地文件到-HDFS" class="headerlink" title="六：案例二：实时读取本地文件到 HDFS"></a>六：案例二：实时读取本地文件到 HDFS</h1><ol>
<li>需求：**实时监控 hive 日志，并上传到 HDFS 中 **1.  拷贝 Hadoop 相关 jar 到 Flume 的 /opt/module/apache-flume-1.7.0-bin/lib/ 目录下：  <img alt="" class="has" height="172" src="https://img-blog.csdnimg.cn/20190908130750465.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="635"><li>创建job_flume_2hdfs.conf 文件    <img alt="" class="has" height="142" src="https://img-blog.csdnimg.cn/20190908134244861.png" width="412">         内容：    <pre class="has"><code class="language-bash">#把agent起个名叫a2,sources叫r2,sinks叫k2.hdfs,channels叫c2<br>a2.sources = r2<br>a2.sinks = k2<br>a2.channels = c2 </li>
</ol>
<h1 id="Describe-configure-the-source"><a href="#Describe-configure-the-source" class="headerlink" title="Describe/configure the source"></a>Describe/configure the source</h1><h1 id="exec即execute执行命令"><a href="#exec即execute执行命令" class="headerlink" title="exec即execute执行命令"></a>exec即execute执行命令</h1><p>a2.sources.r2.type = exec </p>
<h1 id="要执行的命令"><a href="#要执行的命令" class="headerlink" title="要执行的命令"></a>要执行的命令</h1><p>a2.sources.r2.command = tail -F /opt/module/hive/logs/hive.log </p>
<h1 id="执行shell脚本的绝对路径"><a href="#执行shell脚本的绝对路径" class="headerlink" title="执行shell脚本的绝对路径"></a>执行shell脚本的绝对路径</h1><p>a2.sources.r2.shell = /bin/bash -c </p>
<h1 id="Describe-the-sink"><a href="#Describe-the-sink" class="headerlink" title="Describe the sink"></a>Describe the sink</h1><p>a2.sinks.k2.type = hdfs </p>
<h1 id="上传到hdfs的路径"><a href="#上传到hdfs的路径" class="headerlink" title="上传到hdfs的路径"></a>上传到hdfs的路径</h1><p>a2.sinks.k2.hdfs.path = hdfs://hadoop102:9000/flume/%Y%m%d/%H<br>#上传文件的前缀<br>a2.sinks.k2.hdfs.filePrefix = logs-<br>#是否按照时间滚动文件夹<br>a2.sinks.k2.hdfs.round = true<br>#多少时间单位创建一个新的文件夹<br>a2.sinks.k2.hdfs.roundValue = 1<br>#重新定义时间单位<br>a2.sinks.k2.hdfs.roundUnit = hour<br>#是否使用本地时间戳<br>a2.sinks.k2.hdfs.useLocalTimeStamp = true<br>#积攒多少个 Event 才 flush 到 HDFS 一次<br>a2.sinks.k2.hdfs.batchSize = 1000<br>#设置文件类型，可支持压缩<br>a2.sinks.k2.hdfs.fileType = DataStream<br>#多久生成一个新的文件 （单位：秒）<br>a2.sinks.k2.hdfs.rollInterval = 600<br>#设置每个文件的滚动大小 （单位：字节）<br>a2.sinks.k2.hdfs.rollSize = 134217700<br>#文件的滚动与 Event 数量无关<br>a2.sinks.k2.hdfs.rollCount = 0<br>#最小副本数<br>a2.sinks.k2.hdfs.minBlockReplicas = 1 </p>
<h1 id="Use-a-channel-which-buffers-events-in-memory"><a href="#Use-a-channel-which-buffers-events-in-memory" class="headerlink" title="Use a channel which buffers events in memory"></a>Use a channel which buffers events in memory</h1><p>#channels阶段以内存的形式保存数据  event数量100<br>a2.channels.c2.type = memory<br>a2.channels.c2.capacity = 1000<br>a2.channels.c2.transactionCapacity = 100 </p>
<h1 id="Bind-the-source-and-sink-to-the-channel"><a href="#Bind-the-source-and-sink-to-the-channel" class="headerlink" title="Bind the source and sink to the channel"></a>Bind the source and sink to the channel</h1><p>#把source和sink和channel对接   source可以对接多个channels  sinks只能对接一个channel<br>a2.sources.r2.channels = c2<br>a2.sinks.k2.channel = c2</code></pre>   </li>1. 启动hdfs集群1. 执行监控配置   <img alt="" class="has" height="360" src="https://img-blog.csdnimg.cn/20190908135744365.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="734">  下面我们再次打开一个控制台，操作hive:  <img alt="" class="has" height="497" src="https://img-blog.csdnimg.cn/20190908140009436.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="590">  在浏览器上查看hdfs:  <img alt="" class="has" height="260" src="https://img-blog.csdnimg.cn/20190908141649630.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="574">  </p>
<h1 id="七：案例三：实时读取目录文件到-HDFS"><a href="#七：案例三：实时读取目录文件到-HDFS" class="headerlink" title="七：案例三：实时读取目录文件到 HDFS"></a>七：案例三：实时读取目录文件到 HDFS</h1><ol>
<li>需求：使用 flume 监听整个目录的文件 <li>创建配置文件job_flume_dir.conf  <img alt="" class="has" height="242" src="https://img-blog.csdnimg.cn/20190908163632722.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="764">   内容：    <pre class="has"><code class="language-bash">#把agent起个名叫a3,sources叫r3,sinks叫k3.hdfs,channels叫c3<br>a3.sources = r3<br>a3.sinks = k3<br>a3.channels = c3 </li>
</ol>
<h1 id="Describe-configure-the-source-1"><a href="#Describe-configure-the-source-1" class="headerlink" title="Describe/configure the source"></a>Describe/configure the source</h1><p>a3.sources.r3.type = spooldir<br>#要监听的目录<br>a3.sources.r3.spoolDir = /opt/module/apache-flume-1.7.0-bin/upload<br>#上传后的文件结尾<br>a3.sources.r3.fileSuffix = .COMPLETED<br>a3.sources.r3.fileHeader = true<br>#忽略所有以.tmp 结尾的文件，不上传<br>a3.sources.r3.ignorePattern = ([^ ]*.tmp) </p>
<h1 id="Describe-the-sink-1"><a href="#Describe-the-sink-1" class="headerlink" title="Describe the sink"></a>Describe the sink</h1><p>a3.sinks.k3.type = hdfs<br>a3.sinks.k3.hdfs.path = hdfs://hadoop102:9000/flume/upload/%Y%m%d/%H<br>#上传文件的前缀<br>a3.sinks.k3.hdfs.filePrefix = upload-<br>#是否按照时间滚动文件夹<br>a3.sinks.k3.hdfs.round = true<br>#多少时间单位创建一个新的文件夹<br>a3.sinks.k3.hdfs.roundValue = 1<br>#重新定义时间单位<br>a3.sinks.k3.hdfs.roundUnit = hour<br>#是否使用本地时间戳<br>a3.sinks.k3.hdfs.useLocalTimeStamp = true<br>#积攒多少个 Event 才 flush 到 HDFS 一次<br>a3.sinks.k3.hdfs.batchSize = 100<br>#设置文件类型，可支持压缩<br>a3.sinks.k3.hdfs.fileType = DataStream<br>#多久生成一个新的文件<br>a3.sinks.k3.hdfs.rollInterval = 600<br>#设置每个文件的滚动大小大概是 128M<br>a3.sinks.k3.hdfs.rollSize = 134217700<br>#文件的滚动与 Event 数量无关<br>a3.sinks.k3.hdfs.rollCount = 0<br>#最小副本数<br>a3.sinks.k3.hdfs.minBlockReplicas = 1 </p>
<h1 id="Use-a-channel-which-buffers-events-in-memory-1"><a href="#Use-a-channel-which-buffers-events-in-memory-1" class="headerlink" title="Use a channel which buffers events in memory"></a>Use a channel which buffers events in memory</h1><p>a3.channels.c3.type = memory<br>#通道中存储的最大事件数<br>a3.channels.c3.capacity = 1000<br>#每个事务通道从源或提供给接收器的最大事件数<br>a3.channels.c3.transactionCapacity = 100 </p>
<h1 id="Bind-the-source-and-sink-to-the-channel-1"><a href="#Bind-the-source-and-sink-to-the-channel-1" class="headerlink" title="Bind the source and sink to the channel"></a>Bind the source and sink to the channel</h1><p>a3.sources.r3.channels = c3<br>a3.sinks.k3.channel = c3 </code></pre>   </li>1.  创建监听的目录，并且添加测试的文件   <img alt="" class="has" height="211" src="https://img-blog.csdnimg.cn/20190908163901941.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="600">1. 执行测试：执行如下脚本后，请向 upload 文件夹中添加文件试试   <img alt="" class="has" height="311" src="https://img-blog.csdnimg.cn/20190908164224658.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="601">1. 查看upload目录  <img alt="" class="has" height="233" src="https://img-blog.csdnimg.cn/20190908164321635.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="737">  再添加2个文件  <img alt="" class="has" height="347" src="https://img-blog.csdnimg.cn/20190908164505248.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="588">1.  到hdfs上查看  <img alt="" class="has" height="259" src="https://img-blog.csdnimg.cn/20190908164653537.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="595">         <img alt="" class="has" height="207" src="https://img-blog.csdnimg.cn/20190908164809207.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="565"></p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Flume知识点入门学习二"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/Flume%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E4%BA%8C/"
    >Flume知识点入门学习二</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/Flume%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0%E4%BA%8C/" class="article-date">
  <time datetime="2021-07-18T14:10:35.019Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: Flume知识点入门学习二<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：案例四：Flume 与 Flume 之间数据传递，单 Flume 多 Channel、Sink</p>
<ol>
<li> 需求：使用 flume-1 监控文件变动，flume-1 将变动内容传递给 flume-2，flume-2 负责存储到HDFS。             同时 flume-1 将变动内容传递给 flume-3，flume-3 负责输出到local filesystem      <img alt="" class="has" height="319" src="https://img-blog.csdnimg.cn/20190908221502846.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="640"><li>创建 flume-1.conf，用于监控 hive.log 文件的变动，同时产生两个 channel 和两个 sink 分 别输送给 flume-2 和 flume3，文件名称为job_flume1.conf   <pre class="has"><code class="language-bash"># Name the components on this agent<br>#把agent起个名叫a1,sources叫r1,sinks叫k1 k2,channels叫c1 c2<br>a1.sources = r1<br>a1.sinks = k1 k2<br>a1.channels = c1 c2 <h1 id="将数据流复制给多个-channel"><a href="#将数据流复制给多个-channel" class="headerlink" title="将数据流复制给多个 channel"></a>将数据流复制给多个 channel</h1>a1.sources.r1.selector.type = replicating </li>
</ol>
<h1 id="Describe-configure-the-source"><a href="#Describe-configure-the-source" class="headerlink" title="Describe/configure the source"></a>Describe/configure the source</h1><p>#配置执行监控hive日志<br>a1.sources.r1.type = exec<br>a1.sources.r1.command = tail -F /opt/module/hive/logs/hive.log<br>a1.sources.r1.shell = /bin/bash -c </p>
<h1 id="Describe-the-sink"><a href="#Describe-the-sink" class="headerlink" title="Describe the sink"></a>Describe the sink</h1><p>#这里我们flume1中的sinks对接的是flume2,flume3中的sources,这种对接方式类型我们使用avro<br>a1.sinks.k1.type = avro<br>a1.sinks.k1.hostname = hadoop102<br>a1.sinks.k1.port = 4141 </p>
<p>a1.sinks.k2.type = avro<br>a1.sinks.k2.hostname = hadoop102<br>a1.sinks.k2.port = 4142 </p>
<h1 id="Describe-the-channel"><a href="#Describe-the-channel" class="headerlink" title="Describe the channel"></a>Describe the channel</h1><p>a1.channels.c1.type = memory<br>#通道中存储的最大事件数<br>a1.channels.c1.capacity = 1000<br>#每个事务通道从源或提供给接收器的最大事件数<br>a1.channels.c1.transactionCapacity = 100 </p>
<p>a1.channels.c2.type = memory<br>a1.channels.c2.capacity = 1000<br>a1.channels.c2.transactionCapacity = 100 </p>
<h1 id="Bind-the-source-and-sink-to-the-channel"><a href="#Bind-the-source-and-sink-to-the-channel" class="headerlink" title="Bind the source and sink to the channel"></a>Bind the source and sink to the channel</h1><p>a1.sources.r1.channels = c1 c2<br>a1.sinks.k1.channel = c1<br>a1.sinks.k2.channel = c2 </code></pre> <img alt="" class="has" height="184" src="https://img-blog.csdnimg.cn/20190908223703379.png" width="534"> </li><li> 创建 flume-2.conf，用于接收 flume-1 的 event，同时产生 1 个 channel 和 1 个 sink，将数据输送给 hdfs ,名称为job_flume2.conf    <pre class="has"><code class="language-bash"># Name the components on this agent<br>a2.sources = r1<br>a2.sinks = k1<br>a2.channels = c1 </p>
<h1 id="Describe-configure-the-source-1"><a href="#Describe-configure-the-source-1" class="headerlink" title="Describe/configure the source"></a>Describe/configure the source</h1><p>#表示这个flume的source去hadoop102服务器的4141获取数据<br>a2.sources.r1.type = avro<br>a2.sources.r1.bind = hadoop102<br>a2.sources.r1.port = 4141 </p>
<h1 id="Describe-the-sink-1"><a href="#Describe-the-sink-1" class="headerlink" title="Describe the sink"></a>Describe the sink</h1><p>#这个flume的sinks是将数据传输到hdfs上<br>a2.sinks.k1.type = hdfs<br>a2.sinks.k1.hdfs.path = hdfs://hadoop102:9000/flume2/%Y%m%d/%H<br>#上传文件的前缀<br>a2.sinks.k1.hdfs.filePrefix = flume2-<br>#是否按照时间滚动文件夹<br>a2.sinks.k1.hdfs.round = true<br>#多少时间单位创建一个新的文件夹<br>a2.sinks.k1.hdfs.roundValue = 1<br>#重新定义时间单位<br>a2.sinks.k1.hdfs.roundUnit = hour<br>#是否使用本地时间戳<br>a2.sinks.k1.hdfs.useLocalTimeStamp = true<br>#积攒多少个 Event 才 flush 到 HDFS 一次<br>a2.sinks.k1.hdfs.batchSize = 100<br>#设置文件类型，可支持压缩<br>a2.sinks.k1.hdfs.fileType = DataStream<br>#多久生成一个新的文件<br>a2.sinks.k1.hdfs.rollInterval = 600<br>#设置每个文件的滚动大小大概是 128M<br>a2.sinks.k1.hdfs.rollSize = 134217700<br>#文件的滚动与 Event 数量无关<br>a2.sinks.k1.hdfs.rollCount = 0<br>#最小冗余数<br>a2.sinks.k1.hdfs.minBlockReplicas = 1 </p>
<h1 id="Describe-the-channel-1"><a href="#Describe-the-channel-1" class="headerlink" title="Describe the channel"></a>Describe the channel</h1><p>a2.channels.c1.type = memory<br>a2.channels.c1.capacity = 1000<br>a2.channels.c1.transactionCapacity = 100 </p>
<h1 id="Bind-the-source-and-sink-to-the-channel-1"><a href="#Bind-the-source-and-sink-to-the-channel-1" class="headerlink" title="Bind the source and sink to the channel"></a>Bind the source and sink to the channel</h1><p>a2.sources.r1.channels = c1<br>a2.sinks.k1.channel = c1 </code></pre> <img alt="" class="has" height="184" src="https://img-blog.csdnimg.cn/20190908224342894.png" width="645"> </li><li>创建 flume-3.conf，用于接收 flume-1 的 event，同时产生 1 个 channel 和 1 个 sink，将数据输送给本地目录，名称job_flume3.conf   <pre class="has"><code class="language-bash"># Name the components on this agent<br>a3.sources = r1<br>a3.sinks = k1<br>a3.channels = c1 </p>
<h1 id="Describe-configure-the-source-2"><a href="#Describe-configure-the-source-2" class="headerlink" title="Describe/configure the source"></a>Describe/configure the source</h1><p>#表示这个flume的source去hadoop102服务器的4142获取数据<br>a3.sources.r1.type = avro<br>a3.sources.r1.bind = hadoop102<br>a3.sources.r1.port = 4142 </p>
<h1 id="Describe-the-sink-2"><a href="#Describe-the-sink-2" class="headerlink" title="Describe the sink"></a>Describe the sink</h1><p>#sinks将数据写到本地模式的类型<br>a3.sinks.k1.type = file_roll<br>#注意：输出的本地目录必须是已经存在的目录，如果该目录不存在，并不会创建新的目录<br>a3.sinks.k1.sink.directory = /opt/module/datas/flume3 </p>
<h1 id="Describe-the-channel-2"><a href="#Describe-the-channel-2" class="headerlink" title="Describe the channel"></a>Describe the channel</h1><p>a3.channels.c1.type = memory<br>a3.channels.c1.capacity = 1000<br>a3.channels.c1.transactionCapacity = 100 </p>
<h1 id="Bind-the-source-and-sink-to-the-channel-2"><a href="#Bind-the-source-and-sink-to-the-channel-2" class="headerlink" title="Bind the source and sink to the channel"></a>Bind the source and sink to the channel</h1><p>a3.sources.r1.channels = c1<br>a3.sinks.k1.channel = c1 </code></pre> <img alt="" class="has" height="246" src="https://img-blog.csdnimg.cn/20190908224909157.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="597"> </li>1. 启动hadoop集群，并且开3个hadoop102窗口，用来启动3个flume  <img alt="" class="has" height="137" src="https://img-blog.csdnimg.cn/20190908225340812.png" width="917">1. 执行测试：分别开启对应 flume-job（依次启动 flume-3，flume-2，flume-1），  <img alt="" class="has" height="253" src="https://img-blog.csdnimg.cn/2019090822573657.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="639">   <img alt="" class="has" height="242" src="https://img-blog.csdnimg.cn/20190908225805265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="637">  <img alt="" class="has" height="251" src="https://img-blog.csdnimg.cn/20190908225834660.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="642">1. 再开一个hadoop102操作hive  <img alt="" class="has" height="355" src="https://img-blog.csdnimg.cn/20190908230034423.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="649">1. 查看效果  ⑴先看flume3是否写到本地，如下，是有数据 的。       <img alt="" class="has" height="293" src="https://img-blog.csdnimg.cn/20190908232517648.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="678">   ⑵查看hdfs上数据           <img alt="" class="has" height="293" src="https://img-blog.csdnimg.cn/20190908232553876.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="660">
 </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Flume知识点入门学习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/Flume%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/"
    >Flume知识点入门学习</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/Flume%E7%9F%A5%E8%AF%86%E7%82%B9%E5%85%A5%E9%97%A8%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2021-07-18T14:10:35.011Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: Flume知识点入门学习<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—title: Flume知识点入门学习<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—1. 1. 1. </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Flume Taildir Source监听实时追加内容的文件"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/Flume%20Taildir%20Source%E7%9B%91%E5%90%AC%E5%AE%9E%E6%97%B6%E8%BF%BD%E5%8A%A0%E5%86%85%E5%AE%B9%E7%9A%84%E6%96%87%E4%BB%B6/"
    >Flume Taildir Source监听实时追加内容的文件</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/Flume%20Taildir%20Source%E7%9B%91%E5%90%AC%E5%AE%9E%E6%97%B6%E8%BF%BD%E5%8A%A0%E5%86%85%E5%AE%B9%E7%9A%84%E6%96%87%E4%BB%B6/" class="article-date">
  <time datetime="2021-07-18T14:10:35.003Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: Flume Taildir Source监听实时追加内容的文件<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—title: Flume Taildir Source监听实时追加内容的文件<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—1. 简介         flume中有三种可监控文件或目录的source、分别是Exec Source、Spooling Directory Source和Taildir Source。         Taildir Source是1.7版本的新特性，综合了Spooling Directory Source和Exec Source的优点。1. Exec Source使用场景         Exec Source可通过tail -f命令去tail住一个文件，然后实时同步日志到sink。但存在的问题是，当agent进程挂掉重启后，会有重复消费的问题。可以通过增加UUID来解决，或通过改进ExecSource来解决。1.  Spooling Directory Source使用场景        Spooling Directory Source可监听一个目录，同步目录中的新文件到sink,被同步完的文件可被立即删除或被打上标记。适合用于同步新文件，但不适合对实时追加日志的文件进行监听并同步。如果需要实时监听追加内容的文件，可对SpoolDirectorySource进行改进 1.  Taildir Source使用场景  Taildir Source可实时监控一批文件，并记录每个文件最新消费位置，agent进程重启后不会有重复消费的问题。  使用时建议用1.8.0版本的flume，1.8.0版本中解决了Taildir Source一个可能会丢数据的bug。 <li> TailSource配置如下   <pre class="has"><code class="language-bash"># source的名字<br>agent.sources = s1</p>
<h1 id="channels的名字"><a href="#channels的名字" class="headerlink" title="channels的名字"></a>channels的名字</h1><p>agent.channels = c1</p>
<h1 id="sink的名字"><a href="#sink的名字" class="headerlink" title="sink的名字"></a>sink的名字</h1><p>agent.sinks = r1</p>
<h1 id="指定source使用的channel"><a href="#指定source使用的channel" class="headerlink" title="指定source使用的channel"></a>指定source使用的channel</h1><p>agent.sources.s1.channels = c1</p>
<h1 id="指定sink使用的channel"><a href="#指定sink使用的channel" class="headerlink" title="指定sink使用的channel"></a>指定sink使用的channel</h1><p>agent.sinks.r1.channel = c1</p>
<p>######## source相关配置 ########</p>
<h1 id="source类型"><a href="#source类型" class="headerlink" title="source类型"></a>source类型</h1><p>agent.sources.s1.type = TAILDIR</p>
<h1 id="元数据位置"><a href="#元数据位置" class="headerlink" title="元数据位置"></a>元数据位置</h1><p>agent.sources.s1.positionFile = /Users/wangpei/tempData/flume/taildir_position.json</p>
<h1 id="监控的目录"><a href="#监控的目录" class="headerlink" title="监控的目录"></a>监控的目录</h1><p>agent.sources.s1.filegroups = f1<br>agent.sources.s1.filegroups.f1=/Users/wangpei/tempData/flume/data/.*log<br>agent.sources.s1.fileHeader = true</p>
<p>######## channel相关配置 ########</p>
<h1 id="channel类型"><a href="#channel类型" class="headerlink" title="channel类型"></a>channel类型</h1><p>agent.channels.c1.type = file</p>
<h1 id="数据存放路径"><a href="#数据存放路径" class="headerlink" title="数据存放路径"></a>数据存放路径</h1><p>agent.channels.c1.dataDirs = /Users/wangpei/tempData/flume/filechannle/dataDirs</p>
<h1 id="检查点路径"><a href="#检查点路径" class="headerlink" title="检查点路径"></a>检查点路径</h1><p>agent.channels.c1.checkpointDir = /Users/wangpei/tempData/flume/filechannle/checkpointDir</p>
<h1 id="channel中最多缓存多少"><a href="#channel中最多缓存多少" class="headerlink" title="channel中最多缓存多少"></a>channel中最多缓存多少</h1><p>agent.channels.c1.capacity = 1000</p>
<h1 id="channel一次最多吐给sink多少"><a href="#channel一次最多吐给sink多少" class="headerlink" title="channel一次最多吐给sink多少"></a>channel一次最多吐给sink多少</h1><p>agent.channels.c1.transactionCapacity = 100</p>
<p>######## sink相关配置 ########</p>
<h1 id="sink类型"><a href="#sink类型" class="headerlink" title="sink类型"></a>sink类型</h1><p>agent.sinks.r1.type = org.apache.flume.sink.kafka.KafkaSink</p>
<h1 id="brokers地址"><a href="#brokers地址" class="headerlink" title="brokers地址"></a>brokers地址</h1><p>agent.sinks.r1.kafka.bootstrap.servers = localhost:9092</p>
<h1 id="topic"><a href="#topic" class="headerlink" title="topic"></a>topic</h1><p>agent.sinks.r1.kafka.topic = testTopic3</p>
<h1 id="压缩"><a href="#压缩" class="headerlink" title="压缩"></a>压缩</h1><p>agent.sinks.r1.kafka.producer.compression.type = snappy<br></code></pre>   </li><li> 记录每个文件消费位置的元数据 <pre class="has"><code class="language-bash">#配置<br>agent.sources.s1.positionFile = /Users/wangpei/tempData/flume/taildir_position.json<br>#内容<br>[<br>&#123;<br>    "inode":6028358,<br>    "pos":144,<br>    "file":"/Users/wangpei/tempData/flume/data/test.log"<br>&#125;,<br>&#123;<br>    "inode":6028612,<br>    "pos":20,<br>    "file":"/Users/wangpei/tempData/flume/data/test_a.log"<br>&#125;<br>]  </p>
<p>可以看到，在taildir_position.json文件中，通过json数组的方式，记录了每个文件最新的消费位置，每消费一次便去更新这个文件。<br></code></pre> 转载自： </li></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-DataNode的工作机制，多目录配置，服役新数据节点，退役旧数据节点，Hadoop  存档"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/DataNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%9A%E7%9B%AE%E5%BD%95%E9%85%8D%E7%BD%AE%EF%BC%8C%E6%9C%8D%E5%BD%B9%E6%96%B0%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9%EF%BC%8C%E9%80%80%E5%BD%B9%E6%97%A7%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9%EF%BC%8CHadoop%20%C2%A0%E5%AD%98%E6%A1%A3/"
    >DataNode的工作机制，多目录配置，服役新数据节点，退役旧数据节点，Hadoop  存档</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/DataNode%E7%9A%84%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6%EF%BC%8C%E5%A4%9A%E7%9B%AE%E5%BD%95%E9%85%8D%E7%BD%AE%EF%BC%8C%E6%9C%8D%E5%BD%B9%E6%96%B0%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9%EF%BC%8C%E9%80%80%E5%BD%B9%E6%97%A7%E6%95%B0%E6%8D%AE%E8%8A%82%E7%82%B9%EF%BC%8CHadoop%20%C2%A0%E5%AD%98%E6%A1%A3/" class="article-date">
  <time datetime="2021-07-18T14:10:34.901Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: DataNode的工作机制，多目录配置，服役新数据节点，退役旧数据节点，Hadoop  存档<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—title: DataNode的工作机制，多目录配置，服役新数据节点，退役旧数据节点，Hadoop  存档<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—         rsync -rvl $pdir/$fname $user@hadoop$host:$pdir<br>done;</code></pre>   ⑸增加新增节点的ssh无密登录（102，103，104，105这几个节点上都要确保有所有节点的ssh免密登录）         注意：102的root用户也要配置对方的ssh免密登录   ⑹                  </li>1. 服役新节点具体步骤，在102服务器这个namenode节点上操作     ⑴在 102服务器namenode 的/opt/module/hadoop-2.7.2/etc/hadoop 目录下创建 dfs.hosts 文件           <img alt="" class="has" height="245" src="https://img-blog.csdnimg.cn/20190722220804852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="474">      ⑵在 namenode 的 hdfs-site.xml 配置文件中增加 dfs.hosts 属性             <img alt="" class="has" height="262" src="https://img-blog.csdnimg.cn/20190722221210500.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="413">      ⑶刷新 namenode            <img alt="" class="has" height="219" src="https://img-blog.csdnimg.cn/20190722221330463.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="552">      ⑷更新 resourcemanager 节点             <img alt="" class="has" height="83" src="https://img-blog.csdnimg.cn/20190722221432616.png" width="659">      ⑸可以在浏览器上发现多了一个节点            <img alt="" class="has" height="369" src="https://img-blog.csdnimg.cn/20190722221540752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="386">    ⑹在 namenode 的 slaves 文件中增加新主机名称            <img alt="" class="has" height="160" src="https://img-blog.csdnimg.cn/20190722221706325.png" width="227">    ⑺将修改的hdfs-site.xml,slaves文件同步到其它服务器          <img alt="" class="has" height="316" src="https://img-blog.csdnimg.cn/20190722221929771.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="365">   ⑻到105服务器上单独命令启动新的数据节点和节点管理器         a:启动datanode                  <img alt="" class="has" height="152" src="https://img-blog.csdnimg.cn/20190722222155289.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="465">         b：启动nodemanager               <img alt="" class="has" height="106" src="https://img-blog.csdnimg.cn/20190722222259780.png" width="575">    ⑼我们到浏览器看一下状态           <img alt="" class="has" height="362" src="https://img-blog.csdnimg.cn/20190722222355781.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="424">     ⑽如果数据不均衡，可以用命令实现集群的再平衡            <img alt="" class="has" height="165" src="https://img-blog.csdnimg.cn/2019072222273332.png" width="580">      </p>
<h1 id="四：退役旧数据节点"><a href="#四：退役旧数据节点" class="headerlink" title="四：退役旧数据节点"></a>四：退役旧数据节点</h1><ol>
<li>需求：        随着公司业务的减少，数据量变少了，原有的数据节点的容量有点多了， 需要在原有集群基础上动态减少一些数据节点。1. 环境准备    ⑴保持集群正在运行状态           <img alt="" class="has" height="372" src="https://img-blog.csdnimg.cn/2019072321450086.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="395">   ⑵要退的节点我们选择105   ⑶在 namenode 的/opt/module/hadoop-2.7.2/etc/hadoop 目录下创建 dfs.hosts.exclude 文件，       在文件中添加主机名称（要退役的节点）       <img alt="" class="has" height="168" src="https://img-blog.csdnimg.cn/20190723214828682.png" width="499">   ⑷在 namenode 的 hdfs-site.xml 配置文件中增加 dfs.hosts.exclude 属性       <img alt="" class="has" height="294" src="https://img-blog.csdnimg.cn/20190723215125482.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="527">  ⑸刷新 namenode、刷新 resourcemanager        <img alt="" class="has" height="144" src="https://img-blog.csdnimg.cn/20190723215254849.png" width="619">         <img alt="" class="has" height="77" src="https://img-blog.csdnimg.cn/20190723215344699.png" width="676">   ⑹检查 web 浏览器，退役节点的状态为 decommission in progress（退役中），说明数据节点正在复制块到其他节点        <img alt="" class="has" height="67" src="https://img-blog.csdnimg.cn/20190723215646506.png" width="763">    ⑺等待退役节点状态为 decommissioned（所有块已经复制完成），停止该节点及节点资源管理器。       注意：如果副本数是 3，服役的节点小于等于 3，是不能退役成功的，需要修改副本数后才能退役       <img alt="" class="has" height="320" src="https://img-blog.csdnimg.cn/20190723215747719.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="323">    ⑻到105节点上关闭指定的datanode和nodemanager         sbin/hadoop-daemon.sh stop datanode         sbin/yarn-daemon.sh stop nodemanager    ⑼后面还要进行下面操作          a:从 namenode 的 dfs.hosts 文件中删除退役节点 hadoop105               <img alt="" class="has" height="213" src="https://img-blog.csdnimg.cn/20190723221113495.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="377">          b：刷新 namenode，刷新 resourcemanager                  hdfs dfsadmin -refreshNodes                  yarn rmadmin -refreshNodes          c：从 namenode 的 slave 文件中删除退役节点 hadoop105                <img alt="" class="has" height="194" src="https://img-blog.csdnimg.cn/20190723221406524.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="333">         d:同步各个服务器的配置文件               <img alt="" class="has" height="199" src="https://img-blog.csdnimg.cn/20190723221539134.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="313">     ⑽最后再次看浏览器，105已经没了          <img alt="" class="has" height="366" src="https://img-blog.csdnimg.cn/20190723221627586.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="391">
 </li>
</ol>
<h1 id="五：集群间数据拷贝"><a href="#五：集群间数据拷贝" class="headerlink" title="五：集群间数据拷贝"></a>五：集群间数据拷贝</h1><ol>
<li>采用 discp 命令实现两个 hadoop 集群之间的递归数据复制  <img alt="" class="has" height="86" src="https://img-blog.csdnimg.cn/20190723225029499.png" width="678">           1. scp 实现两个远程主机之间的文件复制（目前一直在用的）<h1 id="六：Hadoop-存档"><a href="#六：Hadoop-存档" class="headerlink" title="六：Hadoop  存档"></a>六：Hadoop  存档</h1></li>
<li>简介       每个文件均按块存储，每个块的元数据存储在 namenode 的内存中，因此 hadoop 存储小文件会非常低效。 因为大量的小文件会耗尽 namenode 中的大部分内存。但注意，存储小文件所需要的磁盘容量和存储这些文件 原始内容所需要的磁盘空间相比也不会增多。例如，一个 1MB 的文件以大小为 128MB 的块存储，使用的是 1MB 的磁盘空间，而不是 128MB。       Hadoop 存档文件或 HAR 文件，是一个更高效的文件存档工具，它将文件存入 HDFS块，在减少 namenode 内存使用的同时，允许对文件进行透明的访问。具体说来，Hadoop存档文件可以用作 MapReduce 的输入。        就是说hadoop会将许多的小文件看成一个整体，这样就不会存在一个小文件占用一个数据块了。是这个Har 文件整体占用一个数据块。1.  案例实操     ⑴首先我们在hdfs上创建一个目录           <img alt="" class="has" height="96" src="https://img-blog.csdnimg.cn/2019072322591116.png" width="542">     ⑵在这个目录下上传3个小文件          <img alt="" class="has" height="224" src="https://img-blog.csdnimg.cn/20190723230038141.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="533">      ⑶归档文件：归档成一个叫做 xxx.har 的文件夹，该文件夹下有相应的数据文件。Xx.har 目录是                            一个整体，该目录看成是一个归档文件即可。                      下面这个命令表示，将/user/kgf/input目录下的锁文件归档到“/”根目录下，名称为myhar.har的归档文件           命令：hadoop archive -archiveName myhar.har -p /user/kgf/input /           效果：出现下面问题                 <img alt="" class="has" height="304" src="https://img-blog.csdnimg.cn/2019072323282198.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="775">         解决方法：发现集群重新启动一下就好了。             <img alt="" class="has" height="320" src="https://img-blog.csdnimg.cn/20190723234140806.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="479">        ⑷如何查看归档文件里面的文件呢？              <img alt="" class="has" height="138" src="https://img-blog.csdnimg.cn/20190723234325482.png" width="558">                   命令：hadoop fs -lsr har:///myhar.har              我们可以将这里面的文件拷贝到其它目录             <img alt="" class="has" height="177" src="https://img-blog.csdnimg.cn/20190723234526964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="654">             <img alt="" class="has" height="299" src="https://img-blog.csdnimg.cn/20190723234549256.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="590"> </li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-azkaban知识点学习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/azkaban%E7%9F%A5%E8%AF%86%E7%82%B9%E5%AD%A6%E4%B9%A0/"
    >azkaban知识点学习</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/azkaban%E7%9F%A5%E8%AF%86%E7%82%B9%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2021-07-18T14:10:34.895Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: azkaban知识点学习<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—title: azkaban知识点学习<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—1. </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/10/">上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/9/">9</a><a class="page-number" href="/page/10/">10</a><span class="page-number current">11</span><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/12/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2021
        <i class="ri-heart-fill heart_icon"></i> kgf
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 <a href="https://hexo.io" target="_blank">Hexo</a> 强力驱动
        <span class="division">|</span>
        主题 - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/image1.ico" alt="爱上口袋的天空"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/2019/about">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
  </div>
</body>

</html>