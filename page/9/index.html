<!DOCTYPE html>


<html lang="zh-CN">
  

    <head>
      <meta charset="utf-8" />
        
      <meta
        name="viewport"
        content="width=device-width, initial-scale=1, maximum-scale=1"
      />
      <title> 爱上口袋的天空</title>
  <meta name="generator" content="hexo-theme-ayer">
      
      <link rel="shortcut icon" href="/image1.ico" />
       
<link rel="stylesheet" href="/dist/main.css">

      <link
        rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/css/remixicon.min.css"
      />
      
<link rel="stylesheet" href="/css/custom.css">
 
      <script src="https://cdn.jsdelivr.net/npm/pace-js@1.0.2/pace.min.js"></script>
       
 

      <!-- mermaid -->
      
    </head>
  </html>
</html>


<body>
  <div id="app">
    
      
    <main class="content on">
      
<section class="cover">
    
      
      <a class="forkMe" href="https://github.com/Shen-Yu/hexo-theme-ayer"
        target="_blank"><img width="149" height="149" src="/images/forkme.png"
          class="attachment-full size-full" alt="Fork me on GitHub" data-recalc-dims="1"></a>
    
  <div class="cover-frame">
    <div class="bg-box">
      <img src="/images/cover1.jpg" alt="image frame" />
    </div>
    <div class="cover-inner text-center text-white">
      <h1><a href="/">爱上口袋的天空</a></h1>
      <div id="subtitle-box">
        
        <span id="subtitle"></span>
        
      </div>
      <div>
        
      </div>
    </div>
  </div>
  <div class="cover-learn-more">
    <a href="javascript:void(0)" class="anchor"><i class="ri-arrow-down-line"></i></a>
  </div>
</section>



<script src="https://cdn.jsdelivr.net/npm/typed.js@2.0.11/lib/typed.min.js"></script>


<!-- Subtitle -->

  <script>
    try {
      var typed = new Typed("#subtitle", {
        strings: ['欢迎来到爱上口袋的天空的博客', '愿你一生努力，一生被爱', '想要的都拥有，得不到的都释怀'],
        startDelay: 0,
        typeSpeed: 200,
        loop: true,
        backSpeed: 100,
        showCursor: true
      });
    } catch (err) {
      console.log(err)
    }
  </script>
  
<div id="main">
  <section class="outer">
  
  <ul class="ads">
    
        <li>
            <a target="_blank" rel="noopener" href="https://curl.qcloud.com/kvO7hb43">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/ten_1.jpg" width="300" alt="云服务器限时秒杀">
            </a>
        </li>
    
        <li>
            <a target="_blank" rel="noopener" href="https://www.vultr.com/?ref=8630075">
                <img src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/vultr.png" width="300" alt="vultr优惠vps">
            </a>
        </li>
    
</ul>
  
  
  

<div class="notice" style="margin-top:50px">
    <i class="ri-heart-fill"></i>
    <div class="notice-content" id="broad"></div>
</div>
<script type="text/javascript">
    fetch('https://v1.hitokoto.cn')
        .then(response => response.json())
        .then(data => {
            document.getElementById("broad").innerHTML = data.hitokoto;
        })
        .catch(console.error)
</script>

<style>
    .notice {
        padding: 20px;
        border: 1px dashed #e6e6e6;
        color: #969696;
        position: relative;
        display: inline-block;
        width: 100%;
        background: #fbfbfb50;
        border-radius: 10px;
    }

    .notice i {
        float: left;
        color: #999;
        font-size: 16px;
        padding-right: 10px;
        vertical-align: middle;
        margin-top: -2px;
    }

    .notice-content {
        display: initial;
        vertical-align: middle;
    }
</style>
  
  <article class="articles">
    
    
    
    
    <article
  id="post-clickhouse副本同步与高可用功能验证,分布式表与集群配置,数据副本与复制表,ZooKeeper整合,创建复制表,副本同步机制,数据原子写入与去重,负载平衡策略,案例"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/clickhouse%E5%89%AF%E6%9C%AC%E5%90%8C%E6%AD%A5%E4%B8%8E%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81,%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE,%E6%95%B0%E6%8D%AE%E5%89%AF%E6%9C%AC%E4%B8%8E%E5%A4%8D%E5%88%B6%E8%A1%A8,ZooKeeper%E6%95%B4%E5%90%88,%E5%88%9B%E5%BB%BA%E5%A4%8D%E5%88%B6%E8%A1%A8,%E5%89%AF%E6%9C%AC%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6,%E6%95%B0%E6%8D%AE%E5%8E%9F%E5%AD%90%E5%86%99%E5%85%A5%E4%B8%8E%E5%8E%BB%E9%87%8D,%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1%E7%AD%96%E7%95%A5,%E6%A1%88%E4%BE%8B/"
    >clickhouse副本同步与高可用功能验证,分布式表与集群配置,数据副本与复制表,ZooKeeper整合,创建复制表,副本同步机制,数据原子写入与去重,负载平衡策略,案例</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/clickhouse%E5%89%AF%E6%9C%AC%E5%90%8C%E6%AD%A5%E4%B8%8E%E9%AB%98%E5%8F%AF%E7%94%A8%E5%8A%9F%E8%83%BD%E9%AA%8C%E8%AF%81,%E5%88%86%E5%B8%83%E5%BC%8F%E8%A1%A8%E4%B8%8E%E9%9B%86%E7%BE%A4%E9%85%8D%E7%BD%AE,%E6%95%B0%E6%8D%AE%E5%89%AF%E6%9C%AC%E4%B8%8E%E5%A4%8D%E5%88%B6%E8%A1%A8,ZooKeeper%E6%95%B4%E5%90%88,%E5%88%9B%E5%BB%BA%E5%A4%8D%E5%88%B6%E8%A1%A8,%E5%89%AF%E6%9C%AC%E5%90%8C%E6%AD%A5%E6%9C%BA%E5%88%B6,%E6%95%B0%E6%8D%AE%E5%8E%9F%E5%AD%90%E5%86%99%E5%85%A5%E4%B8%8E%E5%8E%BB%E9%87%8D,%E8%B4%9F%E8%BD%BD%E5%B9%B3%E8%A1%A1%E7%AD%96%E7%95%A5,%E6%A1%88%E4%BE%8B/" class="article-date">
  <time datetime="2021-07-18T14:18:56.822Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Clickhouse/">Clickhouse</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: clickhouse副本同步与高可用功能验证,分布式表与集群配置,数据副本与复制表,ZooKeeper整合,创建复制表,副本同步机制,数据原子写入与去重,负载平衡策略,案例<br>categories:</p>
<ul>
<li>clickhouse</li>
</ul>
<p>—### 1.分布式表与集群配置</p>
<blockquote>
</blockquote>
<ul>
<li>分布式表基于Distributed引擎创建，在多个分片上运行分布式查询。- 读取是自动并行化的，可使用远程服务器上的索引（如果有）。- 数据在请求的本地服务器上尽可能地被部分处理。例如，对于GROUP BY查询，数据将在远程服务器 上聚合，聚合函数的中间状态将发送到请求服务器，然后数据将进一步聚合。<br>创建分布式表： <pre><code class="language-sql">ENGINE = Distributed(cluster_name, db_name, table_name[, sharding_key[, policy_name]])
</code></pre> </li>
</ul>
<p> <strong>参数：</strong> </p>
<ul>
<li><strong>cluster_name:集群名称。</strong>- <strong>db_name:数据库名称，可使用常量表达式：currentDatabase()。</strong>- <strong>table_name: 各分片上的表名称。</strong>- <strong>sharding_key: (可选)分片的key，可设置为rand()。</strong>- <strong>policy_name: (可选）策略名称，用于存储异步发送的临时文件。</strong><br>例如下面的/etc/metrika.xml的一部分内容： <pre><code class="language-XML">&lt;remote_servers&gt;
 &lt;logs&gt;
     &lt;shard&gt;
         &lt;weight&gt;1&lt;/weight&gt;
         &lt;internal_replication&gt;false&lt;/internal_replication&gt;
         &lt;replica&gt;
             &lt;host&gt;example01-01-1&lt;/host&gt;
             &lt;port&gt;9000&lt;/port&gt;
         &lt;/replica&gt;
         &lt;replica&gt;
             &lt;host&gt;example01-01-2&lt;/host&gt;
             &lt;port&gt;9000&lt;/port&gt;
         &lt;/replica&gt;
     &lt;/shard&gt;
     &lt;shard&gt;
         &lt;weight&gt;2&lt;/weight&gt;
         &lt;internal_replication&gt;false&lt;/internal_replication&gt;
         &lt;replica&gt;
             &lt;host&gt;example01-02-1&lt;/host&gt;
             &lt;port&gt;9000&lt;/port&gt;
         &lt;/replica&gt;
         &lt;replica&gt;
             &lt;host&gt;example01-02-2&lt;/host&gt;
             &lt;secure&gt;1&lt;/secure&gt;
             &lt;port&gt;9000&lt;/port&gt;
         &lt;/replica&gt;
     &lt;/shard&gt;
 &lt;/logs&gt;
&lt;/remote_servers&gt;
</code></pre> 
这里定义了一个名为logs的集群名称，它有两个分片（shard）组成，每个分片包含两个副本（replica）。<br>分片是包含数据的不同服务器（要读取所有数据，必须访问所有分片）。<br>副本是存储复制数据的服务器（要读取所有数据，访问该分片上的任意一个副本上的数据即可）。 </li>
</ul>
<p> 1.weight : 可选，写入数据时分片的权重，建议忽略该配置。 2.internal_repliacation : 可选，同一时刻是否只将数据写入其中一个副本。默认值：false（将数据写入所有副本），建议设置为true。写一个即可。避免重复写。 3.副本配置：配置每个Server的信息，必须参数:host和port，可选参数：user、password、secure和compression。 （1）、host ： 远程服务器地址。支持IPv4和IPv6。也可指定域名，更改域名解析需 重启服务。 （2）、port ： 消息传递的TCP端口。配置文件的tcp_port指定的端口，通常设置为 9000。 （3）、user ： 用于连接到服务的用户名称。默认值：true。在users.xml文件中配置 了访问权限。 （4）、password：用于连接到远程服务的密码。默认值：空字符串。 （5）、secure ： 使用ssl进行连接，通常还应该定义port=9440。 （6）、compression ： 使用数据压缩。默认值：true。 </p>
<h3 id="2-数据副本与复制表"><a href="#2-数据副本与复制表" class="headerlink" title="2.数据副本与复制表"></a>2.数据副本与复制表</h3><blockquote>
</blockquote>
<ul>
<li>只有MergeTree系列引擎支持数据副本，支持副本的引擎是在MergeTree引擎名称的前面加上前缀 Replicated。- 副本是表级别的而不是整个服务器级别的，因此服务器可以同时存储复制表和非复制表。- 副本不依赖于分片，每个分片都有自己独立的副本。</li>
</ul>
<p> <strong>副本表如：</strong> </p>
<ul>
<li><strong>ReplicatedMergeTree</strong>- <strong>ReplicatedSummingMergeTree</strong>- <strong>ReplicatedReplacingMergeTree</strong>- <strong>ReplicatedAggregatingMergeTree</strong>- <strong>ReplicatedCollapsingMergeTree</strong>- <strong>ReplicatedVersionedCollapsingMergeTree</strong>- <strong>ReplicatedGraphiteMergeTree</strong></li>
</ul>
<h3 id="3-ZooKeeper整合"><a href="#3-ZooKeeper整合" class="headerlink" title="3.ZooKeeper整合"></a>3.ZooKeeper整合</h3><blockquote>
<p> ClickHouse使用Apache ZooKeeper来存储副本元信息, 在配置文件设置 zookeeper相关的参数。 ClickHouse在创建复制表的时候指定Zookeeper的目录，指定的目录会在建 表时自动创建。 如果ClickHouse的配置文件未配置ZooKeeper， 则无法创建复制表， 并且 任何存量的复制表都将是只读的。 对本地复制表的查询，不会使用ZooKeeper， 其查询速度和非复制表一样快。<br> 本地复制表的数据插入，针对每个数据块（一个块最多有 max_insert_block_size = 1048576条记录），会通过几个事务将大约十个条目添加到Zookeeper。因此，与非复制表相比， 复制表的INSERT操作等 待时间稍长。<br> <pre><code class="language-XML">&lt;zookeeper&gt;<br>    &lt;node index="1"&gt;<br>        &lt;host&gt;example1&lt;/host&gt;<br>        &lt;port&gt;2181&lt;/port&gt;<br>    &lt;/node&gt;<br>    &lt;node index="2"&gt;<br>        &lt;host&gt;example2&lt;/host&gt;<br>        &lt;port&gt;2181&lt;/port&gt;<br>    &lt;/node&gt;<br>    &lt;node index="3"&gt;<br>        &lt;host&gt;example3&lt;/host&gt;<br>        &lt;port&gt;2181&lt;/port&gt;<br>    &lt;/node&gt;<br>&lt;/zookeeper&gt;<br></code></pre> 
   </p>
</blockquote>
<h3 id="4-创建复制表"><a href="#4-创建复制表" class="headerlink" title="4.创建复制表"></a>4.创建复制表</h3><blockquote>
<p> 复制表的引擎要以Replicated为前缀，例如：ReplicatedMergeTree。<br> <pre><code class="language-sql">CREATE TABLE table_name<br>(<br>    EventDate DateTime,<br>    CounterID UInt32,<br>    UserID UInt32<br>) ENGINE = ReplicatedMergeTree('/clickhouse/tables/&#123;layer&#125;-&#123;shard&#125;/table_name', '&#123;replica&#125;')<br>PARTITION BY toYYYYMM(EventDate)<br>ORDER BY (CounterID, EventDate, intHash32(UserID))<br>SAMPLE BY intHash32(UserID);<br></code></pre><br> 引擎参数包含了变量，这些变量是在配置文件的”macros”部分配置的，例如：<br> <pre><code class="language-XML">&lt;macros&gt;<br>    &lt;layer&gt;05&lt;/layer&gt;<br>    &lt;shard&gt;02&lt;/shard&gt;<br>    &lt;replica&gt;clickhouse1&lt;/replica&gt;<br>&lt;/macros&gt;<br></code></pre><br> <strong>Replicated*MergeTree引擎参数：</strong> zoo_path : ZooKeeper中表的路径。 replica_name : ZooKeeper中的副本名称。<br> 1.第一个参数ZooKeeper路径组成： （1）、通用前缀：/clickhouse/tables/,建议复制表都使用类似这样的前缀。 （2）、分片标识符：{layer}-{shard},在本示例中，分片标识符有两部分组成，只要保证分片标识符能唯一标识一个分片即可。 （3）、ZooKeeper节点名称：table_name。节点名称最好与表名相同，节点名称在定义后不会更改，即使执行表的重命名操作。<strong>2.第二个参数是副本名称，用于标识同一个分片的不同副本。副本名称只需要在每个shard中唯一即可。</strong><br> 上面的示例中，复制引擎的参数使用了变量替换。ClickHouse也支持使用显示的参数。在这种情况下，不能使用分布式的DDL查询（ON CLUSTER）。建议使用变量替换的方式传入参数，<br> 降低出错概率。<br> 在每个副本服务器上运行CREATE TABLE语句，如果该分片的表在其他节点已经创建且有数据，则该新副本自动同步其他副本的数据。 </p>
</blockquote>
<h3 id="5-副本同步机制"><a href="#5-副本同步机制" class="headerlink" title="5.副本同步机制"></a>5.副本同步机制</h3><blockquote>
</blockquote>
<ul>
<li>复制是多主异步的。- INSERT语句（以及ALTER）可在任意可用的服务器上执行。数据首先插入到本地的服务器 （即运行查询的服务器），然后数据被复制到其他服务器。- 由于复制是异步的，所以最近插入的数据出现在其他副本上会有一定的延迟。- 如果部分副本不可用，则在它们可用时写入数据。- 如果副本可用， 则等待的时间是通过网络传输压缩数据块所耗费的时间。- 默认情况下， INSERT操作只需等待一个副本写入成功后返回。如果仅将数据成功写入一个 副本，并且该副本的服务器不再存在， 则存储的数据将丢失。要启动来自多个副本的写入确 认机制，使用insert_quorum选项。</li>
</ul>
<h3 id="6-数据原子写入与去重"><a href="#6-数据原子写入与去重" class="headerlink" title="6.数据原子写入与去重"></a>6.数据原子写入与去重</h3><blockquote>
</blockquote>
<ul>
<li>INSERT查询按照数据块插入数据，每个数据块最多max_insert_block_size(默认 max_insert_block_size = 1048576)条记录。换言之， 如果INSERT插入少于1048576条记 录，则插入操作是原子的。单个数据块的写入是原子的。- 数据块是去重的。 对于同一数据块的多次写入（相同大小的的数据块，包含相同的行以及相 同的顺序），该块仅写入一次。在出现网口故障等异常情况下， 客户端应用程序不知道数据 是否已将数据成功写入数据库，因此可以简单地重复执行INSERT查询。相同的数据发送到哪 个副本进行插入并不重要，INSERT是幂等的。数据去重可通过参数 insert_deduplicate控 制，默认为0(开启去重)。- 在复制过程中， 只有插入的源数据通过网络传输。进一步的数据转换（合并）会在所有副本 上以相同的方式进行处理。 这样可以最大限度减少网络带宽占用，这意味着当副本位于不同 的数据中心时，复制的效果也很好。- ClickHouse内部监控副本的数据同步，并能够在发生故障后恢复。故障转义是自动的（对于数据的微小差异）或半自动的（当数据的差异太大时，这可能表示配置错误）。- ClickHouse内部监控副本上的数据同步，并能够在发生故障后恢复。故障转移是自动的（对于数据的微小差异）或半自动的（当数据差异太大时，这可能表示配置错误）。</li>
</ul>
<h3 id="7-负载平衡策略"><a href="#7-负载平衡策略" class="headerlink" title="7.负载平衡策略"></a>7.负载平衡策略</h3><blockquote>
<p> 执行分布式查询时，首先计算分片的每个副本的错误数，然后将查询发送至最少错误的副本。如果没有错误或者错误数相同，则按如下的策略查询数据： 1.random(默认) ： 将查询发送至任意一个副本。 2.nearest_hostname : 将查询发送至主机名最相似的副本。 3.in_order : 将查询按配置文件中的配置顺序发送至副本。 4.first_or_random : 选择第一个副本，如果第一个副本不可用，随机选择一个可用的副本。<br> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/202012162320232.png#pic_center"><br> <strong>设置策略的方式：</strong><br> <pre><code class="language-sql">set load_balancing = 'first_or_random';<br></code></pre> 
   </p>
</blockquote>
<h3 id="8-案例"><a href="#8-案例" class="headerlink" title="8.案例"></a>8.案例</h3><blockquote>
<p> <strong>1.在所有节点执行如下语句：</strong><br> **     **创建本地复制表：<br> <pre><code class="language-sql">CREATE TABLE table_local on cluster mycluster<br>(<br>    EventDate DateTime,<br>    CounterID UInt32,<br>    UserID UInt32<br>) ENGINE = ReplicatedMergeTree('/clickhouse/tables/&#123;layer&#125;-&#123;shard&#125;/table_local', '&#123;replica&#125;')<br>PARTITION BY toYYYYMM(EventDate)<br>ORDER BY (CounterID, EventDate, intHash32(UserID))<br>SAMPLE BY intHash32(UserID);<br></code></pre><br> <strong>执行效果图： 在docker01-node节点执行的效果图如下：</strong><br> <img alt="" height="669" src="https://img-blog.csdnimg.cn/20210424154112568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="1074"> <br> <strong>在docker02-node上执行后的效果如下（提示已经存在了）：</strong><br> <pre><code class="language-sql">docker02-node :) CREATE TABLE table_local on cluster mycluster<br>:-] (<br>:-]     EventDate DateTime,<br>:-]     CounterID UInt32,<br>:-]     UserID UInt32<br>:-] ) ENGINE = ReplicatedMergeTree('/clickhouse/tables/&#123;layer&#125;-&#123;shard&#125;/table_local', '&#123;replica&#125;')<br>:-] PARTITION BY toYYYYMM(EventDate)<br>:-] ORDER BY (CounterID, EventDate, intHash32(UserID))<br>:-] SAMPLE BY intHash32(UserID);</p>
</blockquote>
<p>CREATE TABLE table_local ON CLUSTER mycluster<br>(<br>    <code>EventDate</code> DateTime,<br>    <code>CounterID</code> UInt32,<br>    <code>UserID</code> UInt32<br>)<br>ENGINE = ReplicatedMergeTree(‘/clickhouse/tables/{layer}-{shard}/table_local’, ‘{replica}’)<br>PARTITION BY toYYYYMM(EventDate)<br>ORDER BY (CounterID, EventDate, intHash32(UserID))<br>SAMPLE BY intHash32(UserID)</p>
<p>┌─host──────────┬─port─┬─status─┬─error─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─num_hosts_remaining─┬─num_hosts_active─┐<br>│ 192.168.56.13 │ 9000 │     57 │ Code: 57, e.displayText() = DB::Exception: Table default.table_local already exists. (version 20.1.4.14 (official build)) │                   3 │                1 │<br>│ 192.168.56.11 │ 9000 │     57 │ Code: 57, e.displayText() = DB::Exception: Table default.table_local already exists. (version 20.1.4.14 (official build)) │                   2 │                1 │<br>│ 192.168.56.10 │ 9000 │     57 │ Code: 57, e.displayText() = DB::Exception: Table default.table_local already exists. (version 20.1.4.14 (official build)) │                   1 │                1 │<br>└───────────────┴──────┴────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┴──────────────────┘<br>┌─host──────────┬─port─┬─status─┬─error─────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┬─num_hosts_remaining─┬─num_hosts_active─┐<br>│ 192.168.56.12 │ 9000 │     57 │ Code: 57, e.displayText() = DB::Exception: Table default.table_local already exists. (version 20.1.4.14 (official build)) │                   0 │                0 │<br>└───────────────┴──────┴────────┴───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┴─────────────────────┴──────────────────┘<br>↓ Progress: 0.00 rows, 0.00 B (0.00 rows/s., 0.00 B/s.)                                                                                                                                0%Received exception from server (version 20.1.4):<br>Code: 57. DB::Exception: Received from 192.168.56.11:9000. DB::Exception: There was an error on [192.168.56.13:9000]: Code: 57, e.displayText() = DB::Exception: Table default.table_local already exists. (version 20.1.4.14 (official build)).</p>
<p>4 rows in set. Elapsed: 0.617 sec.</p>
<p>docker02-node :)</code></pre><br> 通过上面的案例可以知道，只要在一个节点上创建了副本表之后，在其它节点上也已经存在了。<br> <strong>创建分布式表(每个节点都要创建)：</strong><br> <pre><code class="language-sql">CREATE TABLE table_distributed as table_local ENGINE = Distributed(mycluster, default, table_local, rand());<br></code></pre><br> <strong>2.验证副本的复制****在clickhouse1上，对本地表操作。</strong><br> <img alt="" height="505" src="https://img-blog.csdnimg.cn/20210424154842225.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="903"><br> <strong>在docker02-node上（docker02-node的分片副本节点）上，验证数据是否同步：</strong><br> <img alt="" height="335" src="https://img-blog.csdnimg.cn/20210424154940833.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="961"><br> <strong>在docker03-node和docker04-node上执行（即shard2上）。发现查询不到结果，效果如下：</strong><br> <img alt="" height="218" src="https://img-blog.csdnimg.cn/20210424155132498.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="558"><br> <img alt="" height="250" src="https://img-blog.csdnimg.cn/20210424155146647.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="557"><br> 3.<strong>验证集群的功能</strong><br> 在任意节点查看分布式表的数据（都将出现下面的效果）。<br> <img alt="" height="277" src="https://img-blog.csdnimg.cn/20210424155259421.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="738"><br> 在任意一个节点往分布式表里面插入5条数据：<br> <pre><code class="language-sql">insert into table_distributed values('2020-03-11 12:12:31', 21, 1);    docker04-node上执行<br>insert into table_distributed values('2020-03-12 12:12:32', 22, 2);    docker04-node上执行<br>insert into table_distributed values('2020-03-13 12:12:33', 23, 3);    docker03-node上执行<br>insert into table_distributed values('2020-03-14 12:12:34', 24, 4);    docker03-node上执行<br>insert into table_distributed values('2020-03-15 12:12:35', 25, 5);    docker02-node上执行<br></code></pre><br> 然后在任意一台机器上执行：<br> <pre><code class="language-sql">select * from table_distributed;<br></code></pre><br> 都可以看到：<br> <pre><code class="language-sql">docker02-node :) select * from table_distributed;</p>
<p>SELECT *<br>FROM table_distributed</p>
<p>┌───────────EventDate─┬─CounterID─┬─UserID─┐<br>│ 2020-03-11 12:12:33 │        22 │     37 │<br>└─────────────────────┴───────────┴────────┘<br>┌───────────EventDate─┬─CounterID─┬─UserID─┐<br>│ 2020-03-12 12:12:32 │        22 │      2 │<br>└─────────────────────┴───────────┴────────┘<br>┌───────────EventDate─┬─CounterID─┬─UserID─┐<br>│ 2020-03-11 12:12:31 │        21 │      1 │<br>└─────────────────────┴───────────┴────────┘<br>┌───────────EventDate─┬─CounterID─┬─UserID─┐<br>│ 2020-03-14 12:12:34 │        24 │      4 │<br>└─────────────────────┴───────────┴────────┘<br>┌───────────EventDate─┬─CounterID─┬─UserID─┐<br>│ 2020-03-13 12:12:33 │        23 │      3 │<br>└─────────────────────┴───────────┴────────┘<br>┌───────────EventDate─┬─CounterID─┬─UserID─┐<br>│ 2020-03-15 12:12:35 │        25 │      5 │<br>└─────────────────────┴───────────┴────────┘</p>
<p>6 rows in set. Elapsed: 0.007 sec.</p>
<p>docker02-node :)</code></pre><br> 然后，分别在两个分片的主机上查询本地表：<strong>在docker01-node上（shard1）发现的效果是：</strong><br> <img alt="" height="445" src="https://img-blog.csdnimg.cn/2021042416000130.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="555"><br> <strong>在docker03-node上（shard2）发现的效果是：</strong><br> <img alt="" height="340" src="https://img-blog.csdnimg.cn/20210424160051386.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="564"><br> 可以看到，使用分布式表插入数据，数据分散到不同分片（shard）的本地表。 </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-clickhouse分布式集群部署"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/clickhouse%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/"
    >clickhouse分布式集群部署</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/clickhouse%E5%88%86%E5%B8%83%E5%BC%8F%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/" class="article-date">
  <time datetime="2021-07-18T14:18:56.820Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Clickhouse/">Clickhouse</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: clickhouse分布式集群部署<br>categories:</p>
<ul>
<li>clickhouse</li>
</ul>
<p>—### 1.节点规划</p>
<blockquote>
 <table align="left" border="1" cellpadding="1" cellspacing="1"><thead><th style="width:268px;text-align:center;vertical-align:middle;">主机名</th><th style="width:265px;text-align:center;vertical-align:middle;">IP地址</th><th style="width:250px;text-align:center;vertical-align:middle;">分片</th><th style="width:213px;text-align:center;vertical-align:middle;">副本</th>
</blockquote>
</thead><tbody><td style="width:268px;">docker01-node</td><td style="width:265px;">192.168.56.10</td><td style="width:250px;">shard1</td><td style="width:213px;">副本1</td>
<td style="width:268px;">docker02-node</td><td style="width:265px;">192.168.56.11</td><td style="width:250px;">shard1</td><td style="width:213px;">副本2</td>
<td style="width:268px;">docker03-node</td><td style="width:265px;">192.168.56.12</td><td style="width:250px;">shard2</td><td style="width:213px;">副本1</td>
<td style="width:268px;">docker04-node</td><td style="width:265px;">192.168.56.13</td><td style="width:250px;">shard2</td><td style="width:213px;"> 副本2 </td>
</tbody></table>


<p>规划4个节点， 2个分片， 每个分片2个副本。 分片1的副本在主机clickhouse1和clickhouse2上， 2分片的副本在主机clickhouse3和clickhouse4上。 </p>
<h3 id="2-操作系统准备工作"><a href="#2-操作系统准备工作" class="headerlink" title="2.操作系统准备工作"></a>2.<strong>操作系统准备工作</strong></h3><blockquote>
<p> 1）<strong>修改主机名</strong><br> **      <strong>hostname按照上面主机名进行修改。<br> 2）</strong>关闭防火墙、selinux等。**<br> <pre><code class="language-bash">关闭防火墙：<br>systemctl stop firewalld.service<br>systemctl disable firewalld.service<br>systemctl is-enabled firewalld.service</p>
</blockquote>
<p>selinux的配置：<br>vim /etc/sysconfig/selinux<br>SELINUX=enforcing 改为 SELINUX=disabled</p>
<p>检查SELinux的状态<br>[root@localhost etc]# getenforce<br>Disabled<br>[root@localhost etc]#<br></code></pre><br> 3）<strong>配置/etc/hosts</strong><br> **        **必须在/etc/hosts配置主机名和ip地址映射关系， 否则副本之间的数据不能同步。<br> <img alt="" height="183" src="https://img-blog.csdnimg.cn/20210424113605739.png" width="925"> </p>
<h3 id="3-安装配置zookeeper"><a href="#3-安装配置zookeeper" class="headerlink" title="3.安装配置zookeeper"></a><strong>3.安装配置zookeeper</strong></h3><blockquote>
<p> 下载zookeeper,zookeeper版本要求3.4.5以上。<br> 1)将下载的安装包上传到Linux上<br>       <img alt="" height="158" src="https://img-blog.csdnimg.cn/20210424122424231.png" width="872"><br> 2)将conf目录下的zoo_sample.cfg复制一份，命名为：zoo.cfg<br>       <img alt="" height="203" src="https://img-blog.csdnimg.cn/20210424122632832.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="735"><br>       在zoo.cfg增加配置：<br>       <img alt="" height="146" src="https://img-blog.csdnimg.cn/20210424123230341.png" width="696"><br>     上面配置目录需要手工创建。<br> 3)<strong>环境配置：</strong><br> <pre><code class="language-bash">export ZOOKEEPER_HOME=/opt/zk/apache-zookeeper-3.6.2-bin/<br>export PATH=$PATH:$ZOOKEEPER_HOME/bin<br></code></pre><br> **4)**然后启动zookeeper即可。<br>      启动命令： ./bin/zkServer.sh start<br>      <img alt="" height="646" src="https://img-blog.csdnimg.cn/20210424124150418.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="868"> </p>
</blockquote>
<h3 id="4-在所有的主机安装clickhouse-参考之前的安装即可"><a href="#4-在所有的主机安装clickhouse-参考之前的安装即可" class="headerlink" title="4.在所有的主机安装clickhouse,参考之前的安装即可"></a><strong>4.在所有的主机安装clickhouse,参考之前的安装即可</strong></h3><h3 id="5-修改clickhouse的网络相关配置"><a href="#5-修改clickhouse的网络相关配置" class="headerlink" title="5.修改clickhouse的网络相关配置"></a><strong>5.修改clickhouse的网络相关配置</strong></h3><blockquote>
<p> 修改配置文件：/etc/clickhouse-server/config.xml<br> 打开以下注释，并做相关修改：<br> <img alt="" height="116" src="https://img-blog.csdnimg.cn/20210424130017519.png" width="731"><br> <strong>clickhouse1上的修改如下：</strong><br> <img alt="" height="104" src="https://img-blog.csdnimg.cn/20210424130128454.png" width="552"><br> <strong>Clickhouse2上的修改如下：</strong><br> <img alt="" height="75" src="https://img-blog.csdnimg.cn/20210424130246450.png" width="553"><br> <strong>Clickhouse3上的修改如下：</strong><br> <img alt="" height="91" src="https://img-blog.csdnimg.cn/20210424130315221.png" width="507"><br> <strong>Clickhouse4上的修改如下：</strong><br> <img alt="" height="115" src="https://img-blog.csdnimg.cn/20210424130406733.png" width="538"> </p>
</blockquote>
<h3 id="6-增加配置文件：-etc-metrika-xml"><a href="#6-增加配置文件：-etc-metrika-xml" class="headerlink" title="6.增加配置文件：/etc/metrika.xml"></a><strong>6.增加配置文件：/etc/metrika.xml</strong></h3><blockquote>
<p> docker01-node的配置如下：<br> <pre><code class="language-XML">&lt;?xml version="1.0" encoding="utf-8"?&gt;</p>
</blockquote>
<p>&lt;yandex&gt;<br>  &lt;clickhouse_remote_servers&gt;<br>    &lt;mycluster&gt;<br>      &lt;shard&gt;<br>        &lt;internal_replication&gt;true&lt;/internal_replication&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.10&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.11&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>      &lt;/shard&gt;<br>      &lt;shard&gt;<br>        &lt;internal_replication&gt;true&lt;/internal_replication&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.12&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.13&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>      &lt;/shard&gt;<br>    &lt;/mycluster&gt;<br>  &lt;/clickhouse_remote_servers&gt;<br>  &lt;zookeeper-servers&gt;<br>    &lt;node index=”1”&gt;<br>      &lt;host&gt;192.168.56.10&lt;/host&gt;<br>      &lt;port&gt;2181&lt;/port&gt;<br>    &lt;/node&gt;<br>  &lt;/zookeeper-servers&gt;<br>  &lt;macros&gt;<br>    &lt;layer&gt;01&lt;/layer&gt;<br>    &lt;shard&gt;01&lt;/shard&gt;<br>    &lt;replica&gt;192.168.56.10&lt;/replica&gt;<br>  &lt;/macros&gt;<br>&lt;/yandex&gt;<br></code></pre><br> docker02-node的配置如下：<br> <pre><code class="language-XML">&lt;?xml version="1.0" encoding="utf-8"?&gt;</p>
<p>&lt;yandex&gt;<br>  &lt;clickhouse_remote_servers&gt;<br>    &lt;mycluster&gt;<br>      &lt;shard&gt;<br>        &lt;internal_replication&gt;true&lt;/internal_replication&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.10&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.11&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>      &lt;/shard&gt;<br>      &lt;shard&gt;<br>        &lt;internal_replication&gt;true&lt;/internal_replication&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.12&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.13&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>      &lt;/shard&gt;<br>    &lt;/mycluster&gt;<br>  &lt;/clickhouse_remote_servers&gt;<br>  &lt;zookeeper-servers&gt;<br>    &lt;node index=”1”&gt;<br>      &lt;host&gt;192.168.56.10&lt;/host&gt;<br>      &lt;port&gt;2181&lt;/port&gt;<br>    &lt;/node&gt;<br>  &lt;/zookeeper-servers&gt;<br>  &lt;macros&gt;<br>    &lt;layer&gt;01&lt;/layer&gt;<br>    &lt;shard&gt;01&lt;/shard&gt;<br>    &lt;replica&gt;192.168.56.11&lt;/replica&gt;<br>  &lt;/macros&gt;<br>&lt;/yandex&gt;<br></code></pre><br> docker03-node<strong>的配置如下：</strong><br> <pre><code class="language-XML">&lt;?xml version="1.0" encoding="utf-8"?&gt;<br>&lt;yandex&gt;<br>  &lt;clickhouse_remote_servers&gt;<br>    &lt;mycluster&gt;<br>      &lt;shard&gt;<br>        &lt;internal_replication&gt;true&lt;/internal_replication&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.10&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.11&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>      &lt;/shard&gt;<br>      &lt;shard&gt;<br>        &lt;internal_replication&gt;true&lt;/internal_replication&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.12&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.13&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>      &lt;/shard&gt;<br>    &lt;/mycluster&gt;<br>  &lt;/clickhouse_remote_servers&gt;<br>  &lt;zookeeper-servers&gt;<br>    &lt;node index="1"&gt;<br>      &lt;host&gt;192.168.56.10&lt;/host&gt;<br>      &lt;port&gt;2181&lt;/port&gt;<br>    &lt;/node&gt;<br>  &lt;/zookeeper-servers&gt;<br>  &lt;macros&gt;<br>    &lt;layer&gt;01&lt;/layer&gt;<br>    &lt;shard&gt;02&lt;/shard&gt;<br>    &lt;replica&gt;192.168.56.12&lt;/replica&gt;<br>  &lt;/macros&gt;<br>&lt;/yandex&gt;<br></code></pre><br> docker04-node<strong>的配置如下：</strong><br> <pre><code class="language-XML">&lt;?xml version="1.0" encoding="utf-8"?&gt;<br>&lt;yandex&gt;<br>  &lt;clickhouse_remote_servers&gt;<br>    &lt;mycluster&gt;<br>      &lt;shard&gt;<br>        &lt;internal_replication&gt;true&lt;/internal_replication&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.10&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.11&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>      &lt;/shard&gt;<br>      &lt;shard&gt;<br>        &lt;internal_replication&gt;true&lt;/internal_replication&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.12&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>        &lt;replica&gt;<br>          &lt;host&gt;192.168.56.13&lt;/host&gt;<br>          &lt;port&gt;9000&lt;/port&gt;<br>        &lt;/replica&gt;<br>      &lt;/shard&gt;<br>    &lt;/mycluster&gt;<br>  &lt;/clickhouse_remote_servers&gt;<br>  &lt;zookeeper-servers&gt;<br>    &lt;node index="1"&gt;<br>      &lt;host&gt;192.168.56.10&lt;/host&gt;<br>      &lt;port&gt;2181&lt;/port&gt;<br>    &lt;/node&gt;<br>  &lt;/zookeeper-servers&gt;<br>  &lt;macros&gt;<br>    &lt;layer&gt;01&lt;/layer&gt;<br>    &lt;shard&gt;02&lt;/shard&gt;<br>    &lt;replica&gt;192.168.56.13&lt;/replica&gt;<br>  &lt;/macros&gt;<br>&lt;/yandex&gt;<br></code></pre><br> 其中如下这段配置在每个节点是不相同的：<br> <pre><code class="language-XML">&lt;macros&gt;<br>    &lt;layer&gt;01&lt;/layer&gt;<br>    &lt;shard&gt;01&lt;/shard&gt;<br>    &lt;replica&gt;192.168.56.10&lt;/replica&gt;<br>&lt;/macros&gt;<br></code></pre><br> layer表示分层， shard表示分片的编号， 按照配置顺序从1开始。这里的01表示第一个分片。 replica配置副本的标识， 这里配置为本机的主机名。 使用这三个参数可以唯一表示一个副本分片。 这里表示layer为01的分片1的副本，副本标识：192.168.106.103。<br> 配置完成后， 在每个节点启动ClickHouse服务。 systemctl start clickhouse-server<br> 查看状态：systemctl status clickhouse-server<br> <img alt="" height="868" src="https://img-blog.csdnimg.cn/20210424133344316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="1200"> </p>
<h3 id="7-进入docker01-node查看系统表"><a href="#7-进入docker01-node查看系统表" class="headerlink" title="7.进入docker01-node查看系统表"></a><strong>7.进入</strong>docker01-node查看系统表</h3><p>命令：clickhouse-client -h 服务器ip地址</p>
<img alt="" height="169" src="https://img-blog.csdnimg.cn/20210424133428164.png" width="707">

<p>查看系统表命令：select * from system.clusters where cluster=’mycluster’;</p>
<img alt="" height="359" src="https://img-blog.csdnimg.cn/20210424133513204.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="1200">
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-clickhouse20.1.4.14-2单机版安装"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/clickhouse20.1.4.14-2%E5%8D%95%E6%9C%BA%E7%89%88%E5%AE%89%E8%A3%85/"
    >clickhouse20.1.4.14-2单机版安装</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/clickhouse20.1.4.14-2%E5%8D%95%E6%9C%BA%E7%89%88%E5%AE%89%E8%A3%85/" class="article-date">
  <time datetime="2021-07-18T14:18:56.818Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Clickhouse/">Clickhouse</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: clickhouse20.1.4.14-2单机版安装<br>categories:</p>
<ul>
<li>clickhouse</li>
</ul>
<p>—## 1.安装文件清单</p>
<blockquote>
 <img alt="" height="126" src="https://img-blog.csdnimg.cn/20210405111422953.png" width="481"> 
</blockquote>
<h2 id="2-安装方式"><a href="#2-安装方式" class="headerlink" title="2.安装方式"></a>2.安装方式</h2><blockquote>
</blockquote>
<ul>
<li>rpm方式，这种方式适用于环境的依赖都很全，不需要安装其他的环境依赖包  命令：rpm -ivh ./*.rpm- yum方式，这种方式依赖于yum，但是不需要安装其他的环境依赖包，缺少的话会自动帮你安装  命令：yum install *.rpm</li>
</ul>
<h3 id="3-使用rpm方式安装"><a href="#3-使用rpm方式安装" class="headerlink" title="3.使用rpm方式安装"></a>3.使用rpm方式安装</h3><p>   1)在/opt/clickhouse目录下        <img alt="" height="250" src="https://img-blog.csdnimg.cn/20210405112658964.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="1004"></p>
<p>   2）安装           <img alt="" height="295" src="https://img-blog.csdnimg.cn/20210405112853346.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="1200">        </p>
<h3 id="4-启动clickhouse命令"><a href="#4-启动clickhouse命令" class="headerlink" title="4.启动clickhouse命令"></a>4.启动clickhouse命令</h3><blockquote>
<p> systemctl start clickhouse-server </p>
</blockquote>
<h3 id="5-查看状态命令"><a href="#5-查看状态命令" class="headerlink" title="5.查看状态命令"></a>5.查看状态命令</h3><blockquote>
<p> systemctl status clickhouse-server<br> <img alt="" height="297" src="https://img-blog.csdnimg.cn/20210405113223195.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="1200"> </p>
</blockquote>
<h3 id="6-重启命令"><a href="#6-重启命令" class="headerlink" title="6.重启命令"></a>6.重启命令</h3><blockquote>
<p> systemctl restart clickhouse-server </p>
</blockquote>
<h3 id="7-使用客户端命令进入"><a href="#7-使用客户端命令进入" class="headerlink" title="7.使用客户端命令进入"></a>7.使用客户端命令进入</h3><blockquote>
<p> 命令：clickhouse-client -m<br> <img alt="" height="373" src="https://img-blog.csdnimg.cn/20210405113420792.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="814"> </p>
</blockquote>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-clickhouse,硬件管理与优化(cpu,内存,网络,存储,操作系统配置),profile管理，Quotas设置，约束管理，查询权限，用户管理配置等"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/clickhouse,%E7%A1%AC%E4%BB%B6%E7%AE%A1%E7%90%86%E4%B8%8E%E4%BC%98%E5%8C%96(cpu,%E5%86%85%E5%AD%98,%E7%BD%91%E7%BB%9C,%E5%AD%98%E5%82%A8,%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE),profile%E7%AE%A1%E7%90%86%EF%BC%8CQuotas%E8%AE%BE%E7%BD%AE%EF%BC%8C%E7%BA%A6%E6%9D%9F%E7%AE%A1%E7%90%86%EF%BC%8C%E6%9F%A5%E8%AF%A2%E6%9D%83%E9%99%90%EF%BC%8C%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE%E7%AD%89/"
    >clickhouse,硬件管理与优化(cpu,内存,网络,存储,操作系统配置),profile管理，Quotas设置，约束管理，查询权限，用户管理配置等</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/clickhouse,%E7%A1%AC%E4%BB%B6%E7%AE%A1%E7%90%86%E4%B8%8E%E4%BC%98%E5%8C%96(cpu,%E5%86%85%E5%AD%98,%E7%BD%91%E7%BB%9C,%E5%AD%98%E5%82%A8,%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E9%85%8D%E7%BD%AE),profile%E7%AE%A1%E7%90%86%EF%BC%8CQuotas%E8%AE%BE%E7%BD%AE%EF%BC%8C%E7%BA%A6%E6%9D%9F%E7%AE%A1%E7%90%86%EF%BC%8C%E6%9F%A5%E8%AF%A2%E6%9D%83%E9%99%90%EF%BC%8C%E7%94%A8%E6%88%B7%E7%AE%A1%E7%90%86%E9%85%8D%E7%BD%AE%E7%AD%89/" class="article-date">
  <time datetime="2021-07-18T14:18:56.782Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Clickhouse/">Clickhouse</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: clickhouse,硬件管理与优化(cpu,内存,网络,存储,操作系统配置),profile管理，Quotas设置，约束管理，查询权限，用户管理配置等<br>categories:</p>
<ul>
<li>clickhouse</li>
</ul>
<p>—### 1.运维管理与优化</p>
<blockquote>
<p> <strong>1.1)硬件管理与优化</strong><br>       1.1.1)CPU<br>                ClickHouse的并行数据处理机制，使得其能利用所有可用的硬件资源。在选择CPU处理器时，应选择更多核心数而不是更高频率的处理器。<br>                例如：频率为2600MHz的16核心处理器比3600MHz的8核心的处理器的效率更高。                           建议使用Turbo Boost和超线程技术，在高工作负载情况下能显著提高性能。<br>       1.1.2)内存<br>                ClickHouse的服务本身需要的RAM很少，但是ClickHouse需要内存来处理查询，建议至少使用4GB内存。                RAM的容量取决于：<br>                 a.查询的复杂性。<br>                 b.查询处理的数据量。<br>                 c.根据GROUP BY、DISTINCT、JOIN和其他操作产生的临时数据大小估算所需的RAM容量。<br>                对于少量数据，例如压缩后在200GB以下，使用数据量一样多的内存。                对于大量数据，以及交互式查询，热数据集缓存在操作系统的cache中，建议配置尽可能大的内存，例如128GB的内存性能明显比64GB高很多。<br>                如果内存不足， ClickHouse支持使用外部存储器处理数据，如外部聚合和外部排序等，当数据超过设置的阈值时使用外部存储器处理数据。<br>       1.1.3)网络<br>                推荐使用10GB或更高带宽的网络。<br>                ClickHouse在数据处理过程（数据导入、中间数据存储等）中会充分利用网络带宽。网络带宽对于处理具有大量中间数据的分布式查询至关重要。<br>                网络带宽会影响复制的过程。<br>       1.1.4)存储综合考虑安全性、性能和预算。                  如果使用RAID0，则需考虑使用副本机制。                  预算足够，使用SSD，然后HDD。                  RAID10的性能最佳，兼顾了安全性和性能，但是成本高，50%的磁盘利用率。                 主机的磁盘超过4块，可以考虑使用RAID6、RAID50、RAID5等。<br>                 优先使用具有多个本地磁盘的服务器，而不是具有附加磁盘架的服务器。<br>                 考虑压缩率、副本、磁盘存储容量、未来的数据增长估算存储资源。                 数据量：对数据进行采样，计算每行的平均大小，根据总行数估算数据量。                数据压缩系数：ClickHouse的数据通常被压缩5倍左右，根据原始数据大小和ClickHouse存储的表的数据目录占用空间进行比较。                副本：每份副本存储完整的数据。<br> <strong>1.2)操作系统的配置</strong><br>        1.2.1)CPU频率调整策略<br>              建议将CPU的电源管理策略调整为performance，即将CPU频率固定工作在其支持的最高运行频率上， 不动态调节，可以获取到最大的性能。              查看系统支持的策略：<br> <pre><code># cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_available_governors<br>performance powersave<br></code></pre><br>              查看生效的策略：<br> <pre><code># cat /sys/devices/system/cpu/cpu0/cpufreq/scaling_governor<br>powersave<br></code></pre><br>               设置：<br> <pre><code>echo 'performance' | sudo tee /sys/devices/system/cpu/cpu*/cpufreq/scaling_governor<br></code></pre><br>  <br>        1.2.2) 关闭透明大页                开启透明大页，操作系统的内存分配活动需要各种内存锁，直接影响程序的内存访问性能，对于ClickHouse而言，透明大页会严重干扰内存分配器的工作，从而导致性能显著下降。<br> <pre><code class="language-bash">echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag<br>echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled</p>
</blockquote>
<p>echo ‘echo never &gt; /sys/kernel/mm/transparent_hugepage/defrag’&gt;&gt; /etc/rc.local<br>echo ‘echo never &gt; /sys/kernel/mm/transparent_hugepage/enabled’&gt;&gt; /etc/rc.local<br></code></pre><br>        1.2.3)禁用swap文件<br>              避免将数据调度到swap上，影响性能。<br> <pre><code class="language-bash">dd if=/dev/zero of=/home/swap bs=1024 count=1048576<br>swapon /dev/dm-1</p>
<h1 id="swapon"><a href="#swapon" class="headerlink" title="swapon"></a>swapon</h1><p>NAME TYPE SIZE USED PRIO<br>/dev/dm-1 partition 16G 0B -3</p>
<p>swapoff   /dev/dm-1<br></code></pre><br>        1.2.4)内核分配策略<br>             overcommit_memory是一个内核对内存分配的一种策略。具体可见/proc/sys/vm/overcommit_memory下的值。             overcommit_memory取值有三种分别为0,1,2<br>             overcommit_memory=0，表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。             overcommit_memory=1，表示内核允许分配所有的物理内存，而不管当前的内存状态如何。             overcommit_memory=2，表示内核允许分配超过所有物理内存和交换空间总和的内存。<br>        1.2.5)彻底理解ClickHouse的配置文件<br>             ClickHouse支持多文件的配置管理，主配置文件为/etc/clickhouse-server/config.xml，其他的配置文件必须位于/etc/clickhouse-server/config.d目录中。<br>             所有配置文件必须为XML格式，每个文件具有相同的根元素，通常为.<br>            配置替换                  配置的元素可以指定incl属性，这个属性指定了一个”substitutions (替换)”。默认情况下，这个”替换”来自配置文件/etc/metrika.xml的元素，可以通过服务器的配置选项include_from修改”替换”文              件的路径。当指定了”替换”，将使用替换文件中相应元素的内容作为值。“替换”的值是引用”替换”文件的根节点（yarndex）下的子元素。如果incl中指定的”替换”不存在，则在服务器中记录日志，           如果不想记录日志，则为元素指定optional=”true”属性即可。            例如，”替换”文件/etc/metrika.xml中的部分内容：<br> <pre><code class="language-XML">&lt;yandex&gt;<br>...<br>&lt;zookeeper-servers&gt;<br>    &lt;host&gt;192.168.9.100&lt;/host&gt;<br>    &lt;port&gt;2181&lt;/port&gt;<br>&lt;/zookeeper-servers&gt;<br>...<br>&lt;/yandex&gt;<br></code></pre><br> ClickHouse的config.xml配置文件默认引用了这个文件的”替换”：zookeeper-servers，它是/etc/metrika.xml中根节点yandex下的子元素。如下： <br> <pre><code class="language-XML">&lt;zookeeper incl="zookeeper-servers" optional="true" /&gt;<br></code></pre><br> <strong>1.3)用户设置</strong><br>          在config.xml文件默认指定了一个单独的用户管理相关的配置文件，用于管理用户的权限、profiles、quotas等。这个配置文件由users_config元素指定文件路径，默认为users.xml。如果省略users_config元素，则直接在config.xml中指定用户管理相关的配置。<br> <pre><code class="language-XML">&lt;users_config&gt;users.xml&lt;/users_config&gt;<br></code></pre><br>      可以将用户配置拆分为多个配置文件，类似config.xml和config.d。用户配置的目录默认为users.d，目录名称的规则为：user_config设置的文件名去掉.xml后缀，并拼接上.d后缀。例如，如果user_config设置为abc.xml，则用户配置目录为：abc.d。 如下示例，为用户创建了一个单独的配置文件：<br> <pre><code class="language-XML">$ cat /etc/clickhouse-server/users.d/alice.xml<br>&lt;yandex&gt;<br>    &lt;users&gt;<br>      &lt;alice&gt;<br>          &lt;profile&gt;default&lt;/profile&gt;<br>          &lt;networks&gt;<br>              &lt;ip&gt;::/0&lt;/ip&gt;<br>          &lt;/networks&gt;<br>          &lt;password&gt;123&lt;/password&gt;<br>          &lt;quota&gt;default&lt;/quota&gt;<br>      &lt;/alice&gt;<br>    &lt;/users&gt;<br>&lt;/yandex&gt;<br></code></pre><br> <strong>1.4)重复设置项的处理</strong><br> **     **主配置文件的有些设置会被其他配置文件中的设置覆盖。      可以为这些配置文件的元素指定replace和remove属性。      如果replace和remove属性都不指定， ClickHouse将以递归的方式合并元素的内容，替换掉重复子元素的值。      如果指定了replace属性， 则使用当前的元素配置替换从其他配置文件引用的元素内容。      如果指定了remove属性， 则表示删除该元素。      例如下面的配置，它会替换networks元素（由incl指定）的整个配置，而保留当前定义的设置：<br> <pre><code class="language-XML">&lt;networks incl="networks" replace="replace"&gt;<br>    &lt;ip&gt;::/0&lt;/ip&gt;<br>&lt;/networks&gt;<br></code></pre><br>  <br> 1.5)预处理文件<br>      ClickHouse服务在启动时，会生成每类配置文件的预处理文件，这些文件包含了所有已完成的替换和重写，是ClickHouse最终应用的配置。<br>      ClickHouse会跟踪配置文件的更改，能动态加载变更的用户和集群设置。这意味着用户可以在线动态修改集群、用户等相关配置，而无需重新启动服务。<br>       预处理文件路径为/var/lib/clickhouse/preprocessed_configs，使用/etc/clickhouse-server/preprocessed软链接指向这个目录。<br> <pre><code class="language-bash"># ll /etc/clickhouse-server/preprocessed<br>lrwxrwxrwx 1 root root 41 3月   4 09:44 /etc/clickhouse-server/preprocessed -&gt; /var/lib/clickhouse//preprocessed_configs</p>
<h1 id="ls-etc-clickhouse-server-preprocessed"><a href="#ls-etc-clickhouse-server-preprocessed" class="headerlink" title="ls /etc/clickhouse-server/preprocessed/"></a>ls /etc/clickhouse-server/preprocessed/</h1><p>abc_dictionary.xml  config.xml  users.xml<br></code></pre> 
   </p>
<h3 id="2-ZooKeeper的关键优化点"><a href="#2-ZooKeeper的关键优化点" class="headerlink" title="2.ZooKeeper的关键优化点"></a>2.ZooKeeper的关键优化点</h3><blockquote>
 <ol>- **ZooKeeper节点数3个以上，奇数台。**- **与ClickHouse集群独立部署  **ZooKeeper对延迟非常敏感，ClickHouse可能会利用所有可用的系统资源。<li>**配置snapshot文件清理策略** <pre><code class="language-bash">autopurge.purgeInterval=1
</blockquote>
<p>autopurge.snapRetainCount=10<br></code></pre> autopurge.purgeInterval：开启清理事务日志和快照文件的功能，单位是小时。默认是0，表示不开启自动清理功能。 autopurge.snapRetainCount ： 指定了需要保留的文件数目。默认是保留3个。</li>- <strong>限制snapshot数量</strong> snapCount=3000000 每snapCount次事务日志输出后，触发一次快照(snapshot)。 ZooKeeper会生成一个snapshot文件和事务日志文件。 默认是100000。- <strong>log和data数据分磁盘存储</strong> dataDir：存储快照文件snapshot的目录。默认情况下，事务日志也会存储在这里。 dataLogDir：事务日志输出目录。尽量给事务日志的输出配置单独的磁盘或是挂载点，这将极大的提升ZK性能。- <strong>ZooKeeper的磁盘建议使用SSD</strong> 如果数据在TB级别以上，且复制表的数量比较多，超过100个，建议使用SSD磁盘。提升ZooKeeper的响应速度，避免ClickHouse副本间数据同步的延迟。<li>**调整JVM大小  *<em>ZooKeeper的JVM内存默认是根据操作系统本身内存大小的一个百分比预先分配的，所以这不是我们所需要的。 在./bin/zkEnv.sh文件中，有如下配置项： <pre><code class="language-bash">if [ -f "$ZOOCFGDIR/java.env" ]<br>then<br>    . "$ZOOCFGDIR/java.env"<br>fi<br></code></pre> 我们在./conf/java.env文件中配置JVM的内存，增加如下配置： <pre><code class="language-bash">export JAVA_HOME=/usr/local/java/jdk1.8.0_151<br>export JVMFLAGS="-Xms1024m -Xmx2048m $JVMFLAGS"<br></code></pre> 修改完成使用jmap -heap $pid来验证内存修改情况。 </li><li><strong>其它配置</strong> <strong>tickTime=2000</strong> ZK中的一个时间单元。ZK中所有时间都是以这个时间单元为基础，进行整数倍配置的。例如，session的最小超时时间是2</em>tickTime。 默认值2000，单位毫秒。 <strong>initLimit=10</strong> Follower在启动过程中，会从Leader同步所有最新数据，然后确定自己能够对外服务的起始状态。Leader允许F在 initLimit 时间内完成这个工作。通常情况下，我们不用太在意这个参数的设置。如果ZK集群的数据量确实很大了，F在启动的时候，从Leader上同步数据的时间也会相应变长，因此在这种情况下，有必要适当调大这个参数了。 initLimit=30000 <strong>syncLimit=5</strong> 在运行过程中，Leader负责与ZK集群中所有机器进行通信，例如通过一些心跳检测机制，来检测机器的存活状态。如果L发出心跳包在syncLimit之后，还没有从F那里收到响应，那么就认为这个F已经不在线了。注意：不要把这个参数设置得过大，否则可能会掩盖一些问题。 建议设置为10。 <strong>maxClientCnxns ：</strong> 单个客户端与单台服务器之间的连接数的限制，是ip级别的，默认是60，如果设置为0，那么表明不作任何限制。请注意这个限制的使用范围，仅仅是单台客户端机器与单台ZK服务器之间的连接数限制，不是针对指定客户端IP，也不是ZK集群的连接数限制，也不是单台ZK对所有客户端的连接数限制。 建议设置为2000 <strong>maxSessionTimeout=60000000</strong> Session超时时间限制，如果客户端设置的超时时间不在这个范围，那么会被强制设置为最大或最小时间。默认的Session超时时间是在2 * tickTime ~ 20 * tickTime 这个范围 <strong>preAllocSize=131072</strong> 预先开辟磁盘空间，用于后续写入事务日志。默认是64M，每个事务日志大小就是64M。如果ZK的快照频率较大的话，建议适当减小这个参数。单位kb。 <strong>配置：</strong> <pre><code class="language-bash">tickTime=2000</p>
<p>initLimit=30000<br>syncLimit=10</p>
<p>maxClientCnxns=2000<br>maxSessionTimeout=60000000</p>
<p>dataDir=/opt/zookeeper/&#123;<!-- -->&#123; cluster[‘name’] &#125;&#125;/data<br>dataLogDir=/opt/zookeeper/&#123;<!-- -->&#123; cluster[‘name’] &#125;&#125;/logs</p>
<p>autopurge.purgeInterval=1<br>autopurge.snapRetainCount=10</p>
<p>snapCount=3000000</p>
<p>preAllocSize=131072<br></code></pre>   </li>- <strong>建ClickHouse表配置元数据压缩</strong> 建表的时候设置use_minimalistic_part_header_in_zookeeper=1， 则ZooKeeper中会存储较少的数据，支持副本的表使用单个znode紧凑地存储数据片段的头信息。如果表包含很多列， 则此存储方法将大大减少Zookeeper中存储的数据量。</ol></p>
<h3 id="3-服务监控"><a href="#3-服务监控" class="headerlink" title="3.服务监控"></a>3.服务监控</h3><blockquote>
<p> ClickHouse内置了自我状态检测的功能， 可根据系统表、查询日志等监控服务器计算资源的不同度量以及查询处理的通用统计信息。<br> <ol><li> 系统表  ClickHouse提供了system.metrics、system.events和system.asynchronous_metrics表收集服务器的不同的运行度量。  1.1)system.metrics           这张表的数据实时更新， 用于查看正在运行的查询或当前副本的延迟信息等。           包含的列有：               •metric(String) ： 度量名称。               •value(Int64) ：指标值。               •description (String) ：度量描述。           <strong>示例:</strong> <pre><code class="language-sql">select * from system.metrics;</p>
</blockquote>
<p>┌─metric───────────────────────────────────────┬────value─┬─description─────────────────────────────────────────────┐<br>│ Query                                        │        1 │ Number of executing queries                             │<br>│ Merge                                        │        0 │ Number of executing background merges                   │<br>│ PartMutation                                 │        0 │ Number of mutations (ALTER DELETE/UPDATE)               │<br>│ GlobalThread                                 │       65 │ Number of threads in global thread pool.                │<br>│ GlobalThreadActive                           │       51 │ Number of threads in global thread pool running a task. │<br>└──────────────────────────────────────────────┴──────────┴─────────────────────────────────────────────────────────┘<br></code></pre>   1.2)<strong>system.events         用于收集系统中发生的事件数的信息。例如，可以查询自ClickHouse服务启动以来已处理的SELECT 查询数。         列信息如下：             •event (String) ： 事件名称。             •value (UInt64) ： 发生的事件数。             •description (String) ： 事件描述。         示例：</strong> <pre><code class="language-sql">clickhouse1 :) select * from system.events LIMIT 5;</p>
<p>SELECT *<br>FROM system.events<br>LIMIT 5</p>
<p>┌─event───────────────────────┬───value─┬─description────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┐<br>│ Query                       │       7 │ Number of queries to be interpreted and potentially executed. Does not include queries that failed to parse or were rejected due to AST size limits, quota limits or limits on the number of simultaneously running queries. May include internal queries initiated by ClickHouse itself. Does not count subqueries. │<br>│ SelectQuery                 │       7 │ Same as Query, but only for SELECT queries.                                                                                                                                                                                                                │<br>│ QueryTimeMicroseconds       │  140616 │ Total time of all queries.                                                                                                                                                                                                                                 │<br>│ SelectQueryTimeMicroseconds │  140616 │ Total time of SELECT queries.                                                                                                                                                                                                                              │<br>│ FileOpen                    │ 3645730 │ Number of files opened.                                                                                                                                                                                                                                    │<br>└─────────────────────────────┴─────────┴────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────┘</p>
<p>5 rows in set. Elapsed: 0.002 sec. </p>
<p>clickhouse1 :)<br></code></pre>   1.3)<strong>system.asynchronous_metrics        <strong>用于实时监控后台定期计算的指标。例如，正在使用的RAM大小        列信息如下：</strong>             •metric (String)</strong> ： 事件名称。**             •value (Float64)** ：指标值。        **例如：        ** <pre><code class="language-sql">clickhouse1 :) SELECT * FROM system.asynchronous_metrics;</p>
<p>SELECT *<br>FROM system.asynchronous_metrics</p>
<p>┌─metric───────────────────────────────────┬──────value─┐<br>│ CPUFrequencyMHz_0                        │   2591.504 │<br>│ jemalloc.arenas.all.pmuzzy               │          0 │<br>│ jemalloc.arenas.all.pdirty               │       1311 │<br>│ jemalloc.background_thread.run_intervals │          0 │<br>│ jemalloc.background_thread.num_runs      │          0 │<br>│ jemalloc.retained                        │  894418944 │<br>│ jemalloc.mapped                          │  136331264 │<br>│ jemalloc.metadata                        │   13635904 │<br>│ jemalloc.resident                        │  126943232 │<br>│ jemalloc.allocated                       │  102341072 │<br>│ jemalloc.epoch                           │       1298 │<br>│ NumberOfTables                           │         73 │<br>│ jemalloc.active                          │  109989888 │<br>│ NumberOfDatabases                        │          3 │<br>│ MaxPartCountForPartition                 │          7 │<br>│ jemalloc.background_thread.num_threads   │          0 │<br>│ ReplicasSumQueueSize                     │          0 │<br>│ ReplicasMaxMergesInQueue                 │          0 │<br>│ MemoryShared                             │ 4238995456 │<br>│ MemoryCode                               │  432128000 │<br>│ ReplicasMaxAbsoluteDelay                 │          0 │<br>│ ReplicasMaxQueueSize                     │          0 │<br>│ jemalloc.arenas.all.muzzy_purged         │          0 │<br>│ MemoryVirtual                            │ 6097547264 │<br>│ MarkCacheBytes                           │          0 │<br>│ Uptime                                   │      77887 │<br>│ jemalloc.arenas.all.dirty_purged         │  379880169 │<br>│ ReplicasMaxRelativeDelay                 │          0 │<br>│ MemoryResident                           │ 4455157760 │<br>│ ReplicasMaxInsertsInQueue                │          0 │<br>│ jemalloc.metadata_thp                    │          0 │<br>│ UncompressedCacheCells                   │          0 │<br>│ CompiledExpressionCacheCount             │          0 │<br>│ ReplicasSumMergesInQueue                 │          0 │<br>│ UncompressedCacheBytes                   │          0 │<br>│ ReplicasSumInsertsInQueue                │          0 │<br>│ MarkCacheFiles                           │          0 │<br>│ MemoryDataAndStack                       │ 1824006144 │<br>│ jemalloc.arenas.all.pactive              │      26853 │<br>└──────────────────────────────────────────┴────────────┘</p>
<p>39 rows in set. Elapsed: 0.049 sec. </p>
<p>clickhouse1 :)<br></code></pre>   </li><li> 查询日志<strong>query_log</strong> 记录所有查询语句的开始时间、耗时、用户、资源占用等信息，包括SELECT、SET、ALTER等语句。 在ClickHouse的主配置文件config.xml配置了query_log日志的表、分区、数据刷新间隔等信息。 <pre><code class="language-XML">&lt;!-- Query log. Used only for queries with setting log_queries = 1. --&gt;<br>    &lt;query_log&gt;<br>        &lt;!-- What table to insert data. If table is not exist, it will be created.<br>             When query log structure is changed after system update,<br>              then old table will be renamed and new table will be created automatically.<br>        --&gt;<br>        &lt;database&gt;system&lt;/database&gt;<br>        &lt;table&gt;query_log&lt;/table&gt;<br>        &lt;!--<br>            PARTITION BY expr <a target="_blank" rel="noopener" href="https://clickhouse.yandex/docs/en/table_engines/custom_partitioning_key/">https://clickhouse.yandex/docs/en/table_engines/custom_partitioning_key/</a><br>            Example:<br>                event_date<br>                toMonday(event_date)<br>                toYYYYMM(event_date)<br>                toStartOfHour(event_time)<br>        --&gt;<br>        &lt;partition_by&gt;toYYYYMM(event_date)&lt;/partition_by&gt;</p>
<pre><code>    &amp;lt;!-- Instead of partition_by, you can provide full engine expression (starting with ENGINE = ) with parameters,
         Example: &amp;lt;engine&amp;gt;ENGINE = MergeTree PARTITION BY toYYYYMM(event_date) ORDER BY (event_date, event_time) SETTINGS index_granularity = 1024&amp;lt;/engine&amp;gt;
      --&amp;gt;

    &amp;lt;!-- Interval of flushing data. --&amp;gt;
    &amp;lt;flush_interval_milliseconds&amp;gt;7500&amp;lt;/flush_interval_milliseconds&amp;gt;
&amp;lt;/query_log&amp;gt;
</code></pre>
<p></code></pre> ClickHouse默认不会收集查询的日志， 可通过设置log_queries = 1开启查询日志的功能,下面开启的命令要在服务器的黑窗口执行。 <pre><code class="language-bash">set log_queries = 1;<br>show tables;</p>
<p>SELECT * from <code>system</code>.query_log ql limit 1;<br></code></pre> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201217003637697.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RvdG8xMjk3NDg4NTA0,size_16,color_FFFFFF,t_70#pic_center"><strong>trace_log</strong> 存储query profilers收集的堆栈跟踪日志。 堆栈日志根据计时器类型分为REAL和CPU，分别表示实际时钟计时器和CPU时钟计时器。 计时器相关的两个设置： <pre><code class="language-sql">query_profiler_real_time_period_ns：设置query profilers的实际计时器的周期，单位为纳秒，默认为1000000000（1秒）<br>query_profiler_cpu_time_period_ns：设置query profilers的CPU计时器的周期，单位为纳秒，默认为1000000000（1秒）。<br></code></pre> 在ClickHouse的主配置文件config.xml配置了trace_log日志的表、分区、数据刷新间隔等信息。 <pre><code class="language-XML">&lt;trace_log&gt;<br>    &lt;database&gt;system&lt;/database&gt;<br>    &lt;table&gt;trace_log&lt;/table&gt;</p>
<pre><code>&amp;lt;partition_by&amp;gt;toYYYYMM(event_date)&amp;lt;/partition_by&amp;gt;
&amp;lt;flush_interval_milliseconds&amp;gt;7500&amp;lt;/flush_interval_milliseconds&amp;gt;
</code></pre>
<p>&lt;/trace_log&gt;<br></code></pre> <strong>示例：</strong> <pre><code class="language-sql">SELECT * FROM system.trace_log ORDER BY event_date ASC LIMIT 2;<br></code></pre> <strong>query_thread_log</strong> 查询线程日志， 记录查询执行的所有线程的信息。 在ClickHouse的主配置文件config.xml配置了query_thread_log日志的表、分区、数据刷新间隔等信息。 <pre><code class="language-XML">&lt;query_thread_log&gt;<br>    &lt;database&gt;system&lt;/database&gt;<br>    &lt;table&gt;query_thread_log&lt;/table&gt;<br>    &lt;partition_by&gt;toYYYYMM(event_date)&lt;/partition_by&gt;<br>    &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt;<br>&lt;/query_thread_log&gt;<br></code></pre> ClickHouse默认不会收集查询线程的日志， 可通过设置log_query_threads= 1开启查询日志的功能。 <pre><code class="language-sql">set log_query_threads = 1;</p>
<p>select * from system.query_thread_log limit 1;<br></code></pre> <img alt="在这里插入图片描述" src="https://img-blog.csdnimg.cn/20201217003929835.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3RvdG8xMjk3NDg4NTA0,size_16,color_FFFFFF,t_70#pic_center"><strong>part_log</strong> 用户监控MergeTree引擎表中针对数据片段所有操作（创建、删除、合并、下载等）的信息。 part_log默认是关闭， 在ClickHouse的主配置文件使用如下配置开启数据片段日志 <pre><code class="language-XML">&lt;part_log&gt;<br>    &lt;database&gt;system&lt;/database&gt;<br>    &lt;table&gt;part_log&lt;/table&gt;<br>    &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt;<br>&lt;/part_log&gt;<br></code></pre> <strong>text_log</strong> text_log用于记录服务器的常规日志，但是以结构化和高效的方式存储在表中。 text_log默认是关闭的， 在ClickHouse的主配置文件使用如下配置开启日志。 <pre><code class="language-XML">&lt;text_log&gt;<br>    &lt;database&gt;system&lt;/database&gt;<br>    &lt;table&gt;text_log&lt;/table&gt;<br>    &lt;flush_interval_milliseconds&gt;7500&lt;/flush_interval_milliseconds&gt;<br>&lt;/text_log&gt;<br></code></pre> <strong>metric_log</strong> 用于存储来自表system.metrics和system.events指标值的历史记录。collect_interval_milliseconds用于设置采集的时间间隔。metric_log默认是关闭的， 在ClickHouse的主配置文件使用如下配置开启日志。</li></ol></p>
<h3 id="4-用户管理"><a href="#4-用户管理" class="headerlink" title="4.用户管理"></a>4.用户管理</h3><blockquote>
 <ol><li> profile管理 profile是一组设置的集合，类似于角色的概念，每个用户都有一个profile。 可以有两种方式使用profile： 在会话中应用profile的所有设置，例如：SET profile = ‘web’ 。 在users.xml的users部分为某个用户指定profile。**profiles在users.xml配置文件的profiles标签下配置：** <pre><code class="language-XML">&lt;!-- Settings profiles --&gt;
&lt;profiles&gt;
    &lt;!-- Default settings --&gt;
    &lt;default&gt;
        &lt;!-- The maximum number of threads when running a single query. --&gt;
        &lt;max_threads&gt;8&lt;/max_threads&gt;
    &lt;/default&gt;
​
    &lt;!-- Settings for quries from the user interface --&gt;
    &lt;web&gt;
        &lt;max_rows_to_read&gt;1000000000&lt;/max_rows_to_read&gt;
        &lt;max_bytes_to_read&gt;100000000000&lt;/max_bytes_to_read&gt;
​
        &lt;max_rows_to_group_by&gt;1000000&lt;/max_rows_to_group_by&gt;
        &lt;group_by_overflow_mode&gt;any&lt;/group_by_overflow_mode&gt;
​
        &lt;max_rows_to_sort&gt;1000000&lt;/max_rows_to_sort&gt;
        &lt;max_bytes_to_sort&gt;1000000000&lt;/max_bytes_to_sort&gt;
​
        &lt;max_result_rows&gt;100000&lt;/max_result_rows&gt;
        &lt;max_result_bytes&gt;100000000&lt;/max_result_bytes&gt;
        &lt;result_overflow_mode&gt;break&lt;/result_overflow_mode&gt;
​
        &lt;max_execution_time&gt;600&lt;/max_execution_time&gt;
        &lt;min_execution_speed&gt;1000000&lt;/min_execution_speed&gt;
        &lt;timeout_before_checking_execution_speed&gt;15&lt;/timeout_before_checking_execution_speed&gt;
​
        &lt;max_columns_to_read&gt;25&lt;/max_columns_to_read&gt;
        &lt;max_temporary_columns&gt;100&lt;/max_temporary_columns&gt;
        &lt;max_temporary_non_const_columns&gt;50&lt;/max_temporary_non_const_columns&gt;
​
        &lt;max_subquery_depth&gt;2&lt;/max_subquery_depth&gt;
        &lt;max_pipeline_depth&gt;25&lt;/max_pipeline_depth&gt;
        &lt;max_ast_depth&gt;50&lt;/max_ast_depth&gt;
        &lt;max_ast_elements&gt;100&lt;/max_ast_elements&gt;
​
        &lt;readonly&gt;1&lt;/readonly&gt;
    &lt;/web&gt;
&lt;/profiles&gt;
</code></pre> 上面的配置指定了两个profile：default和web。默认的default不能省略，它包含默认的设置，在服务器启动时应用。web是常规的profile，可以使用SET语句或HTTP查询中的URL参数进行设置。 profile设置可相互继承。如果使用继承，在配置文件中列出的其他设置之前， 先指定一个或多个profile。如果一个设置在不同的profile都定义了，则使用最新的设置。 </li><li> Quotas设置 Quotas用于在一段时间内跟踪资源的使用情况或限制资源的使用。quotas在user.xml配置文件的quotas标签下配置，在users标签下分配给用户。 注意：quota用于限制一组查询，并且将所有远程服务器上的分布式查询处理的资源纳入限制范围，而不是限制单个查询。**quotas的配置示例1:** <pre><code class="language-XML">&lt;!-- Quotas --&gt;
&lt;quotas&gt;
    &lt;!-- Quota name. --&gt;
    &lt;default&gt;
        &lt;!-- Restrictions for a time period. You can set many intervals with different restrictions. --&gt;
        &lt;interval&gt;
            &lt;!-- Length of the interval. --&gt;
            &lt;duration&gt;3600&lt;/duration&gt;
​
            &lt;!-- Unlimited. Just collect data for the specified time interval. --&gt;
            &lt;queries&gt;0&lt;/queries&gt;
            &lt;errors&gt;0&lt;/errors&gt;
            &lt;result_rows&gt;0&lt;/result_rows&gt;
            &lt;read_rows&gt;0&lt;/read_rows&gt;
            &lt;execution_time&gt;0&lt;/execution_time&gt;
        &lt;/interval&gt;
    &lt;/default&gt;
&lt;/quotas&gt;
</code></pre> 默认情况下，quota只跟踪一个小时内的资源使用情况，并不会限制资源的使用。每个时间间隔内的资源消耗在每次请求之后输出到服务器日志。**Quota配置示例2：** <pre><code class="language-XML">&lt;statbox&gt;
    &lt;!-- Restrictions for a time period. You can set many intervals with different restrictions. --&gt;
    &lt;interval&gt;
        &lt;!-- Length of the interval. --&gt;
        &lt;duration&gt;3600&lt;/duration&gt;
​
        &lt;queries&gt;1000&lt;/queries&gt;
        &lt;errors&gt;100&lt;/errors&gt;
        &lt;result_rows&gt;1000000000&lt;/result_rows&gt;
        &lt;read_rows&gt;100000000000&lt;/read_rows&gt;
        &lt;execution_time&gt;900&lt;/execution_time&gt;
    &lt;/interval&gt;
</blockquote>
<pre><code>&amp;lt;interval&amp;gt;
    &amp;lt;duration&amp;gt;86400&amp;lt;/duration&amp;gt;
</code></pre>
<p>​<br>        &lt;queries&gt;10000&lt;/queries&gt;<br>        &lt;errors&gt;1000&lt;/errors&gt;<br>        &lt;result_rows&gt;5000000000&lt;/result_rows&gt;<br>        &lt;read_rows&gt;500000000000&lt;/read_rows&gt;<br>        &lt;execution_time&gt;7200&lt;/execution_time&gt;<br>    &lt;/interval&gt;<br>&lt;/statbox&gt;<br></code></pre> 上面配置了一个名称为statbox的quota，在每小时（3600秒）和每24小时（86400秒）设置了对资源的限制。间隔结束时，将清除所有收集的值，在接下来的下一个时间区间，quota将重新开始计算。 可用于quota限制的资源如下： queries ： 请求总数。 errors ： 抛出异常的总数。 result_rows : 作为结果给出的总行数。 read_rows : 在所有远程服务器上，从表中读取用于运行查询的源总行数。 execution_time : 查询执行的总耗时，单位为秒。如果在至少一个时间间隔内超过了限制，则会引发异常，异常信息包括限制的类型、时间间隔以及新时间间隔的开始时间。 对于分布式查询处理， 资源的累积量存储在请求服务器上，因此，如果用户转到另一个服务器，则这个服务器的quota将重新开始累积。重启服务器后，quota将重置。 </li><li> 约束管理 在users.xml配置文件的profiles部分定义对设置的约束，可以禁止用户使用SET语句更改某些设置。 约束的定义模板如下： <pre><code class="language-XML">&lt;profiles&gt;<br>  &lt;profile_name&gt;<br>    &lt;constraints&gt;<br>      &lt;setting_name_1&gt;<br>        &lt;min&gt;lower_boundary&lt;/min&gt;<br>      &lt;/setting_name_1&gt;<br>      &lt;setting_name_2&gt;<br>        &lt;max&gt;upper_boundary&lt;/max&gt;<br>      &lt;/setting_name_2&gt;<br>      &lt;setting_name_3&gt;<br>        &lt;min&gt;lower_boundary&lt;/min&gt;<br>        &lt;max&gt;upper_boundary&lt;/max&gt;<br>      &lt;/setting_name_3&gt;<br>      &lt;setting_name_4&gt;<br>        &lt;readonly/&gt;<br>      &lt;/setting_name_4&gt;<br>    &lt;/constraints&gt;<br>  &lt;/profile_name&gt;<br>&lt;/profiles&gt;<br></code></pre> 如果用户尝试违法冲突，则会引发异常。 支持三种类型的约束：max、min和readonly。 max和min约束使用数值指定约束的上限和下限，这两个约束可以组合使用。 readonly约束指定了用户无法更改相应的设置。 使用示例： <pre><code class="language-XML">&lt;profiles&gt;<br>  &lt;default&gt;<br>    &lt;max_memory_usage&gt;10000000000&lt;/max_memory_usage&gt;<br>    &lt;force_index_by_date&gt;0&lt;/force_index_by_date&gt;<br>    ...<br>    &lt;constraints&gt;<br>      &lt;max_memory_usage&gt;<br>        &lt;min&gt;5000000000&lt;/min&gt;<br>        &lt;max&gt;20000000000&lt;/max&gt;<br>      &lt;/max_memory_usage&gt;<br>      &lt;force_index_by_date&gt;<br>        &lt;readonly/&gt;<br>      &lt;/force_index_by_date&gt;<br>    &lt;/constraints&gt;<br>  &lt;/default&gt;<br>&lt;/profiles&gt;<br></code></pre> 上面的约束配置限制了max_memory_usage的范围在20000000000和5000000000之间，force_index_by_date设置为只读。 使用下面的语句修改配置将引发异常： <pre><code class="language-sql">SET max_memory_usage=20000000001;<br>SET max_memory_usage=4999999999;<br>SET force_index_by_date=1;<br></code></pre> <strong>异常信息如下：</strong> <pre><code class="language-sql">Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be greater than 20000000000.<br>Code: 452, e.displayText() = DB::Exception: Setting max_memory_usage should not be less than 5000000000.<br>Code: 452, e.displayText() = DB::Exception: Setting force_index_by_date should not be changed.<br></code></pre> Note：在default的profile中定义的约束为默认的约束，这些约束会限制所有的用户，除非用户在自己的profile中显式覆盖这些设置。 force_index_by_date，如果表的主键包含日期列，则查询条件必须包含该该列。如果表的主键不包含日期列，则无需包含日期列。 仅适用于MergeTree引擎。 </li><li> 查询权限 ClickHouse中的查询可分为如下几种类型：<strong>读取数据查询</strong>：SELECT、SHOW、DESCRIBE、EXISTS。<strong>写数据查询</strong>：INSERT、OPTIMIZE。<strong>更改设置查询</strong>：SET、USE。<strong>DDL查询</strong>：CREATE、ALTER、RENAME、ATTACH、DETACH、 DROP、TRUNCATE。 KILL QUERY 下面的设置可用户配置用户的查询权限：<strong>readonly <strong>：限制读取、写入和更改三类查询的权限。</strong>allow_ddl</strong>：限制DDL查询的权限。<strong>KILL QUERY</strong>不受任何设置的限制。 <pre><code>1. readonly<br>用于限制读取、写入和更改三类查询的权限，默认值为0。<br>readonly的取值如下：<br>0 ： 允许执行所有查询。<br>1 ： 仅允许读取数据的查询。<br>2 ： 允许读取数据和更改设置。<br>在设置readonly=1后，用户将无法在当前会话中更改readonly和allow_ddl的设置。<br>在HTTP请求中使用GET方法时将自动设置readonly=1。如果在http请求中修改数据，则必须使用POST方法。</p>
<p>设置readonly=1将禁止用户更改所有的设置。如果要禁止用户修改某些特定的设置，可以在users.xml配置文件中配置profiles的约束。</p>
<ol start="2">
<li>allow_ddl<br>用于配置是否允许DDL操作的权限，默认值为1。<br>可设置的值如下：<br>0 ： 不允许DDL查询。<br>1 ： 允许DDL查询。<br>如果当前会话的allow_ddl=0，则无法执行： SET allow_ddl=0。</code></pre>   </li><li> 用户管理配置 xml文件的users标签的结构如下： <pre><code class="language-XML">&lt;users&gt;<br> &lt;!-- If user name was not specified, 'default' user is used. --&gt;<br> &lt;user_name&gt;<pre><code> &amp;lt;password&amp;gt;&amp;lt;/password&amp;gt;
 &amp;lt;!-- Or --&amp;gt;
 &amp;lt;password_sha256_hex&amp;gt;&amp;lt;/password_sha256_hex&amp;gt;
</code></pre>
​<pre><code> &amp;lt;networks incl=&quot;networks&quot; replace=&quot;replace&quot;&amp;gt;
 &amp;lt;/networks&amp;gt;
</code></pre>
​<pre><code> &amp;lt;profile&amp;gt;profile_name&amp;lt;/profile&amp;gt;
</code></pre>
​<pre><code> &amp;lt;quota&amp;gt;default&amp;lt;/quota&amp;gt;
</code></pre>
​<pre><code> &amp;lt;databases&amp;gt;
     &amp;lt;database_name&amp;gt;
         &amp;lt;table_name&amp;gt;
             &amp;lt;filter&amp;gt;expression&amp;lt;/filter&amp;gt;
         &amp;lt;table_name&amp;gt;
     &amp;lt;/database_name&amp;gt;
 &amp;lt;/databases&amp;gt;
</code></pre>
​<pre><code>&amp;lt;allow_databases&amp;gt;
    &amp;lt;database&amp;gt;test&amp;lt;/database&amp;gt;
 &amp;lt;/allow_databases&amp;gt;
 &amp;lt;allow_dictionaries&amp;gt;
    &amp;lt;dictionary&amp;gt;test&amp;lt;/dictionary&amp;gt;
 &amp;lt;/allow_dictionaries&amp;gt;
</code></pre>
 &lt;/user_name&gt;<br> &lt;!– Other users settings –&gt;</li>
</ol>
<p>&lt;/users&gt;<br></code></pre> 在上面的配置中，定义了一个user_name的用户，该用户的所有其他配置都通过该标签的子标签配置。下面详细介绍每个具体子标签的配置信息。 1）**user_name/password       **密码可以使用明文、SHA256或SHA1指定。       明文形式的密码通过password标签指定。       <strong>例如:</strong> <pre><code class="language-XML">&lt;password&gt;qwerty&lt;/password&gt;<br></code></pre>    密码可以为空。    SHA256散列密码通过password_sha256_hex标签指定。    示例： <pre><code class="language-XML">&lt;password_sha256_hex&gt;65e84be33532fb784c48129675f9eff3a682b27168c0ea744b2cf58ee02337c5&lt;/password_sha256_hex&gt;<br></code></pre> **   <strong>可通过如下shell命令生成SHA256散列： <pre><code class="language-bash">PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha256sum | tr -d '-'<br></code></pre> **  <strong>命令的输出如下： <pre><code class="language-bash">pmpjaK2V<br>21c4185b8155e532ca5a1eb0d4ca74ecde83eddee4ca55af393007f7c29f7eb7<br></code></pre> 第一行是明文的密码，第二行对应的SHA256加密散列。 SHA1散列密码通过password_double_sha1_hex标签指定。 SHA1密码是为了兼容MySQL客户端。 使用示例： <pre><code class="language-XML">&lt;password_double_sha1_hex&gt;08b4a0f1de6ad37da17359e592c8d74788a83eb0&lt;/password_double_sha1_hex&gt;<br></code></pre> 可通过如下shell命令生成SHA1散列： <pre><code class="language-bash">PASSWORD=$(base64 &lt; /dev/urandom | head -c8); echo "$PASSWORD"; echo -n "$PASSWORD" | sha1sum | tr -d '-' | xxd -r -p | sha1sum | tr -d '-'<br></code></pre> 命令的输出如下： <pre><code class="language-bash">KCXM95It<br>de4bbcb9bbb35e3e497fcbebafc0f04b3dcef383<br></code></pre> 第一行是明文的密码，第二行对应的SHA1加密散列。</strong>2）user_name/networks      <strong>networks用于配置网络列表，只有网络列表范围内的用户可以连接到ClickHouse。     列表中的元素可以使用如下形式定义：     ： IP地址或带掩码的IP地址。     例如： <pre><code class="language-bash">213.180.204.3, 10.0.0.1/8, 10.0.0.1/255.255.255.0, 2a02:6b8::3, 2a02:6b8::3/64, 2a02:6b8::3/ffff:ffff:ffff:ffff::<br></code></pre> ： 主机名。 例如： <pre><code class="language-bash">example01.host.ru<br></code></pre> &lt;host_regexp&gt; ： 主机名的正则表达式。</strong>例如：</strong> <pre><code class="language-bash">^example\d\d-\d\d-\d.host.ru$<br></code></pre> 使用这种方式指定网络列表，强烈建议regexp以$结尾。 示例1：用户可以从任意网络访问，指定： <pre><code class="language-XML">&lt;ip&gt;::/0&lt;/ip&gt;<br></code></pre> <strong>示例2：仅限本地网络访问，指定：</strong>   <pre><code class="language-XML">&lt;ip&gt;::1&lt;/ip&gt;<br>&lt;ip&gt;127.0.0.1&lt;/ip&gt;<br></code></pre> **3）user_name/profile        <strong>profile用于给用户分配一个配置好的profile，这个profile是在users.xml配置文件的profiles标签下配置的。</strong>4）user_name/quota      <strong>用于给用户分配quota，这里quota是在users.xml配置文件的quotas标签下配置的。quota用于在一定时间内跟踪或限制资源的使用。</strong>5）user_name/databases      <strong>用于限制当前用户的SELECT查询返回的行，可实现基本的行级安全性。     <strong>示例：</strong>      以下的配置将限制用户user1只能查询table1表中id=1000的行，其他记录对用户不可见，就好像表中不存在其他行一样。 <pre><code class="language-XML">&lt;user1&gt;<br>    &lt;databases&gt;<br>        &lt;database_name&gt;<br>            &lt;table1&gt;<br>                &lt;filter&gt;id = 1000&lt;/filter&gt;<br>            &lt;/table1&gt;<br>        &lt;/database_name&gt;<br>    &lt;/databases&gt;<br>&lt;/user1&gt;<br></code></pre> 使用这种方式需要注意WHERE子句的查询谓词不会下推，即禁用了WHERE移动到PREWHERE的优化。</strong>6）user_name/allow_databases      **用于指定用户可以访问的数据库列表。默认情况下，用户可以访问所有数据库。      Note： 用户对system数据库的访问始终是有权限的，因为用户需要根据此数据库处理查询。      配置示例，配置用户可访问test1和test2数据库： <pre><code class="language-XML">&lt;allow_databases&gt;<br>  &lt;database&gt;test1&lt;/database&gt;<br>  &lt;database&gt;test2&lt;/database&gt;<br>&lt;/allow_databases&gt;<br></code></pre> **7）user_name/allow_dictionaries       **配置可以访问的字典列表。默认情况下，用户可访问所有字典。      示例， 配置用户可访问test1和test2字典： <pre><code class="language-XML">&lt;allow_dictionaries&gt;<br>  &lt;dictionary&gt;test1&lt;/dictionary&gt;<br>  &lt;dictionary&gt;test2&lt;/dictionary&gt;<br>&lt;/allow_dictionaries&gt;<br></code></pre>   </li></ol></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-Clickhouse 字典表使用场景"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/Clickhouse%20%E5%AD%97%E5%85%B8%E8%A1%A8%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/"
    >Clickhouse 字典表使用场景</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/Clickhouse%20%E5%AD%97%E5%85%B8%E8%A1%A8%E4%BD%BF%E7%94%A8%E5%9C%BA%E6%99%AF/" class="article-date">
  <time datetime="2021-07-18T14:18:56.779Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/Clickhouse/">Clickhouse</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: Clickhouse 字典表使用场景<br>categories:</p>
<ul>
<li>clickhouse<br>tags:</li>
<li>clickhouse</li>
</ul>
<p>—# 一.字典创建和查询</p>
<h2 id="1-创建表和数据："><a href="#1-创建表和数据：" class="headerlink" title="1.创建表和数据："></a>1.创建表和数据：</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">drop table t_region;</span><br><span class="line">create table t_region(region_id UInt64, parent_region UInt64, region_name String) ENGINE=TinyLog;</span><br><span class="line">insert into t_region values</span><br><span class="line">(1, 0, &#x27;jiangsu&#x27;),(2, 1, &#x27;suzhou&#x27;),(3, 2, &#x27;huqiu&#x27;),(4, 0, &#x27;anhui&#x27;),(5, 4, &#x27;hefei&#x27;);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">创建字典， 指定HIERARCHICAL字段：</span><br><span class="line">DROP DICTIONARY t_dict_region;</span><br><span class="line">CREATE DICTIONARY t_dict_region (</span><br><span class="line">    region_id UInt64,</span><br><span class="line">    parent_region UInt64  HIERARCHICAL,</span><br><span class="line">    region_name String </span><br><span class="line">)</span><br><span class="line">PRIMARY KEY region_id</span><br><span class="line">SOURCE(CLICKHOUSE(</span><br><span class="line">    host &#x27;localhost&#x27;</span><br><span class="line">    port 9001</span><br><span class="line">    user &#x27;default&#x27;</span><br><span class="line">    db &#x27;default&#x27;</span><br><span class="line">    password &#x27;&#x27;</span><br><span class="line">    table &#x27;t_region&#x27;</span><br><span class="line">))</span><br><span class="line">LAYOUT(HASHED())</span><br><span class="line">LIFETIME(30);</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="2-字典的查询"><a href="#2-字典的查询" class="headerlink" title="2.字典的查询"></a>2.字典的查询</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">SELECT dictGetString(&#x27;default.t_dict_region&#x27;, &#x27;region_name&#x27;, toUInt64(2)) AS regionName;</span><br><span class="line"></span><br><span class="line">┌─regionName─┐</span><br><span class="line">│ suzhou     │</span><br><span class="line">└────────────┘</span><br><span class="line">SELECT dictGetHierarchy(&#x27;default.t_dict_region&#x27;, toUInt64(3));</span><br><span class="line"></span><br><span class="line">┌─dictGetHierarchy(&#x27;default.t_dict_region&#x27;, toUInt64(3))─┐</span><br><span class="line">│ [3,2,1]                                                │</span><br><span class="line">└────────────────────────────────────────────────────────┘</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="3-字典数据源之mysql表"><a href="#3-字典数据源之mysql表" class="headerlink" title="3.字典数据源之mysql表"></a>3.字典数据源之mysql表</h2><li>在mysql数据库创建表并插入数据： <pre><code class="language-sql">drop table test.test_dc;
create table test.test_dc(
  id bigint,
  name varchar(100),
  age int,
  PRIMARY KEY (id)
);

<p>insert into test.test_dc values(1, ‘flink’, 4);<br>insert into test.test_dc values(2, ‘spark’, 6);<br>insert into test.test_dc values(3, ‘clickhouse’, 5);</p>
<p>查看MySQL数据：<br>mysql&gt; select * from test.test_dc;<br>+—-+————+——+<br>| id | name       | age  |<br>+—-+————+——+<br>|  1 | flink      |    4 |<br>|  2 | spark      |    6 |<br>|  3 | clickhouse |    5 |<br>+—-+————+——+</p>
<p></code></pre>   </li></p>
<h3 id="4-字典的数据源之文件数据源"><a href="#4-字典的数据源之文件数据源" class="headerlink" title="4.字典的数据源之文件数据源"></a>4.字典的数据源之文件数据源</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">TabSeparated格式</span><br><span class="line">文件示例：</span><br><span class="line"></span><br><span class="line">准备测试数据</span><br><span class="line">文件命名为person.tsv，存放在目录：/var/lib/clickhouse/user_files，字段之间使用制表符分隔，即格式为TabSeparated。数据如下：</span><br><span class="line">1	&#x27;id001&#x27;	&#x27;xiaohe&#x27;	23</span><br><span class="line">2	&#x27;id002&#x27;	&#x27;xiaoxue&#x27;	25</span><br><span class="line">3	&#x27;id003&#x27;	&#x27;xiaoyu&#x27;	26</span><br><span class="line">4	&#x27;id004&#x27;	&#x27;xiaoxi&#x27;	27</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">创建字典：</span><br><span class="line">DROP DICTIONARY t_dict_person_ddl;</span><br><span class="line">CREATE DICTIONARY t_dict_person_ddl</span><br><span class="line">(</span><br><span class="line">    id UInt64,</span><br><span class="line">    code String,</span><br><span class="line">    name String,</span><br><span class="line">    age UInt8</span><br><span class="line">)</span><br><span class="line">PRIMARY KEY id</span><br><span class="line">SOURCE(FILE(path &#x27;/var/lib/clickhouse/user_files/person.tsv&#x27; format &#x27;TabSeparated&#x27;))</span><br><span class="line">LAYOUT(FLAT())</span><br><span class="line">LIFETIME(30);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">SELECT dictGetString(&#x27;default.t_dict_person_ddl&#x27;, &#x27;name&#x27;, toUInt64(2)) AS regionName;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>当然,字典类型的数据也可以通过配置实现</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">&amp;lt;yandex&amp;gt; </span><br><span class="line">  &amp;lt;dictionary&amp;gt; </span><br><span class="line">    &amp;lt;name&amp;gt;t_dict_executable&amp;lt;/name&amp;gt;  </span><br><span class="line"></span><br><span class="line">    &amp;lt;structure&amp;gt; </span><br><span class="line">      &amp;lt;id&amp;gt; </span><br><span class="line">        &amp;lt;name&amp;gt;id&amp;lt;/name&amp;gt; </span><br><span class="line">      &amp;lt;/id&amp;gt;  </span><br><span class="line">      &amp;lt;attribute&amp;gt; </span><br><span class="line">        &amp;lt;name&amp;gt;code&amp;lt;/name&amp;gt;  </span><br><span class="line">        &amp;lt;type&amp;gt;String&amp;lt;/type&amp;gt;  </span><br><span class="line">        &amp;lt;null_value/&amp;gt; </span><br><span class="line">      &amp;lt;/attribute&amp;gt;  </span><br><span class="line">      &amp;lt;attribute&amp;gt; </span><br><span class="line">        &amp;lt;name&amp;gt;name&amp;lt;/name&amp;gt;  </span><br><span class="line">        &amp;lt;type&amp;gt;String&amp;lt;/type&amp;gt;  </span><br><span class="line">        &amp;lt;null_value/&amp;gt; </span><br><span class="line">      &amp;lt;/attribute&amp;gt;  </span><br><span class="line">      &amp;lt;attribute&amp;gt; </span><br><span class="line">        &amp;lt;name&amp;gt;age&amp;lt;/name&amp;gt;  </span><br><span class="line">        &amp;lt;type&amp;gt;UInt8&amp;lt;/type&amp;gt;  </span><br><span class="line">        &amp;lt;null_value/&amp;gt; </span><br><span class="line">      &amp;lt;/attribute&amp;gt; </span><br><span class="line">    &amp;lt;/structure&amp;gt;  </span><br><span class="line">    &amp;lt;source&amp;gt; </span><br><span class="line">      &amp;lt;executable&amp;gt;</span><br><span class="line">        &amp;lt;command&amp;gt;cat /var/lib/clickhouse/user_files/person.tsv&amp;lt;/command&amp;gt;</span><br><span class="line">        &amp;lt;format&amp;gt;TabSeparated&amp;lt;/format&amp;gt;</span><br><span class="line">      &amp;lt;/executable&amp;gt;</span><br><span class="line">    &amp;lt;/source&amp;gt;  </span><br><span class="line">    &amp;lt;layout&amp;gt; </span><br><span class="line">      &amp;lt;hashed/&amp;gt; </span><br><span class="line">    &amp;lt;/layout&amp;gt;  </span><br><span class="line">    &amp;lt;lifetime&amp;gt;10&amp;lt;/lifetime&amp;gt; </span><br><span class="line">  &amp;lt;/dictionary&amp;gt; </span><br><span class="line">&amp;lt;/yandex&amp;gt;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="二-字典的存储方式"><a href="#二-字典的存储方式" class="headerlink" title="二.字典的存储方式"></a>二.字典的存储方式</h1><p>以下测试均在default数据库。</p>
<h2 id="1-flat-hash-sparse-hash-cache"><a href="#1-flat-hash-sparse-hash-cache" class="headerlink" title="1. flat/hash/sparse_hash/cache"></a>1. flat/hash/sparse_hash/cache</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">DROP DICTIONARY t_dict_person_ddl;</span><br><span class="line">CREATE DICTIONARY t_dict_person_ddl</span><br><span class="line">(</span><br><span class="line">    id UInt64,</span><br><span class="line">    code String,</span><br><span class="line">    name String,</span><br><span class="line">    age UInt8</span><br><span class="line">)</span><br><span class="line">PRIMARY KEY id</span><br><span class="line">SOURCE(CLICKHOUSE(</span><br><span class="line">    host &#x27;localhost&#x27;</span><br><span class="line">    port 9001</span><br><span class="line">    user &#x27;default&#x27;</span><br><span class="line">    db &#x27;default&#x27;</span><br><span class="line">    password &#x27;&#x27;</span><br><span class="line">    table &#x27;t_dic_ch&#x27;</span><br><span class="line">    where &#x27;id&amp;gt;0&#x27;</span><br><span class="line">))</span><br><span class="line">LAYOUT(CACHE(SIZE_IN_CELLS 10000))</span><br><span class="line">LIFETIME(30);</span><br><span class="line"></span><br><span class="line">SELECT dictGetString(&#x27;default.t_dict_person_ddl&#x27;, &#x27;name&#x27;, toUInt64(2)) AS regionName;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>FLAT()、HASHED()、SPARSE_HASHED()、CACHE(SIZE_IN_CELLS 10000)</strong></p>
<h2 id="2-complex-key-hashed-complex-key-cache"><a href="#2-complex-key-hashed-complex-key-cache" class="headerlink" title="2. complex_key_hashed/complex_key_cache"></a>2. complex_key_hashed/complex_key_cache</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">DROP DICTIONARY t_dict_person_ddl;</span><br><span class="line">CREATE DICTIONARY t_dict_person_ddl</span><br><span class="line">(</span><br><span class="line">    id UInt64,</span><br><span class="line">    code String,</span><br><span class="line">    name String,</span><br><span class="line">    age UInt8</span><br><span class="line">)</span><br><span class="line">PRIMARY KEY id,code</span><br><span class="line">SOURCE(CLICKHOUSE(</span><br><span class="line">    host &#x27;localhost&#x27;</span><br><span class="line">    port 9001</span><br><span class="line">    user &#x27;default&#x27;</span><br><span class="line">    db &#x27;default&#x27;</span><br><span class="line">    password &#x27;&#x27;</span><br><span class="line">    table &#x27;t_dic_ch&#x27;</span><br><span class="line">    where &#x27;id&amp;gt;0&#x27;</span><br><span class="line">))</span><br><span class="line">LAYOUT(COMPLEX_KEY_HASHED())</span><br><span class="line">LIFETIME(30);</span><br><span class="line"></span><br><span class="line">SELECT dictGet(&#x27;default.t_dict_person_ddl&#x27;, &#x27;name&#x27;, tuple(toUInt64(2), &#x27;id002&#x27;)) AS name;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>COMPLEX_KEY_HASHED()、COMPLEX_KEY_CACHE(SIZE_IN_CELLS 10000)</strong></p>
<h2 id="3-range-hashed"><a href="#3-range-hashed" class="headerlink" title="3. range_hashed"></a>3. range_hashed</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line">drop table t_hash_range;</span><br><span class="line">create table t_hash_range(id UInt64, start Date, end Date, amount Float32) ENGINE=TinyLog;</span><br><span class="line">insert into t_hash_range values</span><br><span class="line">(123, &#x27;2020-03-20&#x27;, &#x27;2020-03-22&#x27;, 0.15)</span><br><span class="line">(123, &#x27;2020-03-23&#x27;, &#x27;2020-03-27&#x27;, 0.25)</span><br><span class="line">(456, &#x27;2020-04-20&#x27;, &#x27;2020-04-30&#x27;, 0.35)</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">查看数据：</span><br><span class="line">SELECT * FROM t_hash_range;</span><br><span class="line"></span><br><span class="line">┌──id─┬──────start─┬────────end─┬─amount─┐</span><br><span class="line">│ 123 │ 2020-03-20 │ 2020-03-22 │   0.15 │</span><br><span class="line">│ 123 │ 2020-03-23 │ 2020-03-27 │   0.25 │</span><br><span class="line">│ 456 │ 2020-04-20 │ 2020-04-30 │   0.35 │</span><br><span class="line">└─────┴────────────┴────────────┴────────┘</span><br><span class="line"></span><br><span class="line">创建字典：</span><br><span class="line">DROP DICTIONARY t_dict_hash_range;</span><br><span class="line">CREATE DICTIONARY t_dict_hash_range (</span><br><span class="line">    id UInt64,</span><br><span class="line">    start Date,</span><br><span class="line">    end Date,</span><br><span class="line">    amount Float32</span><br><span class="line">)</span><br><span class="line">PRIMARY KEY id</span><br><span class="line">SOURCE(CLICKHOUSE(</span><br><span class="line">    host &#x27;localhost&#x27;</span><br><span class="line">    port 9001</span><br><span class="line">    user &#x27;default&#x27;</span><br><span class="line">    db &#x27;default&#x27;</span><br><span class="line">    password &#x27;&#x27;</span><br><span class="line">    table &#x27;t_hash_range&#x27;</span><br><span class="line">))</span><br><span class="line">LAYOUT(RANGE_HASHED())</span><br><span class="line">RANGE(MIN start MAX end)</span><br><span class="line">LIFETIME(30);</span><br><span class="line"></span><br><span class="line">查看id为123的记录，在日期2020-03-21日的amount：</span><br><span class="line">select dictGetFloat32(&#x27;default.t_dict_hash_range&#x27;, &#x27;amount&#x27;, toUInt64(123), toDate(&#x27;2020-03-21&#x27;)) as amount;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">查看id为123的记录，在日期2020-03-25日的amount：</span><br><span class="line">select dictGetFloat32(&#x27;default.t_dict_hash_range&#x27;, &#x27;amount&#x27;, toUInt64(123), toDate(&#x27;2020-03-25&#x27;)) as amount;</span><br><span class="line"></span><br><span class="line">日期之外的记录：</span><br><span class="line">SELECT dictGetFloat32(&#x27;default.t_dict_hash_range&#x27;, &#x27;amount&#x27;, toUInt64(123), toDate(&#x27;2020-03-29&#x27;)) AS amount;</span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="4-ip-tire"><a href="#4-ip-tire" class="headerlink" title="4. ip_tire"></a>4. ip_tire</h2><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">创建表和测试数据：</span><br><span class="line">drop table t_ip_tire;</span><br><span class="line">create table t_ip_tire(prefix String, asn UInt32, ccode String) ENGINE=TinyLog;</span><br><span class="line">insert into t_ip_tire values</span><br><span class="line">(&#x27;202.79.32.0/20&#x27;, 17501, &#x27;NP&#x27;)</span><br><span class="line">(&#x27;2620:0:870::/48&#x27;, 3856, &#x27;US&#x27;)</span><br><span class="line">(&#x27;2a02:6b8:1::/48&#x27;, 13238, &#x27;RU&#x27;)</span><br><span class="line">(&#x27;2001:db8::/32&#x27;, 65536, &#x27;ZZ&#x27;)</span><br><span class="line">;</span><br><span class="line"></span><br><span class="line">查看数据：</span><br><span class="line">SELECT * FROM t_ip_tire;</span><br><span class="line"></span><br><span class="line">┌─prefix──────────┬───asn─┬─ccode─┐</span><br><span class="line">│ 202.79.32.0/20  │ 17501 │ NP    │</span><br><span class="line">│ 2620:0:870::/48 │  3856 │ US    │</span><br><span class="line">│ 2a02:6b8:1::/48 │ 13238 │ RU    │</span><br><span class="line">│ 2001:db8::/32   │ 65536 │ ZZ    │</span><br><span class="line">└─────────────────┴───────┴───────┘</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">创建字典：</span><br><span class="line">DROP DICTIONARY t_dict_ip_tire;</span><br><span class="line">CREATE DICTIONARY t_dict_ip_tire (</span><br><span class="line">    prefix String,</span><br><span class="line">    asn UInt32,</span><br><span class="line">    ccode String</span><br><span class="line">)</span><br><span class="line">PRIMARY KEY prefix</span><br><span class="line">SOURCE(CLICKHOUSE(</span><br><span class="line">    host &#x27;localhost&#x27;</span><br><span class="line">    port 9001</span><br><span class="line">    user &#x27;default&#x27;</span><br><span class="line">    db &#x27;default&#x27;</span><br><span class="line">    password &#x27;&#x27;</span><br><span class="line">    table &#x27;t_ip_tire&#x27;</span><br><span class="line">))</span><br><span class="line">LAYOUT(IP_TRIE())</span><br><span class="line">LIFETIME(30);</span><br><span class="line"></span><br><span class="line">检索数据：</span><br><span class="line">select</span><br><span class="line">dictGetUInt32(&#x27;default.t_dict_ip_tire&#x27;, &#x27;asn&#x27;, tuple(IPv4StringToNum(&#x27;202.79.32.22&#x27;))) as asn,</span><br><span class="line">dictGetString(&#x27;default.t_dict_ip_tire&#x27;, &#x27;ccode&#x27;, tuple(IPv4StringToNum(&#x27;202.79.32.22&#x27;))) as ccode</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p> </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Clickhouse/" rel="tag">Clickhouse</a></li></ul>

    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce知识点学习"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E7%9F%A5%E8%AF%86%E7%82%B9%E5%AD%A6%E4%B9%A0/"
    >MapReduce知识点学习</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E7%9F%A5%E8%AF%86%E7%82%B9%E5%AD%A6%E4%B9%A0/" class="article-date">
  <time datetime="2021-07-18T14:18:48.471Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce知识点学习<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—title: MapReduce知识点学习<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—1. 1. 1. 1. 1. 1. 1. 1. </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之数据清洗(ETL)案例,倒排索引案例,ReduceTask 工作机制,Hadoop 数据压缩简介"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97(ETL)%E6%A1%88%E4%BE%8B,%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E6%A1%88%E4%BE%8B,ReduceTask%20%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6,Hadoop%20%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E7%AE%80%E4%BB%8B/"
    >MapReduce之数据清洗(ETL)案例,倒排索引案例,ReduceTask 工作机制,Hadoop 数据压缩简介</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E4%B9%8B%E6%95%B0%E6%8D%AE%E6%B8%85%E6%B4%97(ETL)%E6%A1%88%E4%BE%8B,%E5%80%92%E6%8E%92%E7%B4%A2%E5%BC%95%E6%A1%88%E4%BE%8B,ReduceTask%20%E5%B7%A5%E4%BD%9C%E6%9C%BA%E5%88%B6,Hadoop%20%E6%95%B0%E6%8D%AE%E5%8E%8B%E7%BC%A9%E7%AE%80%E4%BB%8B/" class="article-date">
  <time datetime="2021-07-18T14:18:48.468Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce之数据清洗(ETL)案例,倒排索引案例,ReduceTask 工作机制,Hadoop 数据压缩简介<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：简介</p>
<p>             在运行核心业务 Mapreduce 程序之前，往往要先对数据进行清洗，清理掉不符合用户要求的数据。       清理的过程往往只需要运行 mapper 程序，不需要运行 reduce 程序。</p>
<h1 id="二：日志清洗案例之简单解析版"><a href="#二：日志清洗案例之简单解析版" class="headerlink" title="二：日志清洗案例之简单解析版"></a>二：日志清洗案例之简单解析版</h1><ol>
<li> 需求：去除日志中字段长度小于等于11的日志(每一行按照空格切割，切割后数组长度小于11的日志不要)1.  数据如下：   <img alt="" class="has" height="154" src="https://img-blog.csdnimg.cn/20190808213958907.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="455"><li>代码实现如下：  ⑴创建mapper类：           <pre class="has"><code class="language-java">package com.kgf.mapreduce.weblog;</li>
</ol>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>public class LogMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</p>
<pre><code>Text k = new Text();

@Override
protected void map(LongWritable key, Text value,Context context)
        throws IOException, InterruptedException &#123;
    
    //1:获取一行
    String line = value.toString();
    
    //2：解析一行数据
    boolean result = parseLog(line);
    
    if(!result) &#123;
        return;
    &#125;
    k.set(line);
    context.write(k, NullWritable.get());
&#125;

private boolean parseLog(String line) &#123;
    String[] fields = line.split(&quot; &quot;);
    if(fields.length&amp;gt;11) &#123;
        return true;
    &#125;
    return false;
&#125;
</code></pre>
<p>}<br></code></pre> ⑵创建Driver: <pre class="has"><code class="language-java">package com.kgf.mapreduce.weblog;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class LogDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1：获取Job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2：设置jar对象
    job.setJarByClass(LogDriver.class);
    
    //3:设置关联Mapper
    job.setMapperClass(LogMapper.class);
    
    //4：设置mapper输出类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(NullWritable.class);
    
    //5:设置reduce task为0
    job.setNumReduceTasks(0);
    
    //6：设置最终输出参数
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    
    //7：设置文件输入输出路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //8：提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>   </li>1.   效果：  ⑴日志清理前数据有如下数量：           <img alt="" class="has" height="120" src="https://img-blog.csdnimg.cn/20190808221608218.png" width="413">         ⑵清理后：            <img alt="" class="has" height="175" src="https://img-blog.csdnimg.cn/20190808221642199.png" width="474">   ⑶程序运行日志如下：           <img alt="" class="has" height="276" src="https://img-blog.csdnimg.cn/201908082218109.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="512">1. 计数器应用   ⑴简介：            Hadoop 为每个作业维护若干内置计数器，以描述多项指标。例如，某些计数器记录已      处理的字节数和记录数，使用户可监控已处理的输入数据量和已产生的输出数据量。   ⑵计数器方式如下：          <img alt="" class="has" height="274" src="https://img-blog.csdnimg.cn/20190808223125431.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="462">  ⑶案例，我们可以在上面的日志清洗案例中加上计数器组应用：代码如下：         <img alt="" class="has" height="349" src="https://img-blog.csdnimg.cn/2019080822352462.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="427">      效果如下（可以发现控制台出现了下面的日志内容，可以帮助我们定位问题）：         <img alt="" class="has" height="322" src="https://img-blog.csdnimg.cn/20190808223631620.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="431"></p>
<h1 id="三：倒排索引案例"><a href="#三：倒排索引案例" class="headerlink" title="三：倒排索引案例"></a>三：倒排索引案例</h1><ol>
<li>现在有三个文件a.txt,b.txt,c.txt,现在我们需要将文件里面的单词汇总 ⑴文件如下：         <img alt="" class="has" height="67" src="https://img-blog.csdnimg.cn/20190809224946281.png" width="106"><img alt="" class="has" height="66" src="https://img-blog.csdnimg.cn/20190809224958271.png" width="156"><img alt="" class="has" height="65" src="https://img-blog.csdnimg.cn/2019080922501325.png" width="208"> ⑵最后要求的结果（汇总每个单词所在的文件中个数）：       <img alt="" class="has" height="117" src="https://img-blog.csdnimg.cn/20190809225058159.png" width="514"><li> 代码实现如下：  ⑴建立bean对象         <pre class="has"><code class="language-java">package com.kgf.mapreduce.index;</li>
</ol>
<p>import java.io.DataInput;<br>import java.io.DataOutput;<br>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.Writable;</p>
<p>public class IndexVo implements Writable&#123;</p>
<pre><code>private String word;

private String file;

private int num;

public IndexVo(String word, String file, int num) &#123;
    super();
    this.word = word;
    this.file = file;
    this.num = num;
&#125;

public IndexVo() &#123;
    super();
&#125;

@Override
public void readFields(DataInput in) throws IOException &#123;
    this.word = in.readUTF();
    this.file = in.readUTF();
    this.num = in.readInt();
&#125;

@Override
public void write(DataOutput out) throws IOException &#123;
    out.writeUTF(word);
    out.writeUTF(file);
    out.writeInt(num);
&#125;

public String getWord() &#123;
    return word;
&#125;

public void setWord(String word) &#123;
    this.word = word;
&#125;

public String getFile() &#123;
    return file;
&#125;

public void setFile(String file) &#123;
    this.file = file;
&#125;

public int getNum() &#123;
    return num;
&#125;

public void setNum(int num) &#123;
    this.num = num;
&#125;

@Override
public String toString() &#123;
    return word + &quot;\t&quot; + file + &quot;\t&quot; + num;
&#125;
</code></pre>
<p>}<br></code></pre>  ⑵建立mapper对象         <pre class="has"><code class="language-java">package com.kgf.mapreduce.index;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>public class IndexMapper extends Mapper&lt;LongWritable, Text, Text, IndexVo&gt;&#123;</p>
<pre><code>private String fileName = null;

Text k = new Text();

IndexVo v = new IndexVo();

@Override
protected void setup(Context context)
        throws IOException, InterruptedException &#123;
    //1：获取文件切片信息
    FileSplit splitFile = (FileSplit) context.getInputSplit();
    fileName = splitFile.getPath().getName();
&#125;

@Override
protected void map(LongWritable key, Text value,Context context)
        throws IOException, InterruptedException &#123;
    
    //1:获取一行数据
    String line = value.toString();
    //2:切割数据
    String[] fields = line.split(&quot;\t&quot;);
    for (String field : fields) &#123;
        k.set(field);
        v.setWord(field);
        v.setFile(fileName);
        v.setNum(1);
        context.write(k, v);
    &#125;
&#125;
</code></pre>
<p>}<br></code></pre> ⑶建立reducer对象       <pre class="has"><code class="language-java">package com.kgf.mapreduce.index;</p>
<p>import java.io.IOException;<br>import java.util.HashMap;<br>import java.util.Map;<br>import java.util.Set;</p>
<p>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>public class IndexReducer extends Reducer&lt;Text, IndexVo, Text, NullWritable&gt; &#123;</p>
<pre><code>Text k = new Text();

@Override
protected void reduce(Text key, Iterable&amp;lt;IndexVo&amp;gt; values,Context context)
        throws IOException, InterruptedException &#123;
    
    Map&amp;lt;String,Integer&amp;gt; map = new HashMap&amp;lt;String,Integer&amp;gt;();
    
    for (IndexVo indexVo : values) &#123;
        String name = indexVo.getFile();
        if(map.containsKey(name)) &#123;
            map.put(name, map.get(name)+indexVo.getNum());
        &#125;else &#123;
            map.put(name, indexVo.getNum());
        &#125;
    &#125;
    
    String result = key.toString()+&quot;\t&quot;;
    for (String fileName : map.keySet()) &#123;
        result+=(fileName+&quot;\t&quot;+map.get(fileName))+&quot;\t&quot;;
    &#125;
    k.set(result);
    context.write(k, NullWritable.get());
&#125;
</code></pre>
<p>}<br></code></pre> ⑷建立Driver对象       <pre class="has"><code class="language-java">package com.kgf.mapreduce.index;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class IndexDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1：获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2：设置jar
    job.setJarByClass(IndexDriver.class);
    
    //3:关联mapper和reducer
    job.setMapperClass(IndexMapper.class);
    job.setReducerClass(IndexReducer.class);
    
    //4:设置mapper输出参数
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IndexVo.class);
    
    //5:设置最终输出参数
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    
    //6：设置数据输入输出路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //7：提交
    boolean rsult = job.waitForCompletion(true);
    System.exit(rsult?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>         </li></p>
<h1 id="四：-ReduceTask-工作机制"><a href="#四：-ReduceTask-工作机制" class="headerlink" title="四： ReduceTask 工作机制"></a>四： ReduceTask 工作机制</h1><ol>
<li>ReduceTask有如下特点：  <img alt="" class="has" height="381" src="https://img-blog.csdnimg.cn/20190810091911354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="529">1.  流程图如下：  <img alt="" class="has" height="461" src="https://img-blog.csdnimg.cn/20190810092256618.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="595">  ⑴Copy阶段：          ReduceTask 从各个 MapTask 上远程拷贝一片数据，并针对某一片数据，如果其大小超过一定阈值，     则写到磁盘上，否则直接放到内存中。  ⑵Merge 阶段：           在远程拷贝数据的同时，ReduceTask 启动了两个后台线程对内存和磁盘上的文件进行合并，     以防止内存使用过多或磁盘上文件过多。   ⑶Sort 阶段：           按照 MapReduce 语义，用户编写 reduce()函数输入数据是按 key 进行聚集的一组数据。     为了将 key 相同的数据聚在一起，Hadoop 采用了基于排序的策略。由于各个 MapTask 已经     实现对自己的处理结果进行了局部排序，因此，ReduceTask 只需对所有数据进行一次归并排序即可。   ⑷Reduce 阶段：            reduce()函数将计算结果写到 HDFS 上。 <h1 id="五：-Hadoop-数据压缩"><a href="#五：-Hadoop-数据压缩" class="headerlink" title="五： Hadoop 数据压缩"></a>五： Hadoop 数据压缩</h1></li>
<li>简介     压缩技术能够有效减少底层存储系统（HDFS）读写字节数。压缩提高了网络带宽和磁盘空间的效率。 在 Hadoop 下，尤其是数据规模很大和工作负载密集的情况下，使用数据压缩显得非常重要。在这种情况下， I/O 操作和网络数据传输要花大量的时间。还有，Shuffle与 Merge 过程同样也面临着巨大的 I/O 压力。     鉴于磁盘 I/O 和网络带宽是 Hadoop 的宝贵资源，数据压缩对于节省资源、最小化磁盘 I/O 和网络传输非常有帮助。不过，尽管压缩与解压操作的 CPU 开销不高，其性能的提升和 资源的节省并非没有代价。      如果磁盘 I/O 和网络带宽影响了 MapReduce 作业性能，在任意 MapReduce 阶段启用压 缩都可以改善端到端处理时间并减少 I/O 和网络流量。       压缩 Mapreduce 的一种优化策略：通过压缩编码对 Mapper 或者 Reducer 的输出进行 压缩，以减少磁盘 IO，提高 MR 程序运行速度（但相应增加了 cpu 运算负担）。       <img alt="" class="has" height="201" src="https://img-blog.csdnimg.cn/20190810095228924.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="562">1. MR支持的压缩编码     <img alt="" class="has" height="372" src="https://img-blog.csdnimg.cn/20190810095333486.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="422">    <img alt="" class="has" height="277" src="https://img-blog.csdnimg.cn/2019081009535852.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="583">1.  压缩性能的比较     <img alt="" class="has" height="297" src="https://img-blog.csdnimg.cn/20190810095427226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="629">1.  压缩方式选择       ⑴Gzip 压缩           <img alt="" class="has" height="291" src="https://img-blog.csdnimg.cn/20190810105343262.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="518">   ⑵ Bzip2 压缩           <img alt="" class="has" height="310" src="https://img-blog.csdnimg.cn/20190810105413209.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="581">    ⑶Lzo 压缩           <img alt="" class="has" height="210" src="https://img-blog.csdnimg.cn/20190810105518675.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="548">          <img alt="" class="has" height="76" src="https://img-blog.csdnimg.cn/20190810105535471.png" width="549">    ⑷Snappy 压缩           <img alt="" class="has" height="205" src="https://img-blog.csdnimg.cn/20190810105618682.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="559">1. 压缩位置选择   压缩可以在 MapReduce 作用的任意阶段启用。   <img alt="" class="has" height="651" src="https://img-blog.csdnimg.cn/20190810105756931.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="754">1. 压缩配置参数   <img alt="" class="has" height="352" src="https://img-blog.csdnimg.cn/20190810105926815.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="683">  <img alt="" class="has" height="369" src="https://img-blog.csdnimg.cn/20190810105950851.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="680">  <img alt="" class="has" height="376" src="https://img-blog.csdnimg.cn/20190810110017564.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="680">  <img alt="" class="has" height="336" src="https://img-blog.csdnimg.cn/20190810110042410.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="677"></li>
</ol>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之入门概述以及WordCount 案例"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E4%B9%8B%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8AWordCount%20%E6%A1%88%E4%BE%8B/"
    >MapReduce之入门概述以及WordCount 案例</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E4%B9%8B%E5%85%A5%E9%97%A8%E6%A6%82%E8%BF%B0%E4%BB%A5%E5%8F%8AWordCount%20%E6%A1%88%E4%BE%8B/" class="article-date">
  <time datetime="2021-07-18T14:18:48.466Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce之入门概述以及WordCount 案例<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：MapReduce定义</p>
<ol>
<li> 简介       Mapreduce 是一个分布式运算程序的编程框架，是用户开发“基于 hadoop 的数据分析应用”的核心框架。       Mapreduce 核心功能是将用户编写的业务逻辑代码和自带默认组件整合成一个完整的分布式运算程序， 并发运行在一个 hadoop 集群上。1. Mapreduce 优缺点       ⑴优点：                 a：MapReduce  易于编程                           它简单的实现一些接口，就可以完成一个分布式程序，这个分布式程序可以分布到大量                      廉价的 PC 机器上运行。也就是说你写一个分布式程序，跟写一个简单的串行程序是一模                      一样的。就是因为这个特点使得 MapReduce 编程变得非常流行。                 b：良好的 扩展性                            当你的计算资源不能得到满足的时候，你可以通过简单的增加机器来扩展它的计算能力。                 c： 高容错性                            MapReduce 设计的初衷就是使程序能够部署在廉价的 PC 机器上，这就要求它具有很高的容错性。                       比如其中一台机器挂了，它可以把上面的计算任务转移到另外一个节点上运行，不至于这个任务运行                       失败，而且这个过程不需要人工参与，而完全是由Hadoop 内部完成的。                 d：适合 PB  级以上海量数据的 离线处理                            这里加红字体离线处理，说明它适合离线处理而不适合在线处理。比如像毫秒级别的返回一个结果，                       MapReduce 很难做到。       ⑵缺点：                 a：MapReduce 不 擅长做实时计算、流式计算、DAG（有向图 ） 计算                 b：实时计算：                            MapReduce 无法像 Mysql 一样，在毫秒或者秒级内返回结果。                 c：流式计算：                             流式计算的输入数据是动态的，而 MapReduce 的输入数据集是静态的，不能动态变化。                       这是因为 MapReduce 自身的设计特点决定了数据源必须是静态的。                 d：DAG （有向图）计算：                              多个应用程序存在依赖关系，后一个应用程序的输入为前一个的输出。在这种情况下，                       MapReduce 并不是不能做，而是使用后，每个 MapReduce 作业的输出结果都会写入到磁盘，                       会造成大量的磁盘 IO，导致性能非常的低下。1. MapReduce的核心思想      <img alt="" class="has" height="478" src="https://img-blog.csdnimg.cn/20190727112452226.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="591">      <img alt="" class="has" height="278" src="https://img-blog.csdnimg.cn/20190727112523638.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="633">          <h1 id="二：MapReduce进程"><a href="#二：MapReduce进程" class="headerlink" title="二：MapReduce进程"></a>二：MapReduce进程</h1></li>
<li> 简介   <img alt="" class="has" height="251" src="https://img-blog.csdnimg.cn/20190727112710794.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="706">         <h1 id="三：MapReduce-编程规范-（八股文）"><a href="#三：MapReduce-编程规范-（八股文）" class="headerlink" title="三：MapReduce  编程规范 （八股文）"></a>三：MapReduce  编程规范 （八股文）</h1></li>
<li>简介       用户编写的程序分成三个部分：Mapper，Reducer，Driver(提交运行 mr 程序的客户端)。1. Mapper阶段       <img alt="" class="has" height="228" src="https://img-blog.csdnimg.cn/20190727113717265.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="587">1. Reducer阶段       <img alt="" class="has" height="204" src="https://img-blog.csdnimg.cn/20190727114144791.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="658">1. Driver 阶段      整个程序需要一个 Drvier 来进行提交，提交的是一个描述了各种必要信息的 job 对象            <h1 id="四：WordCount-案例"><a href="#四：WordCount-案例" class="headerlink" title="四：WordCount 案例"></a>四：WordCount 案例</h1></li>
<li>需求：         在一堆给定的文本文件中统计输出每一个单词出现的总次数。1. 数据准备，一个文件hello.txt,内容如下   <img alt="" class="has" height="287" src="https://img-blog.csdnimg.cn/20190727114806278.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="354">1. 案例分析   <img alt="" class="has" height="361" src="https://img-blog.csdnimg.cn/20190727115633681.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="539"><li>代码实现     ⑴在eclipse中创建工程               <img alt="" class="has" height="154" src="https://img-blog.csdnimg.cn/2019072712170054.png" width="347">               jar包和之前的一样，从hadoop安装包中拷贝出来。环境变量之前已经配置好了。              <img alt="" class="has" height="212" src="https://img-blog.csdnimg.cn/20190727121812316.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="437">     ⑵创建自定义的Mapper类去对数据进行分类，注意：我们这里会将所有数据分类完成后才会进入到下一阶段           <pre class="has"><code class="language-java">package com.kgf.mapreduce;</li>
</ol>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.IntWritable;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>/***</p>
<ul>
<li></li>
<li><p>继承的Mapper参数如下：</p>
</li>
<li><p>第一个参数key：LongWritable表示输入的key的行号</p>
</li>
<li><p>第二个参数value：Text表示一行内容</p>
</li>
<li><p>第三个参数key： Text表示单词</p>
</li>
<li><p>第四个参数value:IntWritable表示计算后的单词的个数</p>
</li>
<li><p>@author kgf</p>
</li>
<li></li>
<li><p>/<br>public class WordCountMapper extends Mapper&lt;LongWritable,Text,Text,IntWritable&gt;&#123;</p>
<p>  Text k = new Text();<br>  IntWritable v = new IntWritable(1);</p>
<p>  /**</p>
<ul>
<li>使用map方法去处理数据，数据是一行一行进入到这个方法处理的</li>
<li>key：表示行号</li>
<li>value：表示一行数据内容</li>
<li>/<br>@Override<br>protected void map(LongWritable key, Text value, Context context)<pre><code>  throws IOException, InterruptedException &#123;
</code></pre>
  //首先我们将一行内容转换成String<br>  String line = value.toString();<br>  //数据的单词之间是以空格切割的<br>  String[] words = line.split(“ “);<br>  //将数据循环写出到下一阶段<br>  for (String word : words) {<pre><code>  k.set(word);
  context.write(k, v);
</code></pre>
  }<br>}<br>}<br></code></pre> ⑶创建自定义的Reducer类对分类的数据进行汇总      <pre class="has"><code class="language-java">package com.kgf.mapreduce;</li>
</ul>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.IntWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;</p>
<p>/**</p>
<ul>
<li><p>注意：这里继承Reducer的前两个入参就是Mappper的出参数</p>
</li>
<li><p>@author kgf</p>
</li>
<li></li>
<li><p>/<br>public class WordCountReducer extends Reducer&lt;Text, IntWritable, Text, IntWritable&gt;&#123;</p>
<p>  /**</p>
<ul>
<li>这个方法主要是对map分类之后的数据进行聚合的</li>
<li>/<br>@Override<br>protected void reduce(Text key, Iterable&lt;IntWritable&gt; values,<pre><code>  Context context) throws IOException, InterruptedException &#123;
</code></pre>
  //统计单词个数<br>  int sum = 0;<br>  for (IntWritable count : values) {<pre><code>  sum+=count.get();
</code></pre>
  }<br>  //输出单词总个数<br>  context.write(key, new IntWritable(sum));<br>}</li>
</ul>
</li>
</ul>
<p>}<br></code></pre> ⑷创建Driver提交任务      <pre class="has"><code class="language-java">package com.kgf.mapreduce;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.IntWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class WordCountDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    
    //1:首先获取job信息
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:获取jar包位置,指定入口类，hadoop会自己找到
    job.setJarByClass(WordCountDriver.class);
    
    //3：关联自定义的mapper和reducer
    job.setMapperClass(WordCountMapper.class);
    job.setReducerClass(WordCountReducer.class);
    
    //4:设置map输出类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(IntWritable.class);
    
    //5:设置reducer输出类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);
    
    //6:设置数据输入和输出文件路径,这里我们通过main方法获取参数路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //7:提交代码
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>   </li>1.   在eclispe上将程序打成jar包   <img alt="" class="has" height="111" src="https://img-blog.csdnimg.cn/2019072715265789.png" width="150">1.  对jar包进行测试  ⑴启动集群  ⑵将jar包以及准备的hello.txt数据文本上传到/opt/module/hadoop-2.7.2目录下，并且设置有权限        <img alt="" class="has" height="336" src="https://img-blog.csdnimg.cn/20190727154242481.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="404">           ⑶将准备的hello.txt文件上传的hdfs指定目录下         <img alt="" class="has" height="79" src="https://img-blog.csdnimg.cn/20190727154354678.png" width="652">         <img alt="" class="has" height="179" src="https://img-blog.csdnimg.cn/20190727154506556.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="548">   ⑷使用我们的jar去测试（当前路径：/opt/module/hadoop-2.7.2）：         命令:hadoop jar +jar名称 +Driver入口的全路径 +输入路径 +输出路径         <img alt="" class="has" height="85" src="https://img-blog.csdnimg.cn/20190727155045147.png" width="666">        执行成功生成的文件：        <img alt="" class="has" height="225" src="https://img-blog.csdnimg.cn/20190727155247312.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="578">    ⑸查看文件内容           <img alt="" class="has" height="158" src="https://img-blog.csdnimg.cn/20190727155447856.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="488">1. 本地模式运行     ⑴上面我们是将jar包放到集群上运行，这里我们们需要在本地直接运行，因为本地我们有hadoop的jar包     ⑵在eclise上配置环境变量          <img alt="" class="has" height="189" src="https://img-blog.csdnimg.cn/20190727160144166.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="405">          注意：输出路径不能提前建好。         <img alt="" class="has" height="168" src="https://img-blog.csdnimg.cn/20190727160219740.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="366">     ⑶执行效果：         <img alt="" class="has" height="195" src="https://img-blog.csdnimg.cn/20190727160422404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="365">         <img alt="" class="has" height="229" src="https://img-blog.csdnimg.cn/20190727160525354.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="405">        </p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之Shuffle机制以及小文件处理案例（自定义 InputFormat），OutputFormat 案例"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E4%B9%8BShuffle%E6%9C%BA%E5%88%B6%E4%BB%A5%E5%8F%8A%E5%B0%8F%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B%EF%BC%88%E8%87%AA%E5%AE%9A%E4%B9%89%20InputFormat%EF%BC%89%EF%BC%8COutputFormat%20%E6%A1%88%E4%BE%8B/"
    >MapReduce之Shuffle机制以及小文件处理案例（自定义 InputFormat），OutputFormat 案例</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E4%B9%8BShuffle%E6%9C%BA%E5%88%B6%E4%BB%A5%E5%8F%8A%E5%B0%8F%E6%96%87%E4%BB%B6%E5%A4%84%E7%90%86%E6%A1%88%E4%BE%8B%EF%BC%88%E8%87%AA%E5%AE%9A%E4%B9%89%20InputFormat%EF%BC%89%EF%BC%8COutputFormat%20%E6%A1%88%E4%BE%8B/" class="article-date">
  <time datetime="2021-07-18T14:18:48.463Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce之Shuffle机制以及小文件处理案例（自定义 InputFormat），OutputFormat 案例<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：Shuffle机制</p>
<ol>
<li>简介        Mapreduce 确保每个 reducer 的输入都是按键排序的。系统执行排序的过程（即将 map 输出作为输入传给 reducer）称为 shuffle。   <img alt="" class="has" height="298" src="https://img-blog.csdnimg.cn/20190801211802997.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="511">1.  详细流程如下：  <img alt="" class="has" height="350" src="https://img-blog.csdnimg.cn/20190801213915875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="415">          <h1 id="二：小文件处理案例（自定义-InputFormat）"><a href="#二：小文件处理案例（自定义-InputFormat）" class="headerlink" title="二：小文件处理案例（自定义 InputFormat）"></a>二：小文件处理案例（自定义 InputFormat）</h1></li>
<li>需求：      无论 hdfs 还是 mapreduce，对于小文件都有损效率，实践中，又难免面临处理大量小文 件的场景，此时，就需要有相应解决方案。将多个小文件合并成一个文件 SequenceFile， SequenceFile 里面存储着多个文件，存储的形式为文件路径+名称为 key，文件内容为 value。1. 分析，小文件的优化无非以下几种方式：   a：在数据采集的时候，就将小文件或小批数据合成大文件再上传 HDFS   b：在业务处理之前，在 HDFS 上使用 mapreduce 程序对小文件进行合并   c：在 mapreduce 处理时，可采用 CombineTextInputFormat 提高效率 <img alt="" class="has" height="295" src="https://img-blog.csdnimg.cn/2019080122024881.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="477">1.  数据准备（准备如下三个小文件）：  <img alt="" class="has" height="205" src="https://img-blog.csdnimg.cn/20190804081453104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="392"><li>具体实现如下：   ⑴自定义 InputFromat             <pre class="has"><code class="language-java">package com.kgf.mapreduce.inputformat;</li>
</ol>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.BytesWritable;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.mapreduce.InputSplit;<br>import org.apache.hadoop.mapreduce.JobContext;<br>import org.apache.hadoop.mapreduce.RecordReader;<br>import org.apache.hadoop.mapreduce.TaskAttemptContext;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;</p>
<p>/**</p>
<ul>
<li>自定义WholeFileInputformat,主要是在自定义的mapper读取数据之前对数据进行处理，然后传给mapper：</li>
<li>这里我们的key是NullWritable，因为我们每次读取的是一个小文件，不需要关注行号等等。</li>
<li>value是BytesWritable，字节数组，代表的是我们每次读取的一个小文件，然后将这个文件内容传递给mapper</li>
<li>@author KGF</li>
<li></li>
<li>/<br>public class WholeFileInputformat extends FileInputFormat&lt;NullWritable,BytesWritable&gt;&#123;<br>  /***<ul>
<li>设置每个小文件不可分片,保证一个小文件生成一个key-value键值对</li>
<li>/<br>@Override<br>protected boolean isSplitable(JobContext context, Path filename) &#123;<br>  return false;<br>&#125;<br>/**</li>
<li>自定义需重写父类RecordReader方法，自定义读取文件的方式</li>
<li>split：表示的是我们每次读取一个小文件时的切片信息</li>
<li>context：代表上下文信息</li>
<li>/<br>@Override<br>public RecordReader&lt;NullWritable, BytesWritable&gt; createRecordReader(InputSplit split,<pre><code>  TaskAttemptContext context)
  throws IOException, InterruptedException &#123;
</code></pre>
  //创建自定义的RecordReader对象<br>  WholeFileRecordReader record = new WholeFileRecordReader();<br>  //初始化record中的方法<br>  record.initialize(split, context);<br>  return record;<br>}</li>
</ul>
</li>
</ul>
<p>}<br></code></pre> ⑵自定义WholeFileRecordReader <pre class="has"><code class="language-java">package com.kgf.mapreduce.inputformat;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.FSDataInputStream;<br>import org.apache.hadoop.fs.FileSystem;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.BytesWritable;<br>import org.apache.hadoop.io.IOUtils;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.mapreduce.InputSplit;<br>import org.apache.hadoop.mapreduce.RecordReader;<br>import org.apache.hadoop.mapreduce.TaskAttemptContext;<br>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>/***</p>
<ul>
<li><p>该类主要用来读取每个小文件内容</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class WholeFileRecordReader extends RecordReader&lt;NullWritable, BytesWritable&gt;&#123;<br>  //创建返回结果对象<br>  BytesWritable v = new BytesWritable();<br>  //当前读取进度，默认没有读<br>  boolean isProgress = false;<br>  FileSplit fileSplit;<br>  Configuration conf;<br>  /***</p>
<ul>
<li>初始化方法：</li>
<li>split:表示传递过来的文件切片</li>
<li>/<br>@Override<br>public void initialize(InputSplit split, TaskAttemptContext context) throws IOException, InterruptedException &#123;<br>  //1：首先我们需要将切片转换为FileSplit<br>  this.fileSplit = (FileSplit) split;<br>  //2:获取上下文配置信息<br>  conf = context.getConfiguration();<br>&#125;</li>
</ul>
<p>  /***</p>
<ul>
<li>读取一个一个文件</li>
<li>/<br>@Override<br>public boolean nextKeyValue() throws IOException, InterruptedException &#123;<br>  //开始读取文件，我们有三个小文件，所以它要读取3次<br>  FileSystem fs = null;<br>  FSDataInputStream fis = null;<br>  if(!isProgress) &#123;//判断状态，程序没有读取文件时，我们就可以开始读了<pre><code>  try &#123;
      //定义一个缓冲区对象，大小就是一个小文件的大小
      byte[] buf = new byte[(int) fileSplit.getLength()];
      //获取文件路径
      Path path = this.fileSplit.getPath();
      //获取文件系统对象
      fs = path.getFileSystem(conf);
      //打开文件输入流
      fis = fs.open(path);
      //流的拷贝,将读取的文件拷贝到缓冲区中
      IOUtils.readFully(fis, buf, 0, buf.length);
      //最后我们将缓冲区的数据拷贝到最终输出的对象中
      v.set(buf, 0, buf.length);
      //是否继续读文件，防止重复读,当下一个小文件进来时，这里的变量就会重新初始化为false
      isProgress = true;
      return true;
  &#125; catch (Exception e) &#123;
      e.printStackTrace();
  &#125;finally &#123;
      IOUtils.closeStream(fs);
      IOUtils.closeStream(fis);
  &#125;
</code></pre>
  }<br>  return false;<br>}<br>/**</li>
<li>获取key,我们这里没有key，所以直接是NullWritable</li>
<li>/<br>@Override<br>public NullWritable getCurrentKey() throws IOException, InterruptedException {<br>  return NullWritable.get();<br>}<br>/***</li>
<li>这个是获取当前的value值方法</li>
<li>/<br>@Override<br>public BytesWritable getCurrentValue() throws IOException, InterruptedException {<br>  return v;<br>}<br>/**</li>
<li>获取当前进度的方法</li>
<li>/<br>@Override<br>public float getProgress() throws IOException, InterruptedException {<br>  //正在读返回1，否则返回0，通过这个方法别人可以知道我们是否正在读取文件<br>  return isProgress?1:0;<br>}</li>
</ul>
<p>  @Override<br>  public void close() throws IOException {</p>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre> ⑶创建SequenseFileMapper.java类 <pre class="has"><code class="language-java">package com.kgf.mapreduce.inputformat;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.BytesWritable;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>/***</p>
<ul>
<li>创建自定义的mapper，前两个参数就是WholeFileInputformat处理数据后的输出参数，</li>
<li>后面两个参数，一个是key-&gt;是文件路径加上名称拼接而成。一个是value，就是小文件内容</li>
<li>@author KGF</li>
<li></li>
<li>/<br>public class SequenseFileMapper extends Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;&#123;  Text k = new Text();  /***<ul>
<li>该方法是主要用来获取文件路径和名称,程序运行会先进入这个setup方法，再进入我们自定义的WholeFileInputformat类中的方法，</li>
<li>然后再回来进入map方法</li>
<li>/<br>@Override<br>protected void setup(Mapper&lt;NullWritable, BytesWritable, Text, BytesWritable&gt;.Context context)<pre><code>  throws IOException, InterruptedException &#123;
</code></pre>
  //1:获取上下文切片信息<br>  FileSplit split = (FileSplit) context.getInputSplit();<br>  Path path = split.getPath();<br>  k.set(path.toString());<br>}<br>/***</li>
<li>map方法是对数据进行处理，并且写出到下一阶段，这里是每次写一个小文件</li>
<li>/<br>@Override<br>protected void map(NullWritable key, BytesWritable value,<pre><code>  Mapper&amp;lt;NullWritable, BytesWritable, Text, BytesWritable&amp;gt;.Context context)
  throws IOException, InterruptedException &#123;
</code></pre>
  context.write(k, value);<br>}<br>}<br></code></pre> ⑷自定义SequenseFileReducer      <img alt="" class="has" height="289" src="https://img-blog.csdnimg.cn/20190804100533568.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="406"> ⑸创建SequenseFileDriver <pre class="has"><code class="language-java">package com.kgf.mapreduce.inputformat;</li>
</ul>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.BytesWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.SequenceFileOutputFormat;</p>
<p>public class SequenseFileDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1:获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:设置jar路径
    job.setJarByClass(SequenseFileDriver.class);
    
    //3：关联自定义的mapper和reducer
    job.setMapperClass(SequenseFileMapper.class);
    job.setReducerClass(SequenseFileReducer.class);
    
    //4:设置我们自定义的WholeFileInputformat对象,输出为SequenceFileOutputFormat
    job.setInputFormatClass(WholeFileInputformat.class);
    job.setOutputFormatClass(SequenceFileOutputFormat.class);
    
    //5：设置mapper的输出类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(BytesWritable.class);
    
    //6:设置reducer的输出类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(BytesWritable.class);
    
    //7：设置文件的输入输出路径
    FileInputFormat.setInputPaths(job,new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //8：提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre> ⑹效果：       <img alt="" class="has" height="211" src="https://img-blog.csdnimg.cn/20190804102313326.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="454">       <img alt="" class="has" height="154" src="https://img-blog.csdnimg.cn/20190804102445966.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="543">         </li></p>
<h1 id="三：OutputFormat-接口实现"><a href="#三：OutputFormat-接口实现" class="headerlink" title="三：OutputFormat 接口实现"></a>三：OutputFormat 接口实现</h1><ol>
<li>简介        OutputFormat 是 MapReduce 输出的基类，所有实现 MapReduce 输出都实现了  OutputFormat 接口。下面我们介绍几种常见的 OutputFormat 实现类。 <img alt="" class="has" height="265" src="https://img-blog.csdnimg.cn/20190804113214875.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="655">1. 自定义 OutputFormat   <img alt="" class="has" height="227" src="https://img-blog.csdnimg.cn/2019080411325760.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="765"><li> 案例：过滤日志及自定义日志输出路径案例（自定义 OutputFormat）   ⑴需求：过滤输入的log日志中是否包含topcheer，包含topcheer的行输出到e:/topcheer.log,否则                 输出到e:/other.log文件中。        <img alt="" class="has" height="268" src="https://img-blog.csdnimg.cn/20190804114012540.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="390">   ⑵创建FilterOutputFormat.java用来自定义文件输出         <img alt="" class="has" height="269" src="https://img-blog.csdnimg.cn/20190804120644666.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="374">  ⑶创建FilterRecordWriter.java,对输出的数据进行自定义处理          <pre class="has"><code class="language-java">package com.kgf.mapreduce.outputformat;</li>
</ol>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.FSDataOutputStream;<br>import org.apache.hadoop.fs.FileSystem;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.RecordWriter;<br>import org.apache.hadoop.mapreduce.TaskAttemptContext;</p>
<p>/**</p>
<ul>
<li><p>自定义RecordWriter实现类，对输出的数据进行自定义处理</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class FilterRecordWriter extends RecordWriter&lt;Text, NullWritable&gt; &#123;</p>
<p>  private Configuration conf = null;</p>
<p>  FSDataOutputStream topcheerFos = null;</p>
<p>  FSDataOutputStream otherFos = null;<br>  /**</p>
<ul>
<li><p>用来创建输出流相关配置</p>
</li>
<li><p>@param context</p>
</li>
<li><p>@throws IOException</p>
</li>
<li><p>/<br>public FilterRecordWriter(TaskAttemptContext context) throws IOException &#123;<br>  try &#123;</p>
<pre><code>  //1：获取配置信息
  conf = context.getConfiguration();
  //2：获取文件输出流，分别输出到两个不同的路径文件用
  FileSystem fs = FileSystem.get(conf);
  //3:创建两个不同路径的输出流
  topcheerFos = fs.create(new Path(&quot;e:/topcheer.log&quot;));
  otherFos = fs.create(new Path(&quot;e:/other.log&quot;));
</code></pre>
<p>  } catch (Exception e) {</p>
<pre><code>  e.printStackTrace();
</code></pre>
<p>  }<br>}<br>/**</p>
</li>
<li><p>根据输入参数key的内容去判断数据要输出到那个文件路径中</p>
</li>
<li><p>/<br>@Override<br>public void write(Text key, NullWritable arg1) throws IOException, InterruptedException {</p>
<p>  //判断key中是否包含topcheer<br>  if(key.toString().contains(“topcheer”)) {</p>
<pre><code>  topcheerFos.write(key.getBytes());
</code></pre>
<p>  }else {</p>
<pre><code>  otherFos.write(key.getBytes());
</code></pre>
<p>  }<br>}<br>/**</p>
</li>
<li><p>最后我们需要将流关闭</p>
</li>
<li><p>/<br>@Override<br>public void close(TaskAttemptContext arg0) throws IOException, InterruptedException {<br>  //关闭资源<br>  if(topcheerFos!=null) {</p>
<pre><code>  topcheerFos.close();
</code></pre>
<p>  }<br>  if(otherFos!=null) {</p>
<pre><code>  otherFos.close();
</code></pre>
<p>  }<br>}<br>}<br></code></pre> ⑷自定义mapper阶段类FilterMapper        <img alt="" class="has" height="331" src="https://img-blog.csdnimg.cn/20190804121243957.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="406">     ⑸自定义reducer类        <img alt="" class="has" height="221" src="https://img-blog.csdnimg.cn/20190804121702752.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="465">   ⑹自定义FilterDriver         <pre class="has"><code class="language-java">package com.kgf.mapreduce.outputformat;</p>
</li>
</ul>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class FilterDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1:获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    
    //2:设置jar
    job.setJarByClass(FilterDriver.class);
    
    //3:关联自定义的mapper和reducer
    job.setMapperClass(FilterMapper.class);
    job.setReducerClass(FilterReducer.class);
    
    //4:设置mapper输出参数类型
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(NullWritable.class);
    
    //5:设置Reducer输出类型
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    
    //6:将自定义的输出outputFormat设置进来
    job.setOutputFormatClass(FilterOutputFormat.class);
    
    //7：设置输出路径，虽然我们自定义了 outputformat，但是因为我们的 outputformat 继承自 fileoutputformat， 
    //而 fileoutputformat 要输出一个_SUCCESS 文件，所以，在这还得指定一个输 出目录 .
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //8：提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre> ⑺效果：       <img alt="" class="has" height="138" src="https://img-blog.csdnimg.cn/20190804123037268.png" width="575">       <img alt="" class="has" height="151" src="https://img-blog.csdnimg.cn/20190804123049492.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="280">       <img alt="" class="has" height="130" src="https://img-blog.csdnimg.cn/20190804123720268.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="378">  ⑻具体流程：       数据–&gt;FilterMapper-&gt;FilterOutputFormat-&gt;FilterRecordWriter中构造方法初始化全局对象—&gt;FilterReducer中接受FilterMapper中       传递的每一行数据并且在输出的时候调用FilterRecordWriter中的write方法，将数据写入到指定的路径下。        </li></p>
 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
    <article
  id="post-MapReduce之Reduce join以及map join分布式缓存"
  class="article article-type-post"
  itemscope
  itemprop="blogPost"
  data-scroll-reveal
>
  <div class="article-inner">
    
    <header class="article-header">
       
<h2 itemprop="name">
  <a class="article-title" href="/2021/07/18/MapReduce%E4%B9%8BReduce%20join%E4%BB%A5%E5%8F%8Amap%20join%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/"
    >MapReduce之Reduce join以及map join分布式缓存</a> 
</h2>
 

      
    </header>
     
    <div class="article-meta">
      <a href="/2021/07/18/MapReduce%E4%B9%8BReduce%20join%E4%BB%A5%E5%8F%8Amap%20join%E5%88%86%E5%B8%83%E5%BC%8F%E7%BC%93%E5%AD%98/" class="article-date">
  <time datetime="2021-07-18T14:18:48.461Z" itemprop="datePublished">2021-07-18</time>
</a> 
  <div class="article-category">
    <a class="article-category-link" href="/categories/BigData/">BigData</a>
  </div>
   
    </div>
      
    <div class="article-entry" itemprop="articleBody">
       
  <p>title: MapReduce之Reduce join以及map join分布式缓存<br>categories:</p>
<ul>
<li>BigData</li>
</ul>
<p>—# 一：Reduce join </p>
<ol>
<li>简介：       ⑴原理：               Map 端的主要工作：                        为来自不同表(文件)的 key/value 对打标签以区别不同来源的记录然后用连接字段作为 key，                   其余部分和新加的标志作为 value，最后进行输出。                Reduce 端的主要工作：                        在 reduce 端以连接字段作为 key 的分组已经完成，我们只需要在每一个分组当中将那些来                   源于不同文件的记录(在 map 阶段已经打标志)分开，最后进行合并就 ok 了。         ⑵缺点：                这种方式的缺点很明显就是会造成 map和 reduce 端也就是 shuffle 阶段出现大量的数据传输，效率很低。   1.  案例：reduce 端表合并（数据倾斜）         ⑴需求：               订单数据表 t_order，在order.txt中：                     <img alt="" class="has" height="156" src="https://img-blog.csdnimg.cn/20190805220657652.png" width="435">                商品信息表 t_product，在t_product.txt文件中。                    <img alt="" class="has" height="119" src="https://img-blog.csdnimg.cn/20190805210117522.png" width="247">                将商品信息表中数据根据商品 pid 合并到订单数据表中。 最终数据如下：                 <img alt="" class="has" height="128" src="https://img-blog.csdnimg.cn/20190806205404170.png" width="254"> <li>代码实现     ⑴数据准备             <img alt="" class="has" height="168" src="https://img-blog.csdnimg.cn/20190806205500385.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="311"><img alt="" class="has" height="97" src="https://img-blog.csdnimg.cn/20190806205522863.png" width="368">      ⑵思路分析：             a：首先订单表和产品表是多对一的关系，因为每个pid在产品表中是唯一的，而在订单表中可能很多个订单都包含                   这个产品。             b：我们需要定义一个bean对象，如下：                     <img alt="" class="has" height="237" src="https://img-blog.csdnimg.cn/20190806205902974.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="314">             c：我们以pId作为Mapper阶段输出的key，其它内容为value                   mapper中整合后的数据如下：                   <img alt="" class="has" height="179" src="https://img-blog.csdnimg.cn/20190806210343656.png" width="227">            d：在ruducer阶段，我们以key（pId）作为条件，一个pId在产品表中只有一条数据，那么就好办了，直接查出产品                  表中对应的名称，将order表中的名称填充上即可输出。      ⑶TableBean对象代码如下：             <pre class="has"><code class="language-java">package com.kgf.mapreduce.reducerJoin;</li>
</ol>
<p>import java.io.DataInput;<br>import java.io.DataOutput;<br>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.Writable;</p>
<p>/***</p>
<ul>
<li><p>定义实体对象</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class TableBean implements Writable &#123;</p>
<p>  /<strong>订单ID</strong>/<br>  private String orderId;<br>  /<strong>产品ID</strong>/<br>  private String pId;<br>  /<strong>产品数量</strong>/<br>  private int amount;<br>  /<strong>产品名称</strong>/<br>  private String pName;<br>  /<strong>表类型：0-订单表，1-产品表</strong>/<br>  private String tableType;</p>
<p>  public TableBean(String orderId, String pId, int amount, String pName, String tableType) &#123;</p>
<pre><code>  super();
  this.orderId = orderId;
  this.pId = pId;
  this.amount = amount;
  this.pName = pName;
  this.tableType = tableType;
</code></pre>
<p>  }</p>
<p>  public TableBean() {</p>
<pre><code>  super();
</code></pre>
<p>  }<br>  /***</p>
<ul>
<li>反序列化方法</li>
<li>/<br>@Override<br>public void readFields(DataInput di) throws IOException {<br>  this.orderId = di.readUTF();<br>  this.pId = di.readUTF();<br>  this.amount = di.readInt();<br>  this.pName = di.readUTF();<br>  this.tableType = di.readUTF();<br>}<br>/***</li>
<li>序列化操作方法</li>
<li>/<br>@Override<br>public void write(DataOutput dot) throws IOException {<br>  dot.writeUTF(orderId);<br>  dot.writeUTF(pId);<br>  dot.writeInt(amount);<br>  dot.writeUTF(pName);<br>  dot.writeUTF(tableType);<br>}</li>
</ul>
<p>  @Override<br>  public String toString() {</p>
<pre><code>  return orderId+&quot;\t&quot;+ pName + &quot;\t&quot;+amount;
</code></pre>
<p>  }</p>
<p>  public String getOrderId() {</p>
<pre><code>  return orderId;
</code></pre>
<p>  }</p>
<p>  public void setOrderId(String orderId) {</p>
<pre><code>  this.orderId = orderId;
</code></pre>
<p>  }</p>
<p>  public String getpId() {</p>
<pre><code>  return pId;
</code></pre>
<p>  }</p>
<p>  public void setpId(String pId) {</p>
<pre><code>  this.pId = pId;
</code></pre>
<p>  }</p>
<p>  public int getAmount() {</p>
<pre><code>  return amount;
</code></pre>
<p>  }</p>
<p>  public void setAmount(int amount) {</p>
<pre><code>  this.amount = amount;
</code></pre>
<p>  }</p>
<p>  public String getpName() {</p>
<pre><code>  return pName;
</code></pre>
<p>  }</p>
<p>  public void setpName(String pName) {</p>
<pre><code>  this.pName = pName;
</code></pre>
<p>  }</p>
<p>  public String getTableType() {</p>
<pre><code>  return tableType;
</code></pre>
<p>  }</p>
<p>  public void setTableType(String tableType) {</p>
<pre><code>  this.tableType = tableType;
</code></pre>
<p>  }<br>}<br></code></pre> ⑷TableMapper类代码： <pre class="has"><code class="language-java">package com.kgf.mapreduce.reducerJoin;</p>
</li>
</ul>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;<br>import org.apache.hadoop.mapreduce.lib.input.FileSplit;</p>
<p>/***</p>
<ul>
<li><p>   创建mapper类：主要功能如下</p>
</li>
<li><p>   a：获取读取数据来自的标名称</p>
</li>
<li><p>   b：对每一行数据进行切割，将我们需要的数据筛选出来，并且标记来自的文件表</p>
</li>
<li><p>   c：最后将数据写出到reducer</p>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class TableMapper extends Mapper&lt;LongWritable, Text, Text, TableBean&gt; &#123;</p>
<p>  TableBean v = new TableBean();</p>
<p>  Text k = new Text();</p>
<p>  @Override<br>  protected void map(LongWritable key, Text value, Context context)</p>
<pre><code>      throws IOException, InterruptedException &#123;
  //1：获取读取文件的名称
  FileSplit splitFile = (FileSplit) context.getInputSplit();
  String fileName = splitFile.getPath().getName();
  //2：获取一行数据
  String line = value.toString();
  //3：判断表文件，对数据进行切割
  String[] files = line.split(&quot;\t&quot;);
  if(fileName.startsWith(&quot;order&quot;)) &#123;
      //订单表
      v.setOrderId(files[0]);
      v.setpId(files[1]);
      v.setAmount(Integer.parseInt(files[2]));
      v.setpName(&quot;&quot;);
      v.setTableType(&quot;0&quot;);
      k.set(files[1]);
  &#125;else &#123;
      //产品表
      v.setOrderId(&quot;&quot;);
      v.setpId(files[0]);
      v.setAmount(0);
      v.setpName(files[1]);
      v.setTableType(&quot;1&quot;);
      k.set(files[0]);
  &#125;
  //4:写出数据
  context.write(k,v);
</code></pre>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre> ⑸TableReducer类代码： <pre class="has"><code class="language-java">package com.kgf.mapreduce.reducerJoin;</p>
<p>import java.io.IOException;<br>import java.lang.reflect.InvocationTargetException;<br>import java.util.ArrayList;<br>import java.util.List;</p>
<p>import org.apache.commons.beanutils.BeanUtils;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Reducer;<br>/***</p>
<ul>
<li><pre><code>创建TableReducer类：
</code></pre>
</li>
<li><p>@author KGF</p>
</li>
<li></li>
<li><p>/<br>public class TableReducer extends Reducer&lt;Text, TableBean, TableBean, NullWritable&gt; {</p>
<p>  @Override<br>  protected void reduce(Text key, Iterable&lt;TableBean&gt; values,Context context) </p>
<pre><code>      throws IOException, InterruptedException &#123;
  //1：循环所有的values,
  List&amp;lt;TableBean&amp;gt; tbList = new ArrayList&amp;lt;TableBean&amp;gt;();//用来存放订单表数据
  TableBean tBean = new TableBean();//存放产品表数据
  for (TableBean val : values) &#123;
      //2：判断表类型
      if(&quot;0&quot;.equals(val.getTableType())) &#123;//订单表
          try &#123;
              //3：创建一个TableBean对象
              TableBean tb = new TableBean();
              //4：将val拷贝到tb中
              BeanUtils.copyProperties(tb, val);
              tbList.add(tb);//注意：如果我们不进行拷贝会出问题，都是最后一个对象的值，前面的对象数据会被覆盖
          &#125; catch (IllegalAccessException e) &#123;
              e.printStackTrace();
          &#125; catch (InvocationTargetException e) &#123;
              e.printStackTrace();
          &#125;
      &#125;else &#123;//产品表
          try &#123;
              BeanUtils.copyProperties(tBean, val);
          &#125; catch (IllegalAccessException e) &#123;
              e.printStackTrace();
          &#125; catch (InvocationTargetException e) &#123;
              e.printStackTrace();
          &#125;
      &#125;
  &#125;
  //拼接表
  for (TableBean tableBean : tbList) &#123;
      tableBean.setpName(tBean.getpName());
      context.write(tableBean, NullWritable.get());
  &#125;
</code></pre>
<p>  }</p>
</li>
</ul>
<p>}<br></code></pre> ⑹TableDriver类代码 <pre class="has"><code class="language-java">package com.kgf.mapreduce.reducerJoin;</p>
<p>import java.io.IOException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class TableDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, ClassNotFoundException, InterruptedException &#123;
    //1：获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    //2:设置jar对象
    job.setJarByClass(TableDriver.class);
    //3:关联mapper和reducer
    job.setMapperClass(TableMapper.class);
    job.setReducerClass(TableReducer.class);
    //4:设置mapper输出参数
    job.setMapOutputKeyClass(Text.class);
    job.setMapOutputValueClass(TableBean.class);
    //5:设置最终输出参数
    job.setOutputKeyClass(TableBean.class);
    job.setOutputValueClass(NullWritable.class);
    //6：设置数据路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    //7:提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre>   </li>1.  缺点       上面这种方式中，合并的操作是在 reduce 阶段完成，reduce 端的处理压力太大，map节点的运算负载则很低，资源利用率不高，且在 reduce 阶段极易产生数据倾斜 。 <li>  解决方案：map 端实现数据合并 之Map Join    ⑴ 使用场景：           Map Join适用于一张表十分小、一张表很大的场景    。  ⑵优点：         在Map端缓存多张表，提前处理业务逻辑，这样增加Map端业务，减少Reduce端数据的压力，尽可能的减少数据倾斜。  ⑶具体解决办法：        采用DistributedCache。        a:<strong>在Mapper的setup阶段，将文件读取到缓存集合中。        b:在驱动函数中加载缓存</strong>。  ⑷DistributedCacheDriver类代码： <pre class="has"><code class="language-java">package com.kgf.mapreduce.mapperJoin;</p>
<p>import java.io.IOException;<br>import java.net.URI;<br>import java.net.URISyntaxException;</p>
<p>import org.apache.hadoop.conf.Configuration;<br>import org.apache.hadoop.fs.Path;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Job;<br>import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;<br>import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;</p>
<p>public class DistributedCacheDriver &#123;</p>
<pre><code>public static void main(String[] args) throws IOException, URISyntaxException, ClassNotFoundException, InterruptedException &#123;
    //1：获取job对象
    Configuration conf = new Configuration();
    Job job = Job.getInstance(conf);
    //2:设置jar对象
    job.setJarByClass(DistributedCacheDriver.class);
    //3:关联mapper
    job.setMapperClass(DistributedCacheMapper.class);
    //4:设置最终输出参数
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(NullWritable.class);
    //5：设置数据路径
    FileInputFormat.setInputPaths(job, new Path(args[0]));
    FileOutputFormat.setOutputPath(job, new Path(args[1]));
    
    //6：加载缓存数据
    job.addCacheFile(new URI(&quot;file:///e:/t_product.txt&quot;));
    
    //7：map端join的逻辑不需要reducer阶段，设置reducetask的数量为0
    job.setNumReduceTasks(0);
    
    //8:提交
    boolean result = job.waitForCompletion(true);
    System.exit(result?0:1);
&#125;
</code></pre>
<p>}<br></code></pre> ⑸DistributedCacheMapper类： <pre class="has"><code class="language-java">package com.kgf.mapreduce.mapperJoin;</p>
<p>import java.io.BufferedReader;<br>import java.io.File;<br>import java.io.FileInputStream;<br>import java.io.IOException;<br>import java.io.InputStreamReader;<br>import java.net.URI;<br>import java.util.HashMap;<br>import java.util.Map;</p>
<p>import org.apache.commons.lang3.StringUtils;<br>import org.apache.hadoop.io.LongWritable;<br>import org.apache.hadoop.io.NullWritable;<br>import org.apache.hadoop.io.Text;<br>import org.apache.hadoop.mapreduce.Mapper;</p>
<p>public class DistributedCacheMapper extends Mapper&lt;LongWritable, Text, Text, NullWritable&gt; &#123;</p>
<pre><code>Map&amp;lt;String, String&amp;gt; pdMap = new HashMap&amp;lt;String, String&amp;gt;();

/***
 * 这个属于初始化方法，只执行一些，这个我们用来加载缓存中文件数据
 */
@Override
protected void setup(Mapper&amp;lt;LongWritable, Text, Text, NullWritable&amp;gt;.Context context)
        throws IOException, InterruptedException &#123;
    // 1：获取缓存文件
    URI[] cacheFiles = context.getCacheFiles();
    String path = cacheFiles[0].getPath().toString();
    BufferedReader reader = new BufferedReader(
            new InputStreamReader(new FileInputStream(path), &quot;UTF-8&quot;));
    String line = null;
    while(StringUtils.isNoneBlank(line=reader.readLine())) &#123;
        //对一行进行切割
        String[] fields = line.split(&quot;\t&quot;);
        //数据缓存到集合中
        pdMap.put(fields[0],fields[1]);
    &#125;
    //关闭流
    reader.close();
&#125;

Text k = new Text();

@Override
protected void map(LongWritable key, Text value,
        Mapper&amp;lt;LongWritable, Text, Text, NullWritable&amp;gt;.Context context)
        throws IOException, InterruptedException &#123;
    //1：读取一行
    String line = value.toString();
    //2：切割
    String[] fields = line.split(&quot;\t&quot;);
    //3：获取pId
    String pid = fields[1];
    //4：获取pid对应的名称
    String pName = pdMap.get(pid);
    //5：替换掉名称
    line = fields[0]+&quot;\t&quot;+pName+&quot;\t&quot;+fields[2];
    k.set(line);
    context.write(k, NullWritable.get());
&#125;
</code></pre>
<p>}<br></code></pre> ⑹这里我们不需要reducer类，注意：可以没有reducer,但是mapper必须有，效果如下：       <img alt="" class="has" height="210" src="https://img-blog.csdnimg.cn/20190806231953323.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0tfNTIwX1c=,size_16,color_FFFFFF,t_70" width="600"> </li></p>
<h1 id=""><a href="#" class="headerlink" title=""></a></h1> 
      <!-- reward -->
      
    </div>
    

    <!-- copyright -->
    
    <footer class="article-footer">
       
    </footer>
  </div>

    
 
    
</article>

    
  </article>
  

  
  <nav class="page-nav">
    
    <a class="extend prev" rel="prev" href="/page/8/">上一页</a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/7/">7</a><a class="page-number" href="/page/8/">8</a><span class="page-number current">9</span><a class="page-number" href="/page/10/">10</a><a class="page-number" href="/page/11/">11</a><a class="page-number" href="/page/12/">12</a><a class="extend next" rel="next" href="/page/10/">下一页</a>
  </nav>
  
</section>
</div>

      <footer class="footer">
  <div class="outer">
    <ul>
      <li>
        Copyrights &copy;
        2015-2021
        <i class="ri-heart-fill heart_icon"></i> kgf
      </li>
    </ul>
    <ul>
      <li>
        
        
        
        由 <a href="https://hexo.io" target="_blank">Hexo</a> 强力驱动
        <span class="division">|</span>
        主题 - <a href="https://github.com/Shen-Yu/hexo-theme-ayer" target="_blank">Ayer</a>
        
      </li>
    </ul>
    <ul>
      <li>
        
        
        <span>
  <span><i class="ri-user-3-fill"></i>访问人数:<span id="busuanzi_value_site_uv"></span></span>
  <span class="division">|</span>
  <span><i class="ri-eye-fill"></i>浏览次数:<span id="busuanzi_value_page_pv"></span></span>
</span>
        
      </li>
    </ul>
    <ul>
      
    </ul>
    <ul>
      
    </ul>
    <ul>
      <li>
        <!-- cnzz统计 -->
        
        <script type="text/javascript" src='https://s9.cnzz.com/z_stat.php?id=1278069914&amp;web_id=1278069914'></script>
        
      </li>
    </ul>
  </div>
</footer>
      <div class="float_btns">
        <div class="totop" id="totop">
  <i class="ri-arrow-up-line"></i>
</div>

<div class="todark" id="todark">
  <i class="ri-moon-line"></i>
</div>

      </div>
    </main>
    <aside class="sidebar on">
      <button class="navbar-toggle"></button>
<nav class="navbar">
  
  <div class="logo">
    <a href="/"><img src="/image1.ico" alt="爱上口袋的天空"></a>
  </div>
  
  <ul class="nav nav-main">
    
    <li class="nav-item">
      <a class="nav-item-link" href="/">主页</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/archives">归档</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/categories">分类</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" href="/tags">标签</a>
    </li>
    
    <li class="nav-item">
      <a class="nav-item-link" target="_blank" rel="noopener" href="https://blog.csdn.net/K_520_W">关于我</a>
    </li>
    
  </ul>
</nav>
<nav class="navbar navbar-bottom">
  <ul class="nav">
    <li class="nav-item">
      
      <a class="nav-item-link nav-item-search"  title="搜索">
        <i class="ri-search-line"></i>
      </a>
      
      
      <a class="nav-item-link" target="_blank" href="/atom.xml" title="RSS Feed">
        <i class="ri-rss-line"></i>
      </a>
      
    </li>
  </ul>
</nav>
<div class="search-form-wrap">
  <div class="local-search local-search-plugin">
  <input type="search" id="local-search-input" class="local-search-input" placeholder="Search...">
  <div id="local-search-result" class="local-search-result"></div>
</div>
</div>
    </aside>
    <div id="mask"></div>

<!-- #reward -->
<div id="reward">
  <span class="close"><i class="ri-close-line"></i></span>
  <p class="reward-p"><i class="ri-cup-line"></i>请我喝杯咖啡吧~</p>
  <div class="reward-box">
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/alipay.jpg">
      <span class="reward-type">支付宝</span>
    </div>
    
    
    <div class="reward-item">
      <img class="reward-img" src="https://cdn.jsdelivr.net/gh/Shen-Yu/cdn/img/wechat.jpg">
      <span class="reward-type">微信</span>
    </div>
    
  </div>
</div>
    
<script src="/js/jquery-2.0.3.min.js"></script>
 
<script src="/js/lazyload.min.js"></script>

<!-- Tocbot -->

<script src="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.js"></script>
<link
  rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/jquery-modal@0.9.2/jquery.modal.min.css"
/>
<script src="https://cdn.jsdelivr.net/npm/justifiedGallery@3.7.0/dist/js/jquery.justifiedGallery.min.js"></script>

<script src="/dist/main.js"></script>

<!-- ImageViewer -->
 <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                        <div class="pswp__preloader__cut">
                            <div class="pswp__preloader__donut"></div>
                        </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css">
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js"></script>

<script>
    function viewer_init() {
        let pswpElement = document.querySelectorAll('.pswp')[0];
        let $imgArr = document.querySelectorAll(('.article-entry img:not(.reward-img)'))

        $imgArr.forEach(($em, i) => {
            $em.onclick = () => {
                // slider展开状态
                // todo: 这样不好，后面改成状态
                if (document.querySelector('.left-col.show')) return
                let items = []
                $imgArr.forEach(($em2, i2) => {
                    let img = $em2.getAttribute('data-idx', i2)
                    let src = $em2.getAttribute('data-target') || $em2.getAttribute('src')
                    let title = $em2.getAttribute('alt')
                    // 获得原图尺寸
                    const image = new Image()
                    image.src = src
                    items.push({
                        src: src,
                        w: image.width || $em2.width,
                        h: image.height || $em2.height,
                        title: title
                    })
                })
                var gallery = new PhotoSwipe(pswpElement, PhotoSwipeUI_Default, items, {
                    index: parseInt(i)
                });
                gallery.init()
            }
        })
    }
    viewer_init()
</script> 
<!-- MathJax -->

<!-- Katex -->

<!-- busuanzi  -->
 
<script src="/js/busuanzi-2.3.pure.min.js"></script>
 
<!-- ClickLove -->

<!-- ClickBoom1 -->

<!-- ClickBoom2 -->

<!-- CodeCopy -->
 
<link rel="stylesheet" href="/css/clipboard.css">
 <script src="https://cdn.jsdelivr.net/npm/clipboard@2/dist/clipboard.min.js"></script>
<script>
  function wait(callback, seconds) {
    var timelag = null;
    timelag = window.setTimeout(callback, seconds);
  }
  !function (e, t, a) {
    var initCopyCode = function(){
      var copyHtml = '';
      copyHtml += '<button class="btn-copy" data-clipboard-snippet="">';
      copyHtml += '<i class="ri-file-copy-2-line"></i><span>COPY</span>';
      copyHtml += '</button>';
      $(".highlight .code pre").before(copyHtml);
      $(".article pre code").before(copyHtml);
      var clipboard = new ClipboardJS('.btn-copy', {
        target: function(trigger) {
          return trigger.nextElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        let $btn = $(e.trigger);
        $btn.addClass('copied');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-checkbox-circle-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPIED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-checkbox-circle-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
      clipboard.on('error', function(e) {
        e.clearSelection();
        let $btn = $(e.trigger);
        $btn.addClass('copy-failed');
        let $icon = $($btn.find('i'));
        $icon.removeClass('ri-file-copy-2-line');
        $icon.addClass('ri-time-line');
        let $span = $($btn.find('span'));
        $span[0].innerText = 'COPY FAILED';
        
        wait(function () { // 等待两秒钟后恢复
          $icon.removeClass('ri-time-line');
          $icon.addClass('ri-file-copy-2-line');
          $span[0].innerText = 'COPY';
        }, 2000);
      });
    }
    initCopyCode();
  }(window, document);
</script>
 
<!-- CanvasBackground -->

<script>
  if (window.mermaid) {
    mermaid.initialize({ theme: "forest" });
  }
</script>


    
  </div>
</body>

</html>